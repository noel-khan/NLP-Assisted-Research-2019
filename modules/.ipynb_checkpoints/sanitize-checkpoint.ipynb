{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BEGIN sanitize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(database)) # process requirement: ensure a database was loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os.path                    # os.path.exists('mydirectory/myfile.txt')\n",
    "#import pickle                     # load,save files\n",
    "import csv                        # load,save files\n",
    "\n",
    "import time                       # performance/completion feedback\n",
    "import re                         # regex for extracting alpha-numeric terms (not whitepace or punctuation)\n",
    "from collections import Counter   # \n",
    "import gensim                     # ngramming\n",
    "import nltk                       # parts-of-speech tagging, filter out prepositions (IN) and adverbs (RB*)\n",
    "                                  # and keep only: Left(POS,2) = NN (noun), left(POS,2) = JJ (adjective),\n",
    "                                  # left(POS,2) = VB (verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords', download_dir=USERARG_NLTKDATADIR)\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(USERARG_NLTKDATADIR + '/corpora/stopwords/english')\n",
    "stop_words.extend(['based', 'paper', 'using', 'proposed'])\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECLARATIONS & CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_words = [] # word dictionary/list derived from documents. Intermediate version of 'words'\n",
    "words        = [] # Counter/Dictionary of words in set of documents. Set at the tail end of this\n",
    "                  # script so as to de-comingle this critical piece of data from intermediate/mutating\n",
    "                  # word lists.\n",
    "\n",
    "iTime = 0     # used to track execution performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename():\n",
    "    _TMP_FILE  = 'data/' + USERARG_DATABASEFILE\n",
    "    _TMP_FILE += '-recs=' + str(USERARG_RECORD_LIMITER)\n",
    "    _TMP_FILE += '-wrds=' + str(USERARG_FILTER_WORDS)\n",
    "    _TMP_FILE += '-posf=' + str(USERARG_FILTER_POS)\n",
    "    _TMP_FILE += '-ngmn=' + str(USERARG_NGRAM_MINCOUNT)\n",
    "    _TMP_FILE += '-ngtr=' + str(USERARG_NGRAM_THRESHOLD)\n",
    "    return _TMP_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO THESE METHODS ARE USED IN OTHER IPYs. BUNDLE THEM IN A CLASS AND REUSE\n",
    "\n",
    "def flat_list_of_wordlists():\n",
    "    return [word for wordlist in get_list_of_wordlists() for word in wordlist]\n",
    "\n",
    "\n",
    "def get_list_of_wordlists():\n",
    "    print(\"\\tCompiling list of word-lists from database[FIELD_BzNDX_ABSTRACT_WORDS]...\")\n",
    "    iTime = time.time()\n",
    "    return [ record[FIELD_BzNDX_ABSTRACT_WORDS] for record in database[0:] ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_db_docwordlist_using_words(WORDS, MSG):\n",
    "    print(\"\\tFiltering abstract-word-list using words-list\",MSG)\n",
    "    iTime = time.time()\n",
    "\n",
    "    _len1 = len(database[0][FIELD_BzNDX_ABSTRACT_WORDS])\n",
    "    _count = 0\n",
    "\n",
    "    for i in range(len(database)):\n",
    "        tokens = [word for word in database[i][FIELD_BzNDX_ABSTRACT_WORDS] if word in WORDS]\n",
    "        database[i][FIELD_BzNDX_ABSTRACT_WORDS] = tokens\n",
    "        _count += len(tokens)\n",
    "\n",
    "    _len2 = len(database[0][FIELD_BzNDX_ABSTRACT_WORDS])\n",
    "\n",
    "    if _len2 < _len1:\n",
    "        print(\"\\t\\tConfirmed reduction in document word-list\")\n",
    "    else:\n",
    "        print(\"\\t\\tWARN: Unable to confirm reduction in document word-list\")\n",
    "    print(\"\\t\\t\",_count,\"words currently in corpus\")\n",
    "    \n",
    "    print(\"\\tCompleted in\", round(time.time() - iTime,3),\"seconds\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer(LIST_OF_WORDLISTS):\n",
    "    print(\"\\tBuilding N-gram models...\")\n",
    "    bigram = gensim.models.Phrases(LIST_OF_WORDLISTS, min_count=USERARG_NGRAM_MINCOUNT, threshold=USERARG_NGRAM_THRESHOLD)\n",
    "        \n",
    "    if USERARG_VERBOSE==1:\n",
    "        print(\"\\tExample of a ngram:\",bigram)\n",
    "    \n",
    "    return [bigram[word] for word in LIST_OF_WORDLISTS] \n",
    "    # RETURN is just like db[FIELD_BzNDX_ABSTRACT_WORDS] but includes ngrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SANITIZE: PRUNE NONWORDS AND NON-{NN,JJ,VB}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\tExtracting wordlists from docs and storing in database[FIELD_BzNDX_ABSTRACT_WORDS]...\")\n",
    "iTime = time.time()\n",
    "\n",
    "# CONSTANTS --- DO NOT FILTER THE WORD EMPTY_DOCUMENT_VALUE\n",
    "COMMON_WORDS = [\"are\",\"was\",\"has\",\"can\",\"been\",\"the\",\"thus\",\"therefore\",\"be\",\"to\",\"of\",\"and\",\"a\",\"in\",\"that\",\"have\",\"I\",\"it\",\"for\",\"not\",\"on\",\"with\",\"he\",\"as\",\"you\",\"do\",\"at\",\"this\",\"but\",\"his\",\"by\",\"from\",\"they\",\"we\",\"say\",\"her\",\"she\",\"or\",\"an\",\"will\",\"my\",\"one\",\"all\",\"would\",\"there\",\"their\",\"what\",\"so\",\"up\",\"out\",\"if\",\"about\",\"who\",\"get\",\"which\",\"go\",\"me\",\"when\",\"make\",\"can\",\"like\",\"time\",\"no\",\"just\",\"him\",\"know\",\"take\",\"person\",\"into\",\"year\",\"your\",\"good\",\"some\",\"could\",\"them\",\"see\",\"other\",\"than\",\"then\",\"now\",\"look\",\"only\",\"come\",\"its\",\"over\",\"think\",\"also\",\"back\",\"after\",\"use\",\"two\",\"how\",\"our\",\"work\",\"first\",\"well\",\"way\",\"even\",\"new\",\"want\",\"because\",\"any\",\"these\",\"give\",\"day\",\"most\",\"us\"]  # ~11% of corpus. REF: https://www.englishclub.com/vocabulary/common-words-100.htm\n",
    "\n",
    "# -- extract list of words --\n",
    "module_words = [] # needed here as well so this cell is re-runnable\n",
    "tokens = []\n",
    "for j in range(len(database)):\n",
    "    # split() breaks on 1 char, but regex splits on all punctuation or whitespace\n",
    "    tokens = re.findall(\"[\\w']+\", database[j][FIELD_BzNDX_ABSTRACT].lower())\n",
    "    \n",
    "    # rather than multiple passes, clean, filter short, common and non-words now\n",
    "    tokens = [x for x in tokens if (x.isalpha()) and (len(x) > 2) and (x not in COMMON_WORDS) and (x not in stop_words)]\n",
    "   \n",
    "    # save word list to database.records to prevent multi passes\n",
    "    if len(database[j]) == FIELD_BzNDX_ABSTRACT_WORDS:\n",
    "        database[j].append(tokens)\n",
    "    else:\n",
    "        database[j][FIELD_BzNDX_ABSTRACT_WORDS] = tokens\n",
    "\n",
    "    # compile unfiltered words\n",
    "    for i in range(len(tokens)):\n",
    "        if len(tokens[i]) > 2:\n",
    "            module_words += [tokens[i]] # double brackets to capture words not letters\n",
    "\n",
    "re = None\n",
    "tokens = None\n",
    "COMMON_WORDS = None\n",
    "stop_words = None\n",
    "\n",
    "print(\"\\tCompleted in\", round(time.time() - iTime,3),\"seconds\")\n",
    "\n",
    "# FWD: words to POS-filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [OPTIONAL] \n",
    "# Apply parts-of-speech filter to words list and \n",
    "# then prune word-lists in database[FIELD_BzNDX_ABSTRACT_WORDS]\n",
    "# the POS filter needs to be applied before \n",
    "# the counter filter in order to remove high\n",
    "# frequency words that may bump push out keywords.\n",
    "\n",
    "if USERARG_FILTER_POS==1:\n",
    "    iTime = time.time()\n",
    "    print(\"\\tApplying parts-of-speech filter...\")\n",
    "    result = nltk.pos_tag(module_words)\n",
    "\n",
    "    for i in range(len(module_words)):\n",
    "        result = nltk.pos_tag([module_words[i]])\n",
    "        pos_tag = result[0][1]\n",
    "        if pos_tag[0:2] not in [\"NN\",\"JJ\",\"VB\"] and module_words[i] not in KEYWORDS:\n",
    "            module_words[i] = \"\"\n",
    "            #print(result)\n",
    "    module_words[:] = [x for x in module_words if x != \"\"]\n",
    "    \n",
    "    filter_db_docwordlist_using_words(module_words, \"after PoS filtering...\")\n",
    "    \n",
    "    print(\"\\t\",len(module_words), \"words in database\")\n",
    "    print(\"\\tCompleted in\", round(time.time() - iTime,3),\"seconds\")\n",
    "\n",
    "words = None # obsolete at this pt, will be recreated from abstract-words later\n",
    "pos_tag = None\n",
    "\n",
    "# FWD: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGGREGATE: WORDS TO OFT-REPEATED PHRASES (N-GRAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\tNOTE: N-grams are underscore delimited words that tend to appear together\")\n",
    "print(\"\\tNOTE: Repeating this process N-times yields an N-gram\")\n",
    "iTime = time.time()\n",
    "\n",
    "for i in range(USERARG_NGRAM_N):\n",
    "    ngrams = ngrammer(get_list_of_wordlists())\n",
    "\n",
    "print(\"\\tReplacing database[FIELD_BzNDX_ABSTRACT_WORDS] with ngrammed word-list...\")\n",
    "for i in range(len(database)):\n",
    "    database[i][FIELD_BzNDX_ABSTRACT_WORDS] = ngrams[i]\n",
    "\n",
    "# _word_list = []\n",
    "# _index = 0\n",
    "# _ngram_count = 0\n",
    "# for i in range(len(ngrams)):\n",
    "#     _word_list = ngrams[i]\n",
    "#     for _word in _word_list:\n",
    "#         module_words.append(_word)\n",
    "#         if _word.find(\"_\") > -1:\n",
    "#             _ngram_count += 1\n",
    "#             if word not in _ngrams:\n",
    "#                 _ngrams.append(_word)\n",
    "#                 _index = i\n",
    "# if USERARG_VERBOSE==1:\n",
    "#     print(\"\\t\", len(_ngrams), \"distinct ngrams found a total of \",_ngram_count,\"times\")\n",
    "#     print(\"\\tExample document with Ngram is database record\",_index)\n",
    "# _ngrams = None\n",
    "# _word_list = None\n",
    "# _index = None\n",
    "# _ngram_count = None\n",
    "\n",
    "ngrams = None\n",
    "\n",
    "print(\"\\tCompleted in\", round(time.time() - iTime,3),\"seconds\")\n",
    "\n",
    "# FWD: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\tCompiling updated words list...\")\n",
    "iTime = time.time()\n",
    "\n",
    "module_words = flat_list_of_wordlists()\n",
    "\n",
    "print(\"\\tNgram report list...\")\n",
    "_ngrams = [w for w in module_words if w.find(\"_\") > -1]\n",
    "print(\"\\t\", len(Counter(_ngrams)), \"distinct ngrams found a total of \",len(_ngrams),\"times\")\n",
    "print(\"\\tNGrams: \", _ngrams)\n",
    "_ngrams = None\n",
    "w = None\n",
    "\n",
    "print(\"\\tCompleted in\", round(time.time() - iTime,3),\"seconds\")\n",
    "\n",
    "# FWD: module_words (list), used by frequency-filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRUNE: FILTER BY FREQUENCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\tAnalyzing word-frequency...\")\n",
    "iTime = time.time()\n",
    "\n",
    "# The Counter object, is essentially a collection of tuples \n",
    "# comprised of a unique string and occurence count. \n",
    "word_counts = Counter(module_words)\n",
    "print(\"\\t\",len(word_counts), \"distinct words\")\n",
    "        \n",
    "# Paradoxically, Counter.most_common() returns a list of tuples rather \n",
    "# than Counter. To get back to a Counter object - required by \n",
    "# the NL smart-filters here - I convert the list of tuples\n",
    "# into a dictionary then pass that into the Counter's constructor,\n",
    "# but we'll convert it back into a Counter later.\n",
    "if USERARG_FILTER_WORDS!=0:\n",
    "    print(\"\\tApplying word-frequency filter...\")\n",
    "    print(\"\\t\",USERARG_FILTER_WORDS, \"most_common words retained\")\n",
    "    module_words = word_counts.most_common(USERARG_FILTER_WORDS)\n",
    "    module_words = [w[0] for w in module_words] # strip counts from words\n",
    "    #-- CRITICAL STEP --\n",
    "    # now filter the word-lists in the database using \n",
    "    # the frequency-filtered and distinct 'words' dictionary.\n",
    "    filter_db_docwordlist_using_words(module_words, \"after filtering on frequency...\")\n",
    "\n",
    "module_words = None\n",
    "word_counts = None\n",
    "w = None\n",
    "print(\"\\tCompleted in\", round(time.time() - iTime,3),\"seconds\")\n",
    "\n",
    "# FWD: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE 'WORDS' AND SANITIZED-DB\n",
    "\n",
    "CRITICAL: \n",
    "TO ALL DATABAASES TO BE CACHED AND USED/SHARED, SAVE ALL\n",
    "PREPROCESSED RESULTS TO FILES: {database, words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\tConverting words-dictionary into Counter object...\")\n",
    "iTime = time.time()\n",
    "\n",
    "# last step gauauntee the only words in dictionary used for featureization\n",
    "words = Counter(flat_list_of_wordlists())\n",
    "assert(len(words))\n",
    "\n",
    "print(\"\\tCompleted in\", round(time.time() - iTime,3),\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word/count dictionary\n",
    "#print(\"\\tINFO\\TYPE: words\",type(words),\" example:\",list(words.items())[:1]) # <key,count>\n",
    "#print(\"\\tINFO\\API: words: e = list(words.elements()) or {k,v} = list(words.{keys,values}())\")\n",
    "#print(\"\\tINFO\\TYPE: database\",type(database), \"dimensions:\",len(database),\"x\",len(database[0]))\n",
    "#if USERARG_VERBOSE==1:\n",
    "#    print(\"\\tExample database record:\", database[0])\n",
    "\n",
    "_TMP_FILE = sanitize_filename()\n",
    "\n",
    "with open(_TMP_FILE + '.words', mode='w', encoding=\"utf-8\") as fp:\n",
    "    csv_writer = csv.writer(fp, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow(words.items())\n",
    "print('\\tINFO: words dictionary cached as ' + _TMP_FILE)\n",
    "\n",
    "# using \"with\" will scope the file-handle to this block (auto-closes).\n",
    "# the database is a list of lists (records) and records are field-arrays.\n",
    "# The csv.writer writes one record per line.\n",
    "with open(_TMP_FILE + '.sandb', mode='w', encoding=\"utf-8\") as fp:\n",
    "    csv_writer = csv.writer(fp, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for record in database:\n",
    "        csv_writer.writerow(record)\n",
    "print('\\tINFO: sanitized dictionary cached as ' + _TMP_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"END sanitize\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
