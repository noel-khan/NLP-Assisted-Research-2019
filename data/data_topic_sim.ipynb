{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database=[\n",
    "[1,1,1,\"182 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 6, NO. 2, APRIL 2002 A Fast and Elitist Multiobjective Genetic Algorithm: NSGA-II Kalyanmoy Deb, Associate Member, IEEE, Amrit Pratap, Sameer Agarwal, and T. Meyarivan Abstract—Multiobjective evolutionary algorithms (EAs) that use nondominated sorting and sharing have been criticized mainly for their: 1) ( 3) computational complexity (where is the number of objectives and is the population size); 2) nonelitism approach; and 3) the need for specifying a sharing parameter. In this paper, we suggest a nondominated sorting-based multiobjective EA (MOEA), called nondominated sorting genetic algorithm II (NSGA-II), which alleviates all the above three difficulties. Specifically, a fast nondominated sorting approach with ( 2) computational complexity is presented. Also, a selection operator is presented that creates a mating pool by combining the parent and offspring populations and selecting the best (with respect to fitness and spread) solutions. Simulation results on difficult test problems show that the proposed NSGA-II, in most problems, is able to find much better spread of solutions and better convergence near the true Pareto-optimal front compared to Pareto-archived evolution strategy and strength-Pareto EA—two other elitist MOEAs that pay special attention to creating a diverse Pareto-optimal front. Moreover, we modify the definition of dominance in order to solve constrained multiobjective problems efficiently. Simulation results of the constrained NSGA-II on a number of test problems, including a five-objective seven-constraint nonlinear problem, are compared with another constrained multiobjective optimizer and much better performance of NSGA-II is observed. Index Terms—Constraint handling, elitism, genetic algorithms, multicriterion decision making, multiobjective optimization, Pareto-optimal solutions. I. INTRODUCTION THE PRESENCE of multiple objectives in a problem, in principle, gives rise to a set of optimal solutions (largely known as Pareto-optimal solutions), instead of a single optimal solution. In the absence of any further information, one of these Pareto-optimal solutions cannot be said to be better than the other. This demands a user to find as many Pareto-optimal solutions as possible. Classical optimization methods (including the multicriterion decision-making methods) suggest converting the multiobjective optimization problem to a single-objective optimization problem by emphasizing one particular Pareto-optimal solution at a time. When such a method is to be used for finding multiple solutions, it has to be applied many times, hopefully finding a different solution at each simulation run. Over the past decade, a number of multiobjective evolutionary algorithms (MOEAs) have been suggested [1], [7], [13], Manuscript received August 18, 2000; revised February 5, 2001 and September 7, 2001. The work of K. Deb was supported by the Ministry of Human Resources and Development, India, under the Research and Development Scheme. The authors are with the Kanpur Genetic Algorithms Laboratory, Indian Institute of Technology, Kanpur PIN 208 016, India (e-mail: deb@iitk.ac.in). Publisher Item Identifier S 1089-778X(02)04101-2. [20], [26]. The primary reason for this is their ability to find multiple Pareto-optimal solutions in one single simulation run. Since evolutionary algorithms (EAs) work with a population of solutions, a simple EA can be extended to maintain a diverse set of solutions. With an emphasis for moving toward the true Pareto-optimal region, an EA can be used to find multiple Pareto-optimal solutions in one single simulation run. The nondominated sorting genetic algorithm (NSGA) proposed in [20] was one of the first such EAs. Over the years, the main criticisms of the NSGA approach have been as follows. 1) High computational complexity of nondominated sorting: The currently-used nondominated sorting algorithm has a computational complexity of (where is the number of objectives and is the population size). This makes NSGA computationally expensive for large population sizes. This large complexity arises because of the complexity involved in the nondominated sorting procedure in every generation. 2) Lack of elitism: Recent results [25], [18] show that elitism can speed up the performance of the GA significantly, which also can help preventing the loss of good solutions once they are found. 3) Need for specifying the sharing parameter : Traditional mechanisms of ensuring diversity in a population so as to get a wide variety of equivalent solutions have relied mostly on the concept of sharing. The main problem with sharing is that it requires the specification of a sharing parameter ( ). Though there has been some work on dynamic sizing of the sharing parameter [10], a parameter- less diversity-preservation mechanism is desirable. In this paper, we address all of these issues and propose an improved version of NSGA, which we call NSGA-II. From the simulation results on a number of difficult test problems, we find that NSGA-II outperforms two other contemporary MOEAs: Pareto-archived evolution strategy (PAES) [14] and strength- Pareto EA (SPEA) [24] in terms of finding a diverse set of solutions and in converging near the true Pareto-optimal set. Constrained multiobjective optimization is important from the point of viewof practical problem solving, but notmuchattention has been paid so far in this respect among the EA researchers. In this paper, we suggest a simple constraint-handling strategy with NSGA-II that suits well for any EA. On four problems chosen from the literature, NSGA-II has been compared with another recently suggested constraint-handling strategy. These results encourage the application of NSGA-II to more complex and real-world multiobjective optimization problems. In the remainder of the paper, we briefly mention a number of existing elitist MOEAs in Section II. Thereafter, in Section III, 1089-778X/02$17.00 © 2002  13:25:36 UTC from IEEE Xplore. Restrictions apply. DEB et al.: A FAST AND ELITIST MULTIOBJECTIVE GA: NSGA-II 183 we describe the proposed NSGA-II algorithm in details. Section IV presents simulation results of NSGA-II and compares them with two other elitist MOEAs (PAES and SPEA). In Section V, we highlight the issue of parameter interactions, a matter that is important in evolutionary computation research. The next section extends NSGA-II for handling constraints and compares the results with another recently proposed constraint-handling method. Finally, we outline the conclusions of this paper. II. ELITIST MULTIOBJECTIVE EVOLUTIONARY ALGORITHMS During 1993–1995, a number of different EAs were suggested to solve multiobjective optimization problems. Of them, Fonseca and Fleming’s MOGA [7], Srinivas and Deb’s NSGA [20], and Horn et al.’s NPGA [13] enjoyed more attention. These algorithms demonstrated the necessary additional operators for converting a simple EA to a MOEA. Two common features on all three operators were the following: i) assigning fitness to population members based on nondominated sorting and ii) preserving diversity among solutions of the same nondominated front. Although they have been shown to find multiple nondominated solutions on many test problems and a number of engineering design problems, researchers realized the need of introducing more useful operators (which have been found useful in single-objective EA’s) so as to solve multiobjective optimization problems better. Particularly, the interest has been to introduce elitism to enhance the convergence properties of a MOEA. Reference [25] showed that elitism helps in achieving better convergence in MOEAs. Among the existing elitist MOEAs, Zitzler and Thiele’s SPEA [26], Knowles and Corne’s Pareto-archived PAES [14], and Rudolph’s elitist GA [18] are well studied. We describe these approaches in brief. For details, readers are encouraged to refer to the original studies. Zitzler and Thiele [26] suggested an elitist multicriterion EA with the concept of nondomination in their SPEA. They suggested maintaining an external population at every generation storing all nondominated solutions discovered so far beginning from the initial population. This external population participates in all genetic operations. At each generation, a combined population with the external and the current population is first constructed. All nondominated solutions in the combined population are assigned a fitness based on the number of solutions they dominate and dominated solutions are assigned fitness worse than the worst fitness of any nondominated solution. This assignment of fitness makes sure that the search is directed toward the nondominated solutions. A deterministic clustering technique is used to ensure diversity among nondominated solutions. Although the implementation suggested in [26] is , with proper bookkeeping the complexity of SPEA can be reduced to . Knowles and Corne [14] suggested a simple MOEA using a single-parent single-offspring EA similar to (1 1)-evolution strategy. Instead of using real parameters, binary strings were used and bitwise mutations were employed to create offsprings. In their PAES, with one parent and one offspring, the offspring is compared with respect to the parent. If the offspring dominates the parent, the offspring is accepted as the next parent and the iteration continues. On the other hand, if the parent dominates the offspring, the offspring is discarded and a new mutated solution (a new offspring) is found. However, if the offspring and the parent do not dominate each other, the choice between the offspring and the parent is made by comparing them with an archive of best solutions found so far. The offspring is compared with the archive to check if it dominates any member of the archive. If it does, the offspring is accepted as the new parent and all the dominated solutions are eliminated from the archive. If the offspring does not dominate any member of the archive, both parent and offspring are checked for their nearness with the solutions of the archive. If the offspring resides in a least crowded region in the objective space among the members of the archive, it is accepted as a parent and a copy of added to the archive. Crowding is maintained by dividing the entire search space deterministically in subspaces, where is the depth parameter and is the number of decision variables, and by updating the subspaces dynamically. Investigators have calculated the worst case complexity of PAES for evaluations as , where is the archive length. Since the archive size is usually chosen proportional to the population size , the overall complexity of the algorithm is . Rudolph [18] suggested, but did not simulate, a simple elitist MOEA based on a systematic comparison of individuals from parent and offspring populations. The nondominated solutions of the offspring population are compared with that of parent solutions to form an overall nondominated set of solutions, which becomes the parent population of the next iteration. If the size of this set is not greater than the desired population size, other individuals from the offspring population are included. With this strategy, he proved the convergence of this algorithm to the Pareto-optimal front. Although this is an important achievement in its own right, the algorithm lacks motivation for the second task of maintaining diversity of Pareto-optimal solutions. An explicit diversity-preserving mechanism must be added to make it more practical. Since the determinism of the first nondominated front is , the overall complexity of Rudolph’s algorithm is also . In the following, we present the proposed nondominated sorting GA approach, which uses a fast nondominated sorting procedure, an elitist-preserving approach, and a parameterless niching operator. III. ELITIST NONDOMINATED SORTING GENETIC ALGORITHM A. Fast Nondominated Sorting Approach For the sake of clarity, we first describe a naive and slow procedure of sorting a population into different nondomination levels. Thereafter, we describe a fast approach. In a naive approach, in order to identify solutions of the first nondominated front in a population of size , each solution can be compared with every other solution in the population to find if it is dominated. This requires comparisons for each solution, where is the number of objectives. When this process is continued to find all members of the first nondominated level in the population, the total complexity is . At this stage, all individuals in the first nondominated front are found. In order to find the individuals in the next nondominated Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:25:36 UTC from IEEE Xplore. Restrictions apply. 184 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 6, NO. 2, APRIL 2002 front, the solutions of the first front are discounted temporarily and the above procedure is repeated. In the worst case, the task of finding the second front also requires computations, particularly when number of solutions belong to the second and higher nondominated levels. This argument is true for finding third and higher levels of nondomination. Thus, the worst case is when there are fronts and there exists only one solution in each front. This requires an overall computations. Note that storage is required for this procedure. In the following paragraph and equation shown at the bottom of the page, we describe a fast nondominated sorting approach which will require computations. First, for each solution we calculate two entities: 1) domination count , the number of solutions which dominate the solution , and 2) , a set of solutions that the solution dominates. This requires comparisons. All solutions in the first nondominated front will have their domination count as zero. Now, for each solution with , we visit each member ( ) of its set and reduce its domination count by one. In doing so, if for any member the domination count becomes zero, we put it in a separate list . These members belong to the second nondominated front. Now, the above procedure is continued with each member of and the third front is identified. This process continues until all fronts are identified. For each solution in the second or higher level of nondomination, the domination count can be at most . Thus, each solution will be visited at most times before its domination count becomes zero. At this point, the solution is assigned a nondomination level and will never be visited again. Since there are at most such solutions, the total complexity is . Thus, the overall complexity of the procedure is . Another way to calculate this complexity is to realize that the body of the first inner loop (for each ) is executed exactly times as each individual can be the member of at most one front and the second inner loop (for each ) can be executed at maximum times for each individual [each individual dominates individuals at maximum and each domination check requires at most comparisons] results in the overall computations. It is important to note that although the time complexity has reduced to , the storage requirement has increased to . B. Diversity Preservation We mentioned earlier that, along with convergence to the Pareto-optimal set, it is also desired that an EA maintains a good spread of solutions in the obtained set of solutions. The original NSGA used the well-known sharing function approach, which has been found to maintain sustainable diversity in a population with appropriate setting of its associated parameters. The sharing function method involves a sharing parameter , which sets the extent of sharing desired in a problem. This parameter is related to the distance metric chosen to calculate the proximity measure between two population members. The parameter denotes the largest value of that distance metric within which any two solutions share each other’s fitness. This parameter is usually set by the user, although there exist some guidelines [4]. There are two difficulties with this sharing function approach. 1) The performance of the sharing function method in maintaining a spread of solutions depends largely on the chosen value. - - - for each for each if then If dominates Add to the set of solutions dominated by else if then Increment the domination counter of if then belongs to the first front Initialize the front counter while Used to store the members of the next front for each for each if then belongs to the next front Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:25:36 UTC from IEEE Xplore. Restrictions apply. DEB et al.: A FAST AND ELITIST MULTIOBJECTIVE GA: NSGA-II 185 Fig. 1. Crowding-distance calculation. Points marked in filled circles are solutions of the same nondominated front. 2) Since each solution must be compared with all other solutions in the population, the overall complexity of the sharing function approach is . In the proposed NSGA-II, we replace the sharing function approach with a crowded-comparison approach that eliminates both the above difficulties to some extent. The new approach does not require any user-defined parameter for maintaining diversity among population members. Also, the suggested approach has a better computational complexity. To describe this approach, we first define a density-estimation metric and then present the crowded-comparison operator. 1) Density Estimation: To get an estimate of the density of solutions surrounding a particular solution in the population, we calculate the average distance of two points on either side of this point along each of the objectives. This quantity serves as an estimate of the perimeter of the cuboid formed by using the nearest neighbors as the vertices (call this the crowding distance). In Fig. 1, the crowding distance of the th solution in its front (marked with solid circles) is the average side length of the cuboid (shown with a dashed box). The crowding-distance computation requires sorting the population according to each objective function value in ascending order of magnitude. Thereafter, for each objective function, the boundary solutions (solutions with smallest and largest function values) are assigned an infinite distance value. All other intermediate solutions are assigned a distance value equal to the absolute normalized difference in the function values of two adjacent solutions. This calculation is continued with other objective functions. The overall crowding-distance value is calculated as the sum of individual distance values corresponding to each objective. Each objective function is normalized before calculating the crowding distance. The algorithm as shown at the bottom of the page outlines the crowding-distance computation procedure of all solutions in an nondominated set . Here, refers to the th objective function value of the th individual in the set and the parameters and are the maximum and minimum values of the th objective function. The complexity of this procedure is governed by the sorting algorithm. Since independent sortings of at most solutions (when all population members are in one front ) are involved, the above algorithm has computational complexity. After all population members in the set are assigned a distance metric, we can compare two solutions for their extent of proximity with other solutions. A solution with a smaller value of this distance measure is, in some sense, more crowded by other solutions. This is exactly what we compare in the proposed crowded-comparison operator, described below. Although Fig. 1 illustrates the crowding-distance computation for two objectives, the procedure is applicable to more than two objectives as well. 2) Crowded-Comparison Operator: The crowded-comparison operator ( ) guides the selection process at the various stages of the algorithm toward a uniformly spread-out Paretooptimal front. Assume that every individual in the population has two attributes: 1) nondomination rank ( ); 2) crowding distance ( ). We now define a partial order as if or and That is, between two solutions with differing nondomination ranks, we prefer the solution with the lower (better) rank. Otherwise, if both solutions belong to the same front, then we prefer the solution that is located in a lesser crowded region. With these three new innovations—a fast nondominated sorting procedure, a fast crowded distance estimation procedure, and a simple crowded comparison operator, we are now ready to describe the NSGA-II algorithm. C. Main Loop Initially, a random parent population is created. The population is sorted based on the nondomination. Each solution is assigned a fitness (or rank) equal to its nondomination level (1 is the best level, 2 is the next-best level, and so on). Thus, minimization of fitness is assumed. At first, the usual binary tournament selection, recombination, and mutation operators are used to create a offspring population of size . Since elitism - - number of solutions in for each set initialize distance for each objective sort sort using each objective value so that boundary points are always selected for to for all other points Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:25:36 UTC from IEEE Xplore. Restrictions apply. 186 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 6, NO. 2, APRIL 2002 is introduced by comparing current population with previously found best nondominated solutions, the procedure is different after the initial generation. We first describe the th generation of the proposed algorithm as shown at the bottom of the page. The step-by-step procedure shows that NSGA-II algorithm is simple and straightforward. First, a combined population is formed. The population is of size . Then, the population is sorted according to nondomination. Since all previous and current population members are included in , elitism is ensured. Now, solutions belonging to the best nondominated set are of best solutions in the combined population and must be emphasized more than any other solution in the combined population. If the size of is smaller then , we definitely choose all members of the set for the new population . The remaining members of the population are chosen from subsequent nondominated fronts in the order of their ranking. Thus, solutions from the set are chosen next, followed by solutions from the set , and so on. This procedure is continued until no more sets can be accommodated. Say that the set is the last nondominated set beyond which no other set can be accommodated. In general, the count of solutions in all sets from to would be larger than the population size. To choose exactly population members, we sort the solutions of the last front using the crowded-comparison operator in descending order and choose the best solutions needed to fill all population slots. The NSGA-II procedure is also shown in Fig. 2. The new population of size is now used for selection, crossover, and mutation to create a newpopulation of size . It is important to note that we use a binary tournament selection operator but the selection criterion is now based on the crowded-comparison operator . Since this operator requires both the rank and crowded distance of each solution in the population, we calculate these quantities while forming the population , as shown in the above algorithm. Consider the complexity of one iteration of the entire algorithm. The basic operations and their worst-case complexities are as follows: 1) nondominated sorting is ; 2) crowding-distance assignment is ; 3) sorting on is . The overall complexity of the algorithm is , which is governed by the nondominated sorting part of the algorithm. If Fig. 2. NSGA-II procedure. performed carefully, the complete population of size need not be sorted according to nondomination. As soon as the sorting procedure has found enough number of fronts to have members in , there is no reason to continue with the sorting procedure. The diversity among nondominated solutions is introduced by using the crowding comparison procedure, which is used in the tournament selection and during the population reduction phase. Since solutions compete with their crowding-distance (a measure of density of solutions in the neighborhood), no extra niching parameter (such as needed in the NSGA) is required. Although the crowding distance is calculated in the objective function space, it can also be implemented in the parameter space, if so desired [3]. However, in all simulations performed in this study, we have used the objective-function space niching. IV. SIMULATION RESULTS In this section, we first describe the test problems used to compare the performance of NSGA-II with PAES and SPEA. For PAES and SPEA, we have identical parameter settings as suggested in the original studies. For NSGA-II, we have chosen a reasonable set of values and have not made any effort in finding the best parameter setting. We leave this task for a future study. combine parent and offspring population - - - all nondominated fronts of and until until the parent population is filled - - calculate crowding-distance in include th nondominated front in the parent pop check the next front for inclusion Sort sort in descending order using choose the first elements of - - use selection, crossover and mutation to create a new population increment the generation counter Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:25:36 UTC from IEEE Xplore. Restrictions apply. DEB et al.: A FAST AND ELITIST MULTIOBJECTIVE GA: NSGA-II 187 TABLE I TEST PROBLEMS USED IN THIS STUDY All objective functions are to be minimized. A. Test Problems We first describe the test problems used to compare different MOEAs. Test problems are chosen from a number of significant past studies in this area. Veldhuizen [22] cited a number of test problems that have been used in the past. Of them, we choose four problems: Schaffer’s study (SCH) [19], Fonseca and Fleming’s study (FON) [10], Poloni’s study (POL) [16], and Kursawe’s study (KUR) [15]. In 1999, the first author suggested a systematic way of developing test problems for multiobjective optimization [3]. Zitzler et al. [25] followed those guidelines and suggested six test problems. We choose five of those six problems here and call them ZDT1, ZDT2, ZDT3, ZDT4, and ZDT6. All problems have two objective functions. None of these problems have any constraint. We describe these problems in Table I. The table also shows the number of variables, their bounds, the Pareto-optimal solutions, and the nature of the Pareto-optimal front for each problem. All approaches are run for a maximum of 25 000 function evaluations. We use the single-point crossover and bitwise mutation for binary-coded GAs and the simulated binary crossover (SBX) operator and polynomial mutation [6] for real-coded GAs. The crossover probability of and a mutation probability of or (where is the number of decision variables for real-coded GAs and is the string length for binary-coded GAs) are used. For real-coded NSGA-II, we use distribution indexes [6] for crossover and mutation operators as and , respectively. The population obtained at the end of 250 generations (the population after elite-preserving operator is applied) is used to calculate a couple of performance metrics, which we discuss in the next section. For PAES, we use a depth value equal to four and an archive size of 100. We use all population members of the archive obtained at the end of 25 000 iterations to calculate the performance metrics. For SPEA, we use a population of size 80 and an external population of size 20 (this 4 : 1 ratio is suggested by the developers of SPEA to maintain an adequate selection pressure for the elite solutions), so that overall population size becomes 100. SPEA is also run until 25 000 function evaluations are done. For SPEA, we use the Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:25:36 UTC from IEEE Xplore. Restrictions apply. 188 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 6, NO. 2, APRIL 2002 Fig. 3. Distance metric \u0007. nondominated solutions of the combined GA and external populations at the final generation to calculate the performance metrics used in this study. For PAES, SPEA, and binary-coded NSGA-II, we have used 30 bits to code each decision variable. B. Performance Measures Unlike in single-objective optimization, there are two goals in a multiobjective optimization: 1) convergence to the Pareto-optimal set and 2) maintenance of diversity in solutions of the Pareto-optimal set. These two tasks cannot be measured adequately with one performance metric. Many performance metrics have been suggested [1], [8], [24]. Here, we define two performance metrics that are more direct in evaluating each of the above two goals in a solution set obtained by a multiobjective optimization algorithm. The first metric measures the extent of convergence to a known set of Pareto-optimal solutions. Since multiobjective algorithms would be tested on problems having a known set of Pareto-optimal solutions, the calculation of this metric is possible. We realize, however, that such a metric cannot be used for any arbitrary problem. First, we find a set of uniformly spaced solutions from the true Pareto-optimal front in the objective space. For each solution obtained with an algorithm, we compute the minimum Euclidean distance of it from chosen solutions on the Pareto-optimal front. The average of these distances is used as the first metric (the convergence metric). Fig. 3 shows the calculation procedure of this metric. The shaded region is the feasible search region and the solid curved lines specify the Pareto-optimal solutions. Solutions with open circles are chosen solutions on the Pareto-optimal front for the calculation of the convergence metric and solutions marked with dark circles are solutions obtained by an algorithm. The smaller the value of this metric, the better the convergence toward the Pareto-optimal front. When all obtained solutions lie exactly on chosen solutions, this metric takes a value of zero. In all simulations performed here, we present the average and variance of this metric calculated for solution sets obtained in multiple runs. Even when all solutions converge to the Pareto-optimal front, the above convergence metric does not have a value of zero. The metric will yield zero only when each obtained solution lies exactly on each of the chosen solutions. Although this metric alone Fig. 4. Diversity metric \u0001. can provide some information about the spread in obtained solutions, we define an different metric to measure the spread in solutions obtained by an algorithm directly. The second metric measures the extent of spread achieved among the obtained solutions. Here, we are interested in getting a set of solutions that spans the entire Pareto-optimal region. We calculate the Euclidean distance between consecutive solutions in the obtained nondominated set of solutions. We calculate the average of these distances. Thereafter, from the obtained set of nondominated solutions, we first calculate the extreme solutions (in the objective space) by fitting a curve parallel to that of the true Pareto-optimal front. Then, we use the following metric to calculate the nonuniformity in the distribution: (1) Here, the parameters and are the Euclidean distances between the extreme solutions and the boundary solutions of the obtained nondominated set, as depicted in Fig. 4. The figure illustrates all distances mentioned in the above equation. The parameter is the average of all distances , , assuming that there are solutions on the best nondominated front. With solutions, there are consecutive distances. The denominator is the value of the numerator for the case when all solutions lie on one solution. It is interesting to note that this is not the worst case spread of solutions possible. We can have a scenario in which there is a large variance in . In such scenarios, the metric may be greater than one. Thus, the maximum value of the above metric can be greater than one. However, a good distribution would make all distances equal to and would make (with existence of extreme solutions in the nondominated set). Thus, for the most widely and uniformly spreadout set of nondominated solutions, the numerator of would be zero, making the metric to take a value zero. For any other distribution, the value of the metric would be greater than zero. For two distributions having identical values of and , the metric takes a higher value with worse distributions of solutions within the extreme solutions. Note that the above diversity metric can be used on any nondominated set of solutions, including one that is not the Pareto-optimal set. Using Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:25:36 UTC from IEEE Xplore. Restrictions apply. DEB et al.: A FAST AND ELITIST MULTIOBJECTIVE GA: NSGA-II 189 TABLE II MEAN (FIRST ROWS) AND VARIANCE (SECOND ROWS) OF THE CONVERGENCE METRIC \u0007 TABLE III MEAN (FIRST ROWS) AND VARIANCE (SECOND ROWS) OF THE DIVERSITY METRIC \u0001 a triangularization technique or a Voronoi diagram approach [1] to calculate , the above procedure can be extended to estimate the spread of solutions in higher dimensions. C. Discussion of the Results Table II shows the mean and variance of the convergence metric obtained using four algorithms NSGA-II (real-coded), NSGA-II (binary-coded), SPEA, and PAES. NSGA-II (real coded or binary coded) is able to converge better in all problems except in ZDT3 and ZDT6, where PAES found better convergence. In all cases with NSGA-II, the variance in ten runs is also small, except in ZDT4 with NSGA-II (binary coded). The fixed archive strategy of PAES allows better convergence to be achieved in two out of nine problems. Table III shows the mean and variance of the diversity metric obtained using all three algorithms. NSGA-II (real or binary coded) performs the best in all nine test problems. The worst performance is observed with PAES. For illustration, we show one of the ten runs of PAES with an arbitrary run of NSGA-II (real-coded) on problem SCH in Fig. 5. On most problems, real-coded NSGA-II is able to find a better spread of solutions than any other algorithm, including binary-coded NSGA-II. In order to demonstrate the working of these algorithms, we also show typical simulation results of PAES, SPEA, and NSGA-II on the test problems KUR, ZDT2, ZDT4, and ZDT6. The problem KUR has three discontinuous regions in the Pareto-optimal front. Fig. 6 shows all nondominated solutions obtained after 250 generations with NSGA-II (real-coded). The Pareto-optimal region is also shown in the figure. This figure demonstrates the abilities of NSGA-II in converging to the true front and in finding diverse solutions in the front. Fig. 7 shows the obtained nondominated solutions with SPEA, which is the next-best algorithm for this problem (refer to Tables II and III). Fig. 5. NSGA-II finds better spread of solutions than PAES on SCH. Fig. 6. Nondominated solutions with NSGA-II (real-coded) on KUR. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:25:36 UTC from IEEE Xplore. Restrictions apply. 190 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 6, NO. 2, APRIL 2002 Fig. 7. Nondominated solutions with SPEA on KUR. Fig. 8. Nondominated solutions with NSGA-II (binary-coded) on ZDT2. In both aspects of convergence and distribution of solutions, NSGA-II performed better than SPEA in this problem. Since SPEA could not maintain enough nondominated solutions in the final GA population, the overall number of nondominated solutions is much less compared to that obtained in the final population of NSGA-II. Next, we show the nondominated solutions on the problem ZDT2 in Figs. 8 and 9. This problem has a nonconvex Pareto-optimal front.We show the performance of binary-coded NSGA-II and SPEA on this function. Although the convergence is not a difficulty here with both of these algorithms, both real- and binary-coded NSGA-II have found a better spread and more solutions in the entire Pareto-optimal region than SPEA (the next-best algorithm observed for this problem). The problem ZDT4 has 21 or 7.94(10 ) different local Pareto-optimal fronts in the search space, of which only one corresponds to the global Pareto-optimal front. The Euclidean distance in the decision space between solutions of two consecutive local Pareto-optimal sets is 0.25. Fig. 10 shows that both real-coded NSGA-II and PAES get stuck at different local Pareto-optimal sets, but the convergence and ability to find a diverse set of solutions are definitely better with NSGA-II. Binary-coded GAs have difficulties in converging Fig. 9. Nondominated solutions with SPEA on ZDT2. Fig. 10. NSGA-II finds better convergence and spread of solutions than PAES on ZDT4. near the global Pareto-optimal front, a matter that is also been observed in previous single-objective studies [5]. On a similar ten-variable Rastrigin’s function [the function here], that study clearly showed that a population of size of about at least 500 is needed for single-objective binary-coded GAs (with tournament selection, single-point crossover and bitwise mutation) to find the global optimum solution in more than 50% of the simulation runs. Since we have used a population of size 100, it is not expected that a multiobjective GA would find the global Pareto-optimal solution, but NSGA-II is able to find a good spread of solutions even at a local Pareto-optimal front. Since SPEA converges poorly on this problem (see Table II), we do not show SPEA results on this figure. Finally, Fig. 11 shows that SPEA finds a better converged set of nondominated solutions in ZDT6 compared to any other algorithm. However, the distribution in solutions is better with real-coded NSGA-II. D. Different Parameter Settings In this study, we do not make any serious attempt to find the best parameter setting for NSGA-II. But in this section, we per- Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:25:36 UTC from IEEE Xplore. Restrictions apply. DEB et al.: A FAST AND ELITIST MULTIOBJECTIVE GA: NSGA-II 191 Fig. 11. Real-coded NSGA-II finds better spread of solutions than SPEA on ZDT6, but SPEA has a better convergence. TABLE IV MEAN AND VARIANCE OF THE CONVERGENCE AND DIVERSITY METRICS UP TO 500 GENERATIONS form additional experiments to show the effect of a couple of different parameter settings on the performance of NSGA-II. First, we keep all other parameters as before, but increase the number of maximum generations to 500 (instead of 250 used before). Table IV shows the convergence and diversity metrics for problems POL, KUR, ZDT3, ZDT4, and ZDT6. Now, we achieve a convergence very close to the true Pareto-optimal front and with a much better distribution. The table shows that in all these difficult problems, the real-coded NSGA-II has converged very close to the true optimal front, except in ZDT6, which probably requires a different parameter setting with NSGA-II. Particularly, the results on ZDT3 and ZDT4 improve with generation number. The problem ZDT4 has a number of local Pareto-optimal fronts, each corresponding to particular value of . A large change in the decision vector is needed to get out of a local optimum. Unless mutation or crossover operators are capable of creating solutions in the basin of another better attractor, the improvement in the convergence toward the true Pareto-optimal front is not possible.We use NSGA-II (real-coded) with a smaller distribution index for mutation, which has an effect of creating solutions with more spread than before. Rest of the parameter settings are identical as before. The convergence metric and diversity measure on problem ZDT4 at the end of 250 generations are as follows: Fig. 12. Obtained nondominated solutions with NSGA-II on problem ZDT4. These results are much better than PAES and SPEA, as shown in Table II. To demonstrate the convergence and spread of solutions, we plot the nondominated solutions of one of the runs after 250 generations in Fig. 12. The figure shows that NSGA-II is able to find solutions on the true Pareto-optimal front with . V. ROTATED PROBLEMS It has been discussed in an earlier study [3] that interactions among decision variables can introduce another level of difficulty to any multiobjective optimization algorithm including EAs. In this section, we create one such problem and investigate the working of previously three MOEAs on the following epistatic problem: minimize minimize where and for (2) An EA works with the decision variable vector , but the above objective functions are defined in terms of the variable vector , which is calculated by transforming the decision variable vector by a fixed rotation matrix . This way, the objective functions are functions of a linear combination of decision variables. In order to maintain a spread of solutions over the Pareto-optimal region or even converge to any particular solution requires an EA to update all decision variables in a particular fashion.With a generic search operator, such as the variablewise SBX operator used here, this becomes a difficult task for an EA. However, here, we are interested in evaluating the overall behavior of three elitist MOEAs. We use a population size of 100 and run each algorithm until 500 generations. For SBX, we use and we use for mutation. To restrict the Pareto-optimal solutions to lie Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:25:36 UTC from IEEE Xplore. Restrictions apply. 192 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 6, NO. 2, APRIL 2002 Fig. 13. Obtained nondominated solutions with NSGA-II, PAES, and SPEA on the rotated problem. within the prescribed variable bounds, we discourage solutions with by adding a fixed large penalty to both objectives. Fig. 13 shows the obtained solutions at the end of 500 generations using NSGA-II, PAES, and SPEA. It is observed that NSGA-II solutions are closer to the true front compared to solutions obtained by PAES and SPEA. The correlated parameter updates needed to progress toward the Pareto-optimal front makes this kind of problems difficult to solve. NSGA-II’s elite-preserving operator along with the real-coded crossover and mutation operators is able to find some solutions close to the Pareto-optimal front [with resulting ]. This example problem demonstrates that one of the known difficulties (the linkage problem [11], [12]) of single-objective optimization algorithm can also cause difficulties in a multiobjective problem. However, more systematic studies are needed to amply address the linkage issue in multiobjective optimization. VI. CONSTRAINT HANDLING In the past, the first author and his students implemented a penalty-parameterless constraint-handling approach for singleobjective optimization. Those studies [2], [6] have shown how a tournament selection based algorithm can be used to handle constraints in a population approach much better than a number of other existing constraint-handling approaches. A similar approach can be introduced with the above NSGA-II for solving constrained multiobjective optimization problems. A. Proposed Constraint-Handling Approach (Constrained NSGA-II) This constraint-handling method uses the binary tournament selection, where two solutions are picked from the population and the better solution is chosen. In the presence of constraints, each solution can be either feasible or infeasible. Thus, there may be at most three situations: 1) both solutions are feasible; 2) one is feasible and other is not; and 3) both are infeasible. For single objective optimization, we used a simple rule for each case. Case 1) Choose the solution with better objective function value. Case 2) Choose the feasible solution. Case 3) Choose the solution with smaller overall constraint violation. Since in no case constraints and objective function values are compared with each other, there is no need of having any penalty parameter, a matter that makes the proposed constraint-handling approach useful and attractive. In the context of multiobjective optimization, the latter two cases can be used as they are and the first case can be resolved by using the crowded-comparison operator as before. To maintain the modularity in the procedures of NSGA-II, we simply modify the definition of domination between two solutions and . Definition 1: A solution is said to constrained-dominate a solution , if any of the following conditions is true. 1) Solution is feasible and solution is not. 2) Solutions and are both infeasible, but solution has a smaller overall constraint violation. 3) Solutions and are feasible and solution dominates solution . The effect of using this constrained-domination principle is that any feasible solution has a better nondomination rank than any infeasible solution. All feasible solutions are ranked according to their nondomination level based on the objective function values. However, among two infeasible solutions, the solution with a smaller constraint violation has a better rank. Moreover, this modification in the nondomination principle does not change the computational complexity of NSGA-II. The rest of the NSGA-II procedure as described earlier can be used as usual. The above constrained-domination definition is similar to that suggested by Fonseca and Fleming [9]. The only difference is in the way domination is defined for the infeasible solutions. In the above definition, an infeasible solution having a larger overall constraint-violation are classified as members of a larger nondomination level. On the other hand, in [9], infeasible solutions violating different constraints are classified as members of the same nondominated front. Thus, one infeasible solution violating a constraint marginally will be placed in the same nondominated level with another solution violating a different constraint to a large extent. This may cause an algorithm to wander in the infeasible search region for more generations before reaching the feasible region through constraint boundaries. Moreover, since Fonseca–Fleming’s approach requires domination checks with the constraint-violation values, the proposed approach of this paper is computationally less expensive and is simpler. B. Ray–Tai–Seow’s Constraint-Handling Approach Ray et al. [17] suggested a more elaborate constraint-handling technique, where constraint violations of all constraints are not simply summed together. Instead, a nondomination check of constraint violations is also made. We give an outline of this procedure here. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:25:36 UTC from IEEE Xplore. Restrictions apply. DEB et al.: A FAST AND ELITIST MULTIOBJECTIVE GA: NSGA-II 193 TABLE V CONSTRAINED TEST PROBLEMS USED IN THIS STUDY All objective functions are to be minimized. Three different nondominated rankings of the population are first performed. The first ranking is performed using objective function values and the resulting ranking is stored in a -dimensional vector . The second ranking is performed using only the constraint violation values of all ( of them) constraints and no objective function information is used. Thus, constraint violation of each constraint is used a criterion and a nondomination classification of the population is performed with the constraint violation values. Notice that for a feasible solution all constraint violations are zero. Thus, all feasible solutions have a rank 1 in . The third ranking is performed on a combination of objective functions and constraint-violation values [a total of values]. This produces the ranking . Although objective function values and constraint violations are used together, one nice aspect of this algorithm is that there is no need for any penalty parameter. In the domination check, criteria are compared individually, thereby eliminating the need of any penalty parameter. Once these rankings are over, all feasible solutions having the best rank in are chosen for the new population. If more population slots are available, they are created from the remaining solutions systematically. By giving importance to the ranking in in the selection operator and by giving importance to the ranking in in the crossover operator, the investigators laid out a systematic multiobjective GA, which also includes a niche-preserving operator. For details, readers may refer to [17]. Although the investigators did not compare their algorithm with any other method, they showed the working of this constraint-handling method on a number of engineering design problems. However, since nondominated sorting of three different sets of criteria are required and the algorithm introduces many different operators, it remains to be investigated how it performs on more complex problems, particularly from the point of view of computational burden associated with the method. In the following section, we choose a set of four problems and compare the simple constrained NSGA-II with the Ray–Tai–Seow’s method. C. Simulation Results We choose four constrained test problems (see Table V) that have been used in earlier studies. In the first problem, a part of the unconstrained Pareto-optimal region is not feasible. Thus, the resulting constrained Pareto-optimal region is a concatenation of the first constraint boundary and some part of the unconstrained Pareto-optimal region. The second problem SRN was used in the original study of NSGA [20]. Here, the constrained Pareto-optimal set is a subset of the unconstrained Pareto-optimal set. The third problem TNK was suggested by Tanaka et al. [21] and has a discontinuous Pareto-optimal region, falling entirely on the first constraint boundary. In the next section, we show the constrained Pareto-optimal region for each of the above problems. The fourth problem WATER is a five-objective and seven-constraint problem, attempted to solve in [17]. With five objectives, it is difficult to discuss the effect of the constraints on the unconstrained Pareto-optimal region. In the next section, we show all or ten pairwise plots of obtained nondominated solutions. We apply real-coded NSGA-II here. In all problems, we use a population size of 100, distribution indexes for real-coded crossover and mutation operators of 20 and 100, respectively, and run NSGA-II (real coded) with the proposed constraint-handling technique and with Ray–Tai–Seow’s constraint-handling algorithm [17] for a maximum of 500 generations. We choose this rather large number of generations to investigate if the spread in solutions Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:25:36 UTC from IEEE Xplore. Restrictions apply. 194 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 6, NO. 2, APRIL 2002 Fig. 14. Obtained nondominated solutions with NSGA-II on the constrained problem CONSTR. Fig. 15. Obtained nondominated solutions with Ray-Tai-Seow’s algorithm on the constrained problem CONSTR. can be maintained for a large number of generations. However, in each case, we obtain a reasonably good spread of solutions as early as 200 generations. Crossover and mutation probabilities are the same as before. Fig. 14 shows the obtained set of 100 nondominated solutions after 500 generations using NSGA-II. The figure shows that NSGA-II is able to uniformly maintain solutions in both Pareto-optimal region. It is important to note that in order to maintain a spread of solutions on the constraint boundary, the solutions must have to be modified in a particular manner dictated by the constraint function. This becomes a difficult task of any search operator. Fig. 15 shows the obtained solutions using Ray-Tai-Seow’s algorithm after 500 generations. It is clear that NSGA-II performs better than Ray–Tai–Seow’s algorithm in terms of converging to the true Pareto-optimal front and also in terms of maintaining a diverse population of nondominated solutions. Next, we consider the test problem SRN. Fig. 16 shows the nondominated solutions after 500 generations using NSGA-II. Fig. 16. Obtained nondominated solutions with NSGA-II on the constrained problem SRN. Fig. 17. Obtained nondominated solutions with Ray–Tai–Seow’s algorithm on the constrained problem SRN. The figure shows how NSGA-II can bring a random population on the Pareto-optimal front. Ray–Tai–Seow’s algorithm is also able to come close to the front on this test problem (Fig. 17). Figs. 18 and 19 show the feasible objective space and the obtained nondominated solutions with NSGA-II and Ray–Tai–Seow’s algorithm. Here, the Pareto-optimal region is discontinuous and NSGA-II does not have any difficulty in finding a wide spread of solutions over the true Pareto-optimal region. Although Ray–Tai–Seow’s algorithm found a number of solutions on the Pareto-optimal front, there exist many infeasible solutions even after 500 generations. In order to demonstrate the working of Fonseca–Fleming’s constraint-handling strategy, we implement it with NSGA-II and apply on TNK. Fig. 20 shows 100 population members at the end of 500 generations and with identical parameter setting as used in Fig. 18. Both these figures demonstrate that the proposed and Fonseca–Fleming’s constraint-handling strategies work well with NSGA-II. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:25:36 UTC from IEEE Xplore. Restrictions apply. DEB et al.: A FAST AND ELITIST MULTIOBJECTIVE GA: NSGA-II 195 Fig. 18. Obtained nondominated solutions with NSGA-II on the constrained problem TNK. Fig. 19. Obtained nondominated solutions with Ray–Tai–Seow’s algorithm on the constrained problem TNK. Ray et al. [17] have used the problem WATER in their study. They normalized the objective functions in the following manner: Since there are five objective functions in the problemWATER, we observe the range of the normalized objective function values of the obtained nondominated solutions. Table VI shows the comparison with Ray–Tai–Seow’s algorithm. In most objective functions, NSGA-II has found a better spread of solutions than Ray–Tai–Seow’s approach. In order to show the pairwise interactions among these five normalized objective functions, we plot all or ten interactions in Fig. 21 for both algorithms. NSGA-II results are shown in the upper diagonal portion of the figure and the Ray–Tai–Seow’s results are shown in the lower diagonal portion. The axes of any plot can be obtained by looking at the corresponding diagonal boxes and their ranges. For example, the plot at the first row and third column has its vertical axis as and horizontal axis as . Since this plot belongs in the upper side of the diagonal, this Fig. 20. Obtained nondominated solutions with Fonseca–Fleming’s constraint-handling strategy with NSGA-II on the constrained problem TNK. plot is obtained using NSGA-II. In order to compare this plot with a similar plot using Ray–Tai–Seow’s approach, we look for the plot in the third row and first column. For this figure, the vertical axis is plotted as and the horizontal axis is plotted as . To get a better comparison between these two plots, we observe Ray–Tai–Seow’s plot as it is, but turn the page 90 in the clockwise direction for NSGA-II results. This would make the labeling and ranges of the axes same in both cases. We observe that NSGA-II plots have better formed patterns than in Ray–Tai–Seow’s plots. For example, figures - , - , and - interactions are very clear from NSGA-II results. Although similar patterns exist in the results obtained using Ray–Tai–Seow’s algorithm, the convergence to the true fronts is not adequate. VII. CONCLUSION We have proposed a computationally fast and elitist MOEA based on a nondominated sorting approach. On nine different difficult test problems borrowed from the literature, the proposed NSGA-II was able to maintain a better spread of solutions and converge better in the obtained nondominated front compared to two other elitist MOEAs—PAES and SPEA. However, one problem, PAES, was able to converge closer to the true Pareto-optimal front. PAES maintains diversity among solutions by controlling crowding of solutions in a deterministic and prespecified number of equal-sized cells in the search space. In that problem, it is suspected that such a deterministic crowding coupled with the effect of mutation-based approach has been beneficial in converging near the true front compared to the dynamic and parameterless crowding approach used in NSGA-II and SPEA. However, the diversity preserving mechanism used in NSGA-II is found to be the best among the three approaches studied here. On a problem having strong parameter interactions, NSGA-II has been able to come closer to the true front than the other two approaches, but the important matter is that all three approaches faced difficulties in solving this so-called highly epistatic problem. Although this has been a matter of ongoing Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:25:36 UTC from IEEE Xplore. Restrictions apply. 196 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 6, NO. 2, APRIL 2002 TABLE VI LOWER AND UPPER BOUNDS OF THE OBJECTIVE FUNCTION VALUES OBSERVED IN THE OBTAINED NONDOMINATED SOLUTIONS Fig. 21. Upper diagonal plots are for NSGA-II and lower diagonal plots are for Ray–Tai–Seow’s algorithm. Compare (i; j) plot (Ray–Tai–Seow’s algorithm with i > j) with (j; i) plot (NSGA-II). Label and ranges used for each axis are shown in the diagonal boxes. research in single-objective EA studies, this paper shows that highly epistatic problems may also cause difficulties to MOEAs. More importantly, researchers in the field should consider such epistatic problems for testing a newly developed algorithm for multiobjective optimization. We have also proposed a simple extension to the definition of dominance for constrained multiobjective optimization. Although this new definition can be used with any other MOEAs, the real-coded NSGA-II with this definition has been shown to solve four different problems much better than another recently- proposed constraint-handling approach. With the properties of a fast nondominated sorting procedure, an elitist strategy, a parameterless approach and a simple yet efficient constraint-handling method, NSGA-II, should find increasing attention and applications in the near future. REFERENCES [1] K. Deb, Multiobjective Optimization Using Evolutionary Algorithms. Chichester, U.K.: Wiley, 2001. [2] , “An efficient constraint-handling method for genetic algorithms,” Comput. Methods Appl. Mech. Eng., vol. 186, no. 2–4, pp. 311–338, 2000. [3] , “Multiobjective genetic algorithms: Problem difficulties and construction of test functions,” in Evol. Comput., 1999, vol. 7, pp. 205–230. [4] K. Deb and D. E. Goldberg, “An investigation of niche and species formation in genetic function optimization,” in Proceedings of the Third International Conference on Genetic Algorithms, J. D. Schaffer, Ed. San Mateo, CA: Morgan Kauffman, 1989, pp. 42–50. [5] K. Deb and S. Agrawal, “Understanding interactions among genetic algorithm parameters,” in Foundations of Genetic Algorithms V, W. Banzhaf and C. Reeves, Eds. San Mateo, CA: Morgan Kauffman, 1998, pp. 265–286. [6] K. Deb and R. B. Agrawal, “Simulated binary crossover for continuous search space,” in Complex Syst., Apr. 1995, vol. 9, pp. 115–148. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:25:36 UTC from IEEE Xplore. Restrictions apply. DEB et al.: A FAST AND ELITIST MULTIOBJECTIVE GA: NSGA-II 197 [7] C. M. Fonseca and P. J. Fleming, “Genetic algorithms for multiobjective optimization: Formulation, discussion and generalization,” in Proceedings of the Fifth International Conference on Genetic Algorithms, S. Forrest, Ed. San Mateo, CA: Morgan Kauffman, 1993, pp. 416–423. [8] , “On the performance assessment and comparison of stochastic multiobjective optimizers,” in Parallel Problem Solving from Nature IV, H.-M. Voigt, W. Ebeling, I. Rechenberg, and H.-P. Schwefel, Eds. Berlin, Germany: Springer-Verlag, 1996, pp. 584–593. [9] , “Multiobjective optimization and multiple constraint handling with evolutionary algorithms—Part I: A unified formulation,” IEEE Trans. Syst., Man, Cybern. A, vol. 28, pp. 26–37, Jan. 1998. [10] , “Multiobjective optimization and multiple constraint handling with evolutionary algorithms—Part II: Application example,” IEEE Trans. Syst., Man, Cybern. A, vol. 28, pp. 38–47, Jan. 1998. [11] D. E. Goldberg, B. Korb, and K. Deb, “Messy genetic algorithms: Motivation, analysis, and first results,” in Complex Syst., Sept. 1989, vol. 3, pp. 93–530. [12] G. Harik, “Learning gene linkage to efficiently solve problems of bounded difficulty using genetic algorithms,” llinois Genetic Algorithms Lab., Univ. Illinois at Urbana-Champaign, Urbana, IL, IlliGAL Rep. 97005, 1997. [13] J. Horn, N. Nafploitis, and D. E. Goldberg, “A niched Pareto genetic algorithm for multiobjective optimization,” in Proceedings of the First IEEE Conference on Evolutionary Computation, Z. Michalewicz, Ed. Piscataway, NJ: IEEE Press, 1994, pp. 82–87. [14] J. Knowles and D. Corne, “The Pareto archived evolution strategy: A new baseline algorithm for multiobjective optimization,” in Proceedings of the 1999 Congress on Evolutionary Computation. Piscataway, NJ: IEEE Press, 1999, pp. 98–105. [15] F. Kursawe, “A variant of evolution strategies for vector optimization,” in Parallel Problem Solving from Nature, H.-P. Schwefel and R. Männer, Eds. Berlin, Germany: Springer-Verlag, 1990, pp. 193–197. [16] C. Poloni, “Hybrid GA for multiobjective aerodynamic shape optimization,” in Genetic Algorithms in Engineering and Computer Science, G. Winter, J. Periaux, M. Galan, and P. Cuesta, Eds. New York: Wiley, 1997, pp. 397–414. [17] T. Ray, K. Tai, and C. Seow, “An evolutionary algorithm for multiobjective optimization,” Eng. Optim., vol. 33, no. 3, pp. 399–424, 2001. [18] G. Rudolph, “Evolutionary search under partially ordered sets,” Dept. Comput. Sci./LS11, Univ. Dortmund, Dortmund, Germany, Tech. Rep. CI-67/99, 1999. [19] J. D. Schaffer, “Multiple objective optimization with vector evaluated genetic algorithms,” in Proceedings of the First International Conference on Genetic Algorithms, J. J. Grefensttete, Ed. Hillsdale, NJ: Lawrence Erlbaum, 1987, pp. 93–100. [20] N. Srinivas and K. Deb, “Multiobjective function optimization using nondominated sorting genetic algorithms,” Evol. Comput., vol. 2, no. 3, pp. 221–248, Fall 1995. [21] M. Tanaka, “GA-based decision support system for multicriteria optimization,” in Proc. IEEE Int. Conf. Systems, Man and Cybernetics-2, 1995, pp. 1556–1561. [22] D.Van Veldhuizen, “Multiobjective evolutionary algorithms: Classifications, analyzes, and new innovations,” Air Force Inst. Technol., Dayton, OH, Tech. Rep. AFIT/DS/ENG/99-01, 1999. [23] D. Van Veldhuizen and G. Lamont, “Multiobjective evolutionary algorithm research: A history and analysis,” Air Force Inst. Technol., Dayton, OH, Tech. Rep. TR-98-03, 1998. [24] E. Zitzler, “Evolutionary algorithms for multiobjective optimization: Methods and applications,” Doctoral dissertation ETH 13398, Swiss Federal Institute of Technology (ETH), Zurich, Switzerland, 1999. [25] E. Zitzler, K. Deb, and L. Thiele, “Comparison of multiobjective evolutionary algorithms: Empirical results,” Evol. Comput., vol. 8, no. 2, pp. 173–195, Summer 2000. [26] E. Zitzler and L. Thiele, “Multiobjective optimization using evolutionary algorithms—A comparative case study,” in Parallel Problem Solving From Nature, V, A. E. Eiben, T. Bäck, M. Schoenauer, and H.-P. Schwefel, Eds. Berlin, Germany: Springer-Verlag, 1998, pp. 292–301. Kalyanmoy Deb (A’02) received the B.Tech degree in mechanical engineering from the Indian Institute of Technology, Kharagpur, India, 1985 and the M.S. and Ph.D. degrees in engineering mechanics from the University of Alabama, Tuscaloosa, in 1989 and 1991, respectively. He is currently a Professor of Mechanical Engineering with the Indian Institute of Technology, Kanpur, India. He has authored or coauthored over 100 research papers in journals and conferences, a number of book chapters, and two books: Multiobjective Optimization Using Evolutionary Algorithms (Chichester, U.K.: Wiley, 2001) and Optimization for Engineering Design (New Delhi, India: Prentice-Hall, 1995). His current research interests are in the field of evolutionary computation, particularly in the areas of multicriterion and real-parameter evolutionary algorithms. Dr. Deb is an Associate Editor of IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION and an Executive Council Member of the International Society on Genetic and Evolutionary Computation. Amrit Pratap was born in Hyderabad, India, on August 27, 1979. He received the M.S. degree in mathematics and scientific computing from the Indian Institute of Technology, Kanpur, India, in 2001. He is working toward the Ph.D. degree in computer science at the California Institute of Technology, Pasadena, CA. He was a member of the Kanpur Genetic Algorithms Laboratory. He is currently a Member of the Caltech Learning Systems Group. His current research interests include evolutionary computation, machine learning, and neural networks. Sameer Agarwal was born in Bulandshahar, India, on February 19, 1977. He received the M.S. degree in mathematics and scientific computing from the Indian Institute of Technology, Kanpur, India, in 2000. He is working toward the Ph.D. degree in computer science at University of California, San Diego. He was a Member of the Kanpur Genetic Algorithms Laboratory. His research interests include evolutionary computation and learning both in humans as well as machines. He is currently developing learning methods for learning by imitation. T. Meyarivan was born in Haldia, India, on November 23, 1977. He is working toward the M.S. degree in chemistry from Indian Institute of Technology, Kanpur, India. He is a Member of the Kanpur Genetic Algorithms Laboratory. His current research interests include evolutionary computation and its applications to biology and various fields in chemistry. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:25:36 UTC from IEEE Xplore. Restrictions apply.\"],\n",
    "[2,1,1,\"Genetic algorithms provide an alternative to traditional optimization techniques by using directed random searches to locate optimal solutions in complex landscapes. This article traces GA research. Genetic Algorithms: A Survey M. Srinivas, Motorola India Electronics Ltd. Lalit M. Patnaik, Indian Institute of Science n the last five years, genetic algorithms have emerged as practical, robust optimization and search methods. Diverse areas such as music generation, genetic synthesis, VLSI technology, strategy planning, and machine learning have profited from these methods. The popularity of genetic algorithms is reflected in three biennial conferences, a new international journal, and an ever-increasing mass of literature devoted to the theory, practice, and applications of such techniques (see the sidebar “To learn more”). Genetic algorithm search methods are rooted in the mechanisms of evolution and natural genetics. The interest in heuristic search algorithms with underpinnings in natural and physical processes began as early as the 1970s, when Holland’ first proposed genetic algorithms. This interest was rekindled by Kirkpatrick, Gelatt, and Vecchi’s simulated annealing technique in 1983.2 Simulated annealing is based on thermodynamic considerations, with annealing interpreted as an optimization procedure. Evolutionary ~ t r a t e g i e sa~n.d~ g enetic algorithm^,^.'^ on the other hand, draw inspiration from the natural search and selection processes leading to the survival of the fittest individuals. Simulated annealing, genetic algorithms, and evolutionary strategies are similar in their use of a probabilistic search mechanism directed toward decreasing cost or increasing payoff. These three methods have a high probability of locating the global solution optimally in a multimodal search landscape. (A multimodal cost function has several locally optimal solutions as well.) However, each method has a significantly different mode of operation. Simulated annealing probabilistically generates a sequence of states based on a cooling schedule to ultimately converge to the global optimum. Evolutionary strategies use mutations as search mechanisms and selection to direct the search toward the prospective regions in the search space. Genetic algorithms generate a sequence of populations by using a selection mechanism, and use crossover and mutation as search mechanisms. The principal difference between genetic algorithms and evolutionary strategies is that genetic algorithms rely on crossover, a mechanism of probabilistic and useful exchange of information among solutions, to locate better solutions, while evolutionary strategies use mutation as the primary search mechanism. Although Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:26:34 UTC from IEEE Xplore. Restrictions apply. simulated annealing, evolutionary strategies, and genetic algorithms have basically different approaches. several hybrids of these techniques that narrow the distinctions among them have been proposed in recent literature. Other optimization algorithms derived from the evolutionary paradigm have also demonstrated considerable success. In this article we introduce the art and science of genetic algorithms and survey current issues in GA theory and practice. We do not present a detailed study, since several wonderful texts on GAS already e~ist.’.~.’‘I.n’~st ead, we offer a quick guide into the labyrinth of GA research. To start, we draw the analogy between genetic algorithms and the search processes in nature. Then we describe the genetic algorithm that Holland introduced in 1975 and the workings of GAS. After a survey of techniques proposed as improvements to Holland’s GA and of some radically different approaches, we survey the advances in GA theory related to modeling. dynamics, and deception. Genetic algorithms and natural selection In nature. individuals best suited to competition for scanty resources survive. Adapting to a changing environment is essential for the survival of individuals of each species. While the various features that uniquely characterize an individual determine its survival capacity. the features in turn are determined by the individual’s genetic content. Specifically, each feature is controlled by a basic unit called a gene. The sets of genes controlling features form the chromosomes, the “keys” to the survival of the individual in a competitive environment. Although evolution manifests itself as a succession of changes in species’ features, it is the changes in the species’ genetic material that form the essence of evolution. Specifically, evolution’s driving force is the joint action of natural selection and the recombination of genetic material that occurs during reproduction. In nature, competition among individuals for scant resources such as food and space and for mates results in the fittest individuals dominating over weaker ones. Only the fittest individuals survive and reproduce, a natural phenomenon called “the survival of the fittest.” Hence, the genes of the fittest survive, while the genes of weaker individuals die out. Natural selection leads to the survival of the fittest individuals, but it also implicitly leads to the survival of the fittest genes. The reproduction process generates diversity in the gene pool. Evolution is initiated when the genetic material (chromosomes) from two parents recombines during reproduction. New combinations To learn more Readers wishing to pursue information on genetic algorithms may be interested in the following materials. nd Applications, Lawrence E of genes are generated from previous ones; a new gene pool is created. Specifically, the exchange of genetic material among chromosomes is called crossover. Segments of the two parent chromosomes are exchanged during crossover, creating the possibility of the “right” combination of genes for better individuals. Repeated selection and crossover cause the continuous evolution of the gene pool and the generation of individuals that survive better in a competitive environment. Holland’ proposed genetic algorithms in the early 1970s as computer programs that mimic the evolutionary processes in nature. Genetic algorithms manipulate a population of potential solutions to an optimization (or search) problem. Specifically, they operate on encoded representations of the solutions, equivalent to the genetic material of individuals in nature, and not directly on the solutions themselves. Holland’s genetic algorithm encodes the solutions as strings of bits from a binary alphabet. As in nature, selection provides the necessary driving mechanism for better solutions to survive. Each solution is associated with a fitness vulue that reflects how good it is, compared with other solutions in the population. The higher the fitness value of an individual, the higher its chances of survival and reproduction and the larger its representation in the subsequent generation. Recombination of genetic material in genetic algorithms is simulated through a crossover mechanism that exchanges portions between strings. Another operation, called mutation, causes sporadic and random alteration of the bits of strings. Mutation too has a direct analogy from nature and plays the role of regenerating lost genetic material. A simple genetic algorithm In the literature, Holland’s genetic algorithm is commonly called the Simple Genetic Algorithm or SGA. Essential to the SGA’s working is a population of binary strings. Each string of Os and Is is the encoded version of a solution to the optimization problem. Using genetic operators - crossover and mutation -the algorithm creates the subsequent generation from the strings of the current population. This generational cycle is re- 18 COMPUTER Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:26:34 UTC from IEEE Xplore. Restrictions apply. peated until a desired termination criterion is reached (for example. a predefined number of generations are processed). Figure 1 summarizes the working of the SGA. which has the following components: 0 a population of binary strings, control parameters. a fitness function, 0 genetic operators (crossover and mua selection mechanism, and 0 a mechanism to encode the solutions tation), as binary strings. Encoding mechanism. Fundamental to the GA structure is the encoding mechanism for representing the optimization problem's variables. The encoding mechanism depends on the nature of the problem variables. For example. when solving for the optimal flows in a transportation problem. the variables (flows in different channels) assume continuous values, while the variables in a travelingsalesman problem are binary quantities representing the inclusion or exclusion of an edge in the Hamiltonian circuit. In each case the encoding mechanism should map each solution to a unique binary string. A large number of optimization problems have real-valued continuous variables. A common method of encoding them uses their integer representation. Each variable is first linearly mapped to an integer defined in a specified range, and the integer is encoded using a fixed number of binary bits. The binary codes of all the variables are then concatenated to obtain a binary string. For example, consider a continuous variable defined in a range from -1.28 to 1.28. We could encode this continuous variable with an accuracy of two decimal places by multiplying its real value by 100 and then discarding the decimal portion of the product. Thus the value that the variable attains is linearly mapped to integers in the range [-128, 1281. The binary code corresponding to each integer can be easily computed. Fitness function. The objective function, the function to be optimized, provides the mechanism for evaluating each string. However. its range of values varies from problem to problem. To maintain uniformity over various problem domains, we use thefitness function to normalize the objective function to a convenient range of 0 to 1. The normalized value of the objective function is thefitness of the string, which the selection mechanism uses to evaluate the strings of the population. Selection. Selection models nature's survival-of-the-fittest mechanism. Fitter solutions survive while weaker ones perish. In the SGA, a fitter string receives a higher number of offspring and thus has a higher chance of surviving in the subsequent generation. In the proportionate selection scheme, a string with fitness value f - is allocated fb'f offspring, Simple Genetic Algorithm () initialize population; evaluate population; [ wherefis the average fitness value of the population. A string with a fitness value higher than the average is allocated more than one offspring, while a string with a fitness value less than the average is allocated less than one offspring. The proportionate selection scheme allocates fractional numbers of offspring to strings. Hence the numberfb'frepresents the string's expected number of offspring. Since in the final allocation some strings have to receive a higher number of offspring thanfi/'and some less than fb'f, allocation methods include some randomization to remove methodical allocation biases toward any particular set of strings. The allocation technique controls the extent to which the actual allocation of offspring to strings matches the expected number of offspringfi/'. The SGA uses the roulette wheel selection scheme' to implement proportionate selection. Each string is allocated a sector (slot) of a roulette wheel with the angle subtended by the sector at the center of the wheel equaling 2x fi@. A string is allocated an offspring if a randomly generated number in the range 0 to 2x falls in the sector corresponding to the string. The algorithm selects strings in this fashion until it has generated the entire population of the next generation. Roulette wheel selection could generate large sampling errors in the sense that the final number of offspring allocated to a string might vary significantly from the expected number. The allocated number of offspring approaches the expected number only for very large population sizes. Figure 1. Simple Genetic Algorithm structure. Crossover. After selection comes crossover, SGA's crucial operation. Pairs of strings are picked at random from the population to be subjected to crossover. The SGA uses the simplest approach - single-point crossover. Assuming that 1 is the string length, it randomly chooses a crossover point that can assume values in the range 1 to 1 - 1. The portions of the two strings beyond this crossover point are exchanged to form two new strings. The crossover point may assume any of the 1 - 1 possible values with equal probability. Further, crossover is not always effected. After choosing a pair of strings, the algorithm invokes crossover only if a randomly generated number in the range 0 to 1 is greater than pc, the crossover rate. (In GA literature, the term crossover rate is also used to denote the probability of crossover.) Otherwise the strings remain unaltered. The value ofp,. lies in the range from 0 to 1. In a large population,p, gives the fraction of strings actually crossed. Mutation. After crossover. strings are subjected to mutation. Mutation of a bit involves flipping it: changing a 0 to 1 or vice versa. Just asp, controls the probability of a crossover, another parameter. P, ,~( the mutation rate), gives the probability that a bit will be flipped. The bits of a string are independently mutated - that is, the mutation of a bit does not affect the probability of mutation of other bits. The SGA treats mutation only as a secondary operator with the role of restoring lost genetic material. For example, suppose all the strings in a population have converged to a 0 at a given position and the optimal solution has a 1 at that position. Then crossover cannot regenerate a 1 at that position. while a mutation could. June 1994 19 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:26:34 UTC from IEEE Xplore. Restrictions apply. Population P1: String Fitness value 0000011100 0.3 1000011111 0.6 01 10101011 0.6 1111111011 0.9 Population P2 : After selection String Fitness value 1000011111 0.6 0110101011 0.6 1111111011 0.9 1111111011 0.9 Population P3 : After crossover String Fitness value 100001 11 01 1 0.5 0110101011 0.6 1111111011 0.9 11111111111 1.0 Population P4 : After mutation String crossed, while the other pair of strings is left intact. The crossover point falls between the fifth and sixth bits of the strings, and portions of strings 1 and 4 beyond the fifth bit are swapped. Population P3 represents the set of strings after crossover. The action of mutation on population P3 can be seen in population P4 on the sixth bit of string 2 and the first bit of string 4: Only two bits out of 40 have been mutated, representing an effective mutation rate of 0.05. Population P4 represents the next generation. (In effect, P1 and P4 are the populations, while P2 and P3 represent intermediate stages in the generational cycle.) The example in Figure 2 is only for illustration. Typically the SGA uses a population size of 30 to 200, crossover rates from 0.5 to 1.0, and mutation rates from 0.001 to 0.05. These parameters - the population size, mutation rate, and crossover rate - are together referred to as the control parameters of the SGA and must be specified before its execution. To terminate execution of the SGA. we must specify a stopping criterion. It could be terminated after a fixed number of generations, after a string with a certain high fitness value is located, or after all the strings in the population have attained a certain degree of homogeneity (a large number of strings have identical bits at most positions). Figure 2. A generational cycle of the Simple Genetic Algorithm. How do genetic algorithms work? Generational cycle. Figure 2 shows a generational cycle of the genetic algorithm with a population (Pl) of four strings with 10 bits each. In the example, the objective function, which can assume values in the range 0 to 10, gives the number of Is in the string. The fitness function performs a “divide by 10” operation to normalize the objective function to the range 0 to 1. The four strings thus have fitness values of 0.3,0.6,0.6, and 0.9. Ideally, the proportional selection scheme should allocate 0.5, 1.0. 1.0 and 1.5 offspring to the strings. However, in this case, the final allocation of offspring is 0, 1.1, and 2. In Figure 2 the population P2 represents this selected set of strings. Next, the four strings are paired randomly for crossover. Strings 1 and 4 form one pair, while strings 2 and 3 form the other pair. At a crossover rate of 0.5, only the pair of strings 1 and 4 is actually Despite successful use of GAS in a large number of optimization problems. progress on the theoretical front has been rather slow. A very clear picture of the workings of GAS has not yet emerged, but the schemu theory and the buildingblock hypothesis of Holland and Goldberg’. 7 capture the essence of GA mechanics. Similarity template. A schema is a similarity template describing a subset of strings with similarities at certain posit i o n~. ’I,n~ o ther words, a schema represents a subset of all possible strings that have the same bits at certain string positions. As an example, consider strings with five bits. A schema **000 represents strings with 0s in the last three positions: the set of strings 00000,01000,10000, and 11000. Similarly, a schema 1*00* represents the strings 10000, 10001,11000, and 11001. Each string represented by a schema is called an instance of the schema. Because the symbol * signifies that a 0 or a 1 could occur at the corresponding string position, the schema ***** represents all possible strings of five bits. The fixed posifions of a schema are the string positions that have a 0 or a 1: in **000, the third, fourth. and fifth positions. The number of fixed positions of a schema is its order: **000 is of order 3. A schema’s defining length is the distance between the outermost fixed positions. Hence, the defining length of **000 is 2, while the defining length of 1 *00* is 3. Any specific string is simultaneously an instance of 2‘schemata ( I is the string length). Since a schema represents a subset of strings, we can associate a fitness value with a schema: the average fitness of the schema. In a given population, this is determined by the average fitness of instances of the schema. Hence, a schema’s average fitness value varies with the population’s composition from one generation to another. Competition. Why are schemata important? Consider a schema with k fixed positions. There are 2k - 1 other schemata with the same fixed positions that can be obtained by considering all permutations of 0s and Is at these k positions. Altogether, for k fixed positions, there are 2h distinct schemata that generate a partitioning of all possible strings. Each such set of k fixed positions generates a schema competition, a survival competition among the 2k schemata. Since there are 2’ possible combinations of fixed positions, 2‘distinct schema competitions are possible. The execution of the GA thus generates 2‘ simultaneous schema competitions. The GA simultaneously, though not independently, attempts to solve all the 2‘ schema competitions and locate the best schema for each set of fixed positions. We can visualize the GA’s search for the optimal string as a simultaneous competition among schemata to increase the number of their instances in the population. If we describe the optimal string as the juxtaposition of schemata with short defining lengths and high average fitness values, then the winners of the individual schema competitions could potentially form the optimal string. Such schemata with high fitness values and small defining lengths are appropriately called building blocks. The notion that 20 COMPUTER Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:26:34 UTC from IEEE Xplore. Restrictions apply. strings with high fitness values can be located by sampling building blocks with high fitness values and combining the building blocks effectively is called the building-block hypothesis.’.’ Building blocks. The genetic operators - crossover and mutation - generate, promote, and juxtapose building blocks to form optimal strings. Crossover tends to conserve the genetic information present in the strings to be crossed. Thus, when the strings to be crossed are similar, its capacity to generate new building blocks diminishes. Mutation is not a conservative operator and can generate radically new building blocks. Selection provides the favorable bias toward building blocks with higher fitness values and ensures that they increase in representation from generation to generation. GAS’ crucial and unique operation is the juxtaposition of building blocks achieved during crossover, and this is the cornerstone of CA mechanics. The building-block hypothesis assumes that the juxtaposition of good building blocks yields good strings. This is not always true. Depending on the nature of the objective function, very bad strings can be generated when good building blocks are combined. Such objective functions are referred to as CA-deceptive functions, and they have been studied extensively. (We discuss them in more detail in a later section.) Schema theorem. When we consider the effects of selection. crossover, and mutation on the rate at which instances of a schema increase from generation to generation. we see that proportionate selection increases or decreases the number in relation to the average fitness value of the schema. Neglecting crossover, a schema with a high average fitness value grows exponentially to win its relevant schema competition. However, a high average fitness value alone is not sufficient for a high growth rate. A schema must have a short defining length too. Because crossover is disruptive, the higher the defining length of a schema, the higher the probability that the crossover point will fall between its fixed positions and an instance will be destroyed. Thus, schemata with high fitness values and small defining lengths grow exponentially with time. This is the essence of the schema theorem, first proposed by Holland as the “fundamental theorem ofgenetic algorithms.”’ (See the sidebar.) The following equation is a formal statement of the schema theorem: where f(h, t): average fitness value of schema h in generation t fct): average fitness value of the population in generation t pc: crossover probability pm: mutation probability F(h): defining length of the schema o(h): order of the schema h N(h, t): expected number of instances of schema h in generation t I: the number of bit positions in a string The factor: gives the probability that an instance of the schema h is disrupted by crossover, andp,,o(h) gives the probability that an instance is disrupted by mutation.’ The CA samples the building blocks at a very high rate. In a single generational cycle the CA processes only P strings (P is the population size), but it implicitly evaluates approximately P’ schemata.’ This capacity of GAS to simultaneously process a large number of schemata, called implicit parallelism, arises from the fact that a string simultaneously represents 2‘ different schemata. Modifications to the SGA Over the last decade, considerable research has focused on improving GA performance. Efficient implementations of the proportionate selection scheme such as the stochastic remainder technique and the stochastic universal sampling technique have been proposed to reduce sampling errors. Selection mechanisms such as rank-based selection, elitist strategies, steady-state selection, and tournament selection have been proposed as alternatives to proportional selection. Crossover mechanisms such as two-point, multipoint. and uniform have been proposed as improvements on the traditional single- point crossover technique. Gray codes and dynamic encoding have overcome some problems associated with fixedpoint integer encoding. Departing from the traditional policy of static control parameters for the GA, adaptive techniques dynamically vary the control parameters (crossover and mutation rates). Significant innovations include the distributed genetic algorithms and parallel genetic algorithms. The rest of this section surveys these developments. Selection mechanisms and scaling. The proportionate selection scheme allocates offspring based on the ratio of a string’s fitness value to the population’s average fitness value. In the initial generations of the CA, the population typically has a low average fitness value. The presence of a few strings with relatively high fitness values causes the proportionate selection scheme to allocate a large number of offspring to these “superstrings,” and they take over the population, causing premature convergence. A different problem arises in the later stages of the CA when the population has converged and the variance in string fitness values becomes small. The proportionate selection scheme allocates approximately equal numbers of offspring to all strings, thereby depleting the driving force that promotes better strings. Scaling mechanisms and rankbased selection schemes overcome these two problems. June 1994 21 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:26:34 UTC from IEEE Xplore. Restrictions apply. Scaling of fitness values involves readjustment of string fitness values. Linear scaling computes the scaled fitness value as f =af+b wherefis the fitness value, f is the scaled fitness value, and a and b are suitably chosen constants. Here a and b are calculated in each generation to ensure that the maximum value of the scaled fitness value is a small number, say 1.5 or 2.0 times the average fitness value of the population. Then the maximum number of offspring allocated to a string is 1.5 or 2.0. Sometimes the scaled fitness values may become negative for strings that have fitness values far smaller than the average fitness of the population. In such cases, we must recompute a and b appropriately to avoid negative fitness values. One way to overcome the problem of negative scaled fitness values is simply to remove these “troublemakers” from the competition. The sigma truncation scheme does exactly this by considering the standard deviation of fitness values before scaling them. Hence the fitness values of strings are determined as follows: f =f-&co) wherefis the average fitness value of the population, CJ is the standard deviation of fitness values in the population, and c is a small constant typically ranging from 1 to 3. Strings whose fitness values are less than c standard deviations from the average fitness value are discarded. This approach ensures that most strings in the population (those whose fitness values are within c standard deviations of the average) are considered for selection, but a few strings that could potentially cause negative scaled fitness values are discarded. An alternate way to avoid the twin problems that plague proportional selection is rank-based selection, which uses a fitness value-based rank of strings to allocate offspring. The scaled fitness values typically vary linearly with the rank of the string. The absolute fitness value of the string does not directly control the number of its offspring. To associate each string with a unique rank, this approach sorts the strings according to their fitness values, introducing the drawback of additional overhead in the GA computation. Another mechanism is tournament selection. For selection, a string must win a competition with a randomly selected set of strings. In a k-ary tournament, the best of k strings is selected for the next generation. In either proportionate selection (with or without scaling) or rank-based selection, the expected number of offspring is not an integer, although only integer numbers of offspring may be allocated to strings. Researchers have proposed several implementations to achieve a distribution of offspring very close to the expected numbers of offspring. Considerable research has focused on improving GA performance. Innovations include distributed and parallel GAS. The stochastic remainder technique deterministically assigns offspring to strings based on the integer part of the expected number of offspring. It allocates the fractional parts in a roulette wheel selection (stochastic selection) to the remaining offspring, thus restricting randomness to only the fractional parts of the expected numbers of offspring. Each iteration of the simple GA creates an entirely new population from an existing population. GAS that replace the entire population are called generational GAS. GAS that replace only a small fraction of strings at a time are called steady-state GAS. Typically, new strings created through recombination replace the worst strings (strings with the lowest fitness values). Functionally, steady-state GAS differ from generational GAS in their use of populational elitism (preservation of the best strings), large population sizes, and high probabilities of crossover and mutation. The elitist selection strategy balances the disruptive effects of high crossover and mutation rates. Crossover mechanisms. Because of their importance to GA functioning, much of the literature has been devoted to different crossover techniques and their analysis. This section discusses the important techniques. Traditionally, GA researchers set the number of crossover points at one or two. In the two-point crossover scheme, two crossover points are randomly chosen and segments of the strings between them are exchanged. Two-point crossover eliminates the single-point crossover bias toward bits at the ends of strings. An extension of the two-point scheme, the multipoint crossover, treats each string as a ring of bits divided by k crossover points into k segments. One set of alternate segments is exchanged between the pair of strings to be crossed. Uniform crossover exchanges bits of a string rather than segments. At each string position, the bits are probabilistically exchanged with some fixed probability. The exchange of bits at one string position is independent of the exchange at other positions. Recent GA literature has compared various techniques, particularly singlepoint and two-point crossover on the one hand, and uniform crossover on the other. To classify techniques, we can use the notions of positional and distributional biases. A crossover operator has positional bias if the probability that a bit is swapped depends on its position in the string. Distributional bias is related to the number of bits exchanged by the crossover operator. If the distribution of the number is nonuniform, the crossover operator has a distributional bias. Among the various crossover operators, single-point crossover exhibits the maximum positional bias and the least distributional bias. Uniform crossover, at the other end of the spectrum, has maximal distributional bias and minimal positional bias. Empirical and theoretical studies have compared the merits of various crossover operators, particularly two-point and uniform crossover. At one end, uniform crossover swaps bits irrespective of their position, but its higher disruptive nature often becomes a drawback. Two-point and single-point crossover preserve schemata because of their low disruption rates, but they become less exploratory when the population becomes homogeneous. A related issue is the interplay between the population size and the type of crossover. Empirical evidence suggests that uniform crossover is more suitable 22 -- COMPUTER Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:26:34 UTC from IEEE Xplore. Restrictions apply. for small populations, while for larger populations, the less disruptive two-point crossover is better. Uniform crossover’s disruptiveness helps sustain a highly explorative search in small populations. The inherent diversity in larger populations reduces the need for exploration and makes two-point crossover more suitable. A rather controversial issue strikes at the heart of GA workings: Is crossover an essential search mechanism, or is mutation alone sufficient for efficient search? Experimental evidence shows that for some objective functions mutation alone can locate the optimal solu- Increasing the population size increases its diversity and reduces the probability that the GA will prematurely converge to a local optimum, but it also increases the time required for the population to converge to the optimal regions in the search space. We cannot choose control parameters until we consider the interactions between the genetic operators. Because they cannot be determined independently, the choice of the control parameters itself can be a complex nonlinear op- Encodings. Critical to GA performance is the choice of the underlying encoding for solutions of the optimization problem. Traditionally, binary encodings have been used because they are easy to implement and maximize the number of schemata processed. The crossover and mutation operators described in the previous sections are specific only to binary encodings. When alphabets other than [OJ] are used, the crossover and mutation operators must be tailored appropriately. A large number of optimization problems have continuous variables that assume real values. A common technique tions, while for objective functions in- for encoding continuous variables in the volving high epistaticity (nonlinear binary alphabet uses a fixed-point inteinteractions among the bits of the ger encoding - each variable is encoded strings), crossover performs a faster Nontraditional using a fixed number of binary bits. The search than mutation. On the other hand, techniques including binary codes of all the variables are concrossover has long been accepted as more catenated to obtain the strings of the useful when optimal solutions can be con- dynamic and population. A drawback of encoding structed by combining building blocks adaptive strategies variables as binary strings is the presence of Hamming cliffs: large Hamming distances between the binary codes of adja- (schemata with short defining lengths and high average fitness values), indicating which requires linear interactions among proposed to improve cent integers. For example, 01111 and the string bits. The question is whether performance. lo000 are the integer representations of the experimental evidence and the gen- 15 and 16, respectively, and have a Hamera1 consensus about the utility of ming distance of 5. For the GA to imhave also been crossover are contradictory. Or is crossover beneficial in most objective functions that have either linear or nonlinear interactions? These questions are far from being resolved, and considerable theoretical and empirical evidence must be gathered before any definite conclusions can be drawn. Control parameters. We can visualize the functioning of GAS as a balanced combination of exploration of new regions in the search space and exploitation of already sampled regions. This balance, which critically controls the performance of GAS, is determined by the right choice of control parameters: the crossover and mutation rates and the population size. The choice of the optimal control parameters has been debated in both analytical and empirical investigations. Here we point out the trade-offs that arise: Increasing the crossover probability increases recombination of building blocks, but it also increases the disruption of good strings. Increasing the mutation probability tends to transform the genetic search into a random search, but it also helps reintroduce lost genetic material. timization problem. Further, it is becoming evident that the optimal control parameters critically depend on the nature of the objective function. Although the choice of optimal control parameters largely remains an open issue, several researchers have proposed control parameter sets that guarantee good performance on carefully chosen testbeds of objective functions. Two distinct parameter sets have emerged: One has a small population size and relatively large mutation and crossover probabilities, while the other has a larger population size, but much smaller crossover and mutation probabilities. Typical of these two categories are crossover rate: 0.6, mutation rate: crossover rate: 0.9, mutation rate: 0.001, population size: and 0.01, population size: 30.8 The first set of parameters clearly gives mutation a secondary role, while the second makes it more significant. The high crossover rate of 0.9 in the second set also indicates that a high level of string disruption is desirable in small populations. prove the code of 15 to that of 16, it must alter all bits simultaneously. Such Hamming cliffs present a problem for the GA, as both mutation and crossover cannot overcome them easily. Gray codes suggested to alleviate the problem ensure that the codes for adjacent integers always have a Hamming distance of l. However, the Hamming distance does not monotonously increase with the difference in integer values, and this phenomenon introduces Hamming cliffs at other levels. Nontraditional techniques in GAS. The previous sections described selection and crossover techniques developed as natural extensions of the simple GA. Hence the techniques still have the traditional mold: binary encodings, statically defined control parameters, and fixed-length encodings. Recently, a wide spectrum of variants has broken away from the traditional setup. The motivation has been the performance criterion: to achieve better GA performance on a wide range of application problems. We refer to these as nontraditional techniques. Dynamic and adaptive strategies. In practical situations, the static configurations of control parameters and encod- June 1994 ’ 23 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:26:34 UTC from IEEE Xplore. Restrictions apply. 1 ings in GAS have some drawbacks. Parameter settings optimal in the earlier stages of the search typically become inefficient during the later stages. Similarly, encodings become too coarse as the search progresses, and the fraction of the search space that the GA focuses its search on becomes progressively smaller. To overcome these drawbacks, several dynamic and adaptive strategies for varying the control parameters and encodings have been proposed. One strategy exponentially decreases mutation rates with increasing numbers of generations, to gradually decrease the search rate and disruption of strings as the population converges in the search space. Another approach considers dynamically modifying the rates at which the various genetic operators are used, based on their performance. Each operator is evaluated for the fitness values of strings it generates in subsequent generations. Very often, after a large fraction of the population has converged (the strings have become homogeneous), crossover becomes ineffective in searching for better strings. Typically, low mutation rates (0.001 to 0.01) are inadequate for continuing exploration. In such a situation, a dynamic approach for varying mutation rates based on the Hamming distance between strings to be crossed can be useful. The mutation rate increases as the Hamming distance between strings decreases. As the strings to be crossed resemble each other to a greater extent, the capacity of crossover to generate new strings decreases, but the increased mutation rate sustains the search. The dynamic encoding of variables in several implementations (DPE, Argot, and Delta Encoding) increases the search resolution as the GA converges. While strings are encoded using the same number of bits, the size of the search space in which strings are sampled is progressively reduced to achieve a higher search resolution. Another adaptive strategy of encoding (“messy” GAS) explicitly searches loworder, high-fitness value schemata in the initial stages and then juxtaposes the building blocks with a splicing operator to form optimal strings. This technique has successfully optimized deceptive functions, which can cause the Simple GA to converge to local optima. Distributed and parallel GAS. Distributed GAS and parallel GAS decentralize the processing of strings. Although they sound similar, the two approaches are basically different. Distributed GAS have a number of weakly interacting subpopulations, and each carries out an independent search. Parallel GAS are parallel implementations of the “sequential” GA on several computation engines to speed execution. Distributed GAS distribute a large population into several smaller subpopulations that evolve independently. Thus, Researchers are developing models of GA dynamics, analyzing problems difficult for GAS, and studying how GAS work. the exploration arising from a large population is evident, but the convergence rates of the subpopulations are also high. To ensure global competition among strings, the best strings of the subpopulations are exchanged. A distributed GA can be implemented on a single computation engine or in parallel with each subpopulation processed by a different engine. Parallel GAS have emerged primarily to enable execution on parallel computers. Issues such as local and global communication, synchronization, and efficacy of parallel computation have led to modifications of the GA structure. Techniques such as local-neighborhood selection have been introduced to increase computation speed. Advances in theory The emergence of new GA implementations for better performance has been accompanied by considerable theoretical research, especially in developing models of GA dynamics, analyzing problems that are hard for GAS, and, most important, gaining a deeper understanding of how GAS work. To analyze the working of the simple GA, Holland compared it with the karmed bandit problem.’ This problem discusses the optimal allocation of trials among k alternatives, each of which has a different payoff, to maximize the total payoff in a fixed number of trials. The payoff of each alternative is treated as a random variable. The distribution of payoffs from the different alternatives is not known a priori and must be characterized based on the payoffs observed during the trials. Holland demonstrated that the GA simultaneously solves a number of such k-armed bandit problems. Consider the competition among schemata of order m that have the same fixed positions. There are 2m competing schemata, and the GA allocates trials to them to locate the fittest. Totally, there are 2‘ ( I is the string length) such competitions occurring in parallel, with the GA attempting to solve all simultaneously. The exponential allocation of trials to the fittest strings by the GA is a near optimal allocation strategy, as it resembles the optimal solution to the k-armed bandit p r ~ b l e m . ~ The schema theorem’ calculates a lower bound on the expected number of schemata under the action of selection, crossover, and mutation. Although the schema theorem captures the essence of the GA mechanism, its applicability in estimating the proportions of various schemata in the population is limited. Attempts to refine the schema theorem model the effects of crossover between instances of the same schema. To make the schema theorem more useful, expressions for the percentage of schema instances generated by crossover and mutation have been derived. The additional terms have extended the inequality of the schema theorem into an equation. However, the abstract nature of the calculations involved in computing these terms reduces the applicability of the schema “equation.” A generalization of schemata defined by Holland has been proposed. It views as a predicate the condition for a string to be included as an instance of a schema. This general definition allows 22‘ predicates to exist, compared with the 3‘ Holland schemata for strings of length 1. While the schema theorem remains valid for these generalized predicates, we can study several new interesting properties regarding their stability and dominance under the action of the genetic operators. 24 COMPUTER Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:26:34 UTC from IEEE Xplore. Restrictions apply. I GA dynamics. The GA’s population dynamics are controlled by the parameters population size, mutation rate, and crossover rate. Characterizing the dynamics - not a simple task - is important for understanding the conditions under which the GA converges to the global optimum. Most work related to the dynamics of GAS looks at convergence results from one of two perspectives: (1) finite versus infinite population re- (2) homogeneous versus inhomogesults, or neous convergence results. The first classification is self-explanatory. The second arises from the state-transition probabilities of the Markov processes that model the GA. If the statetransition probabilities are invariant over generations, we have a homogeneous Markov chain. For the finite population case, we can consider each distinct population as a possible state of a Markov chain, with the state-transition matrix indicating the probabilities of transitions between the populations due to the genetic operators. When the mutation probability is not zero, every population can be reached from every other population with some nonzero probability. This property guarantees the existence of a unique fixed point (a limiting distribution) for the distribution of populations. For a zero mutation probability (with only selection and crossover), any population consisting of multiple copies of a single string is a possible fixed point of the random process modeling the GA. Consider a nonstandard replacement operator after crossover that ensures the following property: For every bit position i there exist strings in the population having a 0 and a 1 at the position i. With this we can show that every population is reachable in a finite number of generations. The replacement operator substitutes one of the strings in the population with another string so the population satisfies the defined property. Further, the property also guarantees convergence of the GA to the global optimum with the probability of 1.0. While these results summarize the homogeneous case, the main inhomogeneous result for finite populations is the demonstration of an exponential annealing schedule that guarantees convergence of the GA to one of the fixed points of the homogeneous case without mutation. However, this does not mean that the population corresponding to this fixed point contains only the global optimum. Empirical evidence suggests that as the population size increases, the probability mass of the limit distributions is concentrated at the optimal populations. In infinite populations, we need model only the proportions of strings. We can model the evolution of populations as the We are faced with an important question: what problems mislead GAS to local optima? interleaving of a quadratic operator representing crossover and mutation, and a linear operator representing selection. When only selection and crossover are considered, all limit points of the probability distribution have mass only at the most fit strings. With mutation and uniform selection, the uniform distribution is the unique fixed point. Although it is important to establish the global convergence of GAS, it is equally important to have GAS with good rates of convergence to the global optimum. We believe that a major direction for future research on the dynamics of GAS is the establishment of bounds on the convergence rates of the GA under various conditions. Deception. An important control on the dynamics of GAS is the nature of the search landscape. We are immediately confronted with a question: What features in search landscapes can GAS exploit efficiently? Or more to the point: What problems mislead GAS to local optima? GAS work by recombining low-order, short schemata with above-average fitness values to form high-order schemata. If the low-order schemata contain the globally optimal solution, then the GA can potentially locate it. However, with functions for which the low-order highfitness value schemata do not contain the optimal string as an instance, the GA could converge to suboptimal strings. Such functions are called de c ept i~eR.~ecently, considerable research has focused on the analysis and design of deceptive functions. The simplest deceptive function is the minimaf deceptive problem, a two-bit function. Assuming that the string “11” represents the optimal solution, the following conditions characterize this problem: The lower order schemata O* or *O do not contain the optimal string 11 as an instance and lead the GA away from 11. The minimal deceptive problem is a partially deceptive function, as both conditions of Equation 2 are not satisfied simultaneously. In a fully deceptiveproblem, all the lower order schemata that contain the optimal string have lower average fitness values than their competitors (other schemata with the same fixed positions). The minimal deceptive problem can easily be extended to higher string lengths. GA literature abounds with analyses of deceptive functions, conditions for problems to be deceptive, and ways of transforming deceptive functions into nondeceptive ones. Some recent studies have investigated the implications of GA deceptiveness in the context of problems that are hard - that is, difficult for GAS to optimize. While it appears that a deceptive objective function offers some measure of difficulty for GAS, there has been some recent consensus that deception is neither a sufficient nor a necessary condition for a problem to be hard. At the heart of this argument is the observation that the definition of deception in GAS derives from a static hyperplane analysis which does not account for the potential difference of GAS’ dynamic behavior from static predictions. Empirical work shows that some nondeceptive functions cannot be optimized easily by GAS, while other deceptive functions are easily optimized. Essentially, other features such as improper problem representations, the disruptive nature of crossover and mutation, finite population sizes, and multimodal landscapes could be potential causes of hardness. June 1994 25 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:26:34 UTC from IEEE Xplore. Restrictions apply. 1 nvented in the early 1970s, genetic algorithms only recently have gained considerable popularity as general-purpose robust optimization and search techniques. The failure of traditional optimization techniques in searching complex, uncharted and vast-payoff landscapes riddled with multimodality and complex constraints has generated interest in alternate approaches. Genetic algorithms are particularly attractive because instead of a naive “search and select” mechanism they use crossover to exchange information among existing solutions to locate better solutions. Despite the algorithms’ success, some open issues remain: the choice of control parameters, the exact roles of crossover and mutation, the characterization of search landscapes amenable to optimization, and convergence properties. Limited empirical evidence points to the efficacy of distributed and parallel GAS and the adaptive strategies for varying control parameters. However, more experimental evidence is needed before we draw any definite conclusions about comparative performance. GAS are emerging as an independent discipline, but they demand considerable work in the practical and theoretical domains before they will be accepted at large as alternatives to traditional optimization techniques. We hope this article stimulates interest in GAS and helps in their establishment as an independent approach for optimization and search. W References 1. J.H. Holland, Adaptation in Natural and Artificial Systems, Univ. of Michigan Press, Ann Arbor, Mich., 1975. 2. S. Kirkpatrick, C.D. Gelatt, and M.P. Vecchi, “Optimization by Simulated Annealing,” Science, Vol. 220, No. 4598, May 1983, pp. 671-681. 3. I. Rechenberg, Evolutionsstrategie: Optimierung technischer Systeme nach Prinzipien der biologische Evolution [Evolutionary Strategy: Optimization of Technical Systems According IO the Principles of Biological Evolution], Frommann Holzboog Verlag, Stuttgart, Germany, 1973. 4. H.P. Schwefel, Numerical Optimization of Computer Models, Wiley, Chichester, UK, 1981. 5. K.A. DeJong, An Analysis of the Behavior of a Class of Genetic Adaptive Systems, doctoral dissertation, Univ. of Michigan, Ann Arbor, Mich., 1975. 6. S. Forrest and M. Mitchell, “What Makes a Problem Hard for a Genetic Algorithm? Some Anomalous Results and their Explanation,” in Machine Learning, Vol. 13, 1993, pp. 285-319. 7. D.E. Goldberg, Genetic Algorithms in Search, Optimization and Machine Learning, Addison-Wesley, Reading, Mass., 1989. 8. J.J. Grefenstette, “Optimization of Control Parameters for Genetic Algorithms,” IEEE Trans. Systems, Man, and Cybernetics, Vol.SMC-16, No. 1, Jan./Feb. 1986, pp. 122-128. 9. H. Muhlenbein et al., “The Parallel GA as a Function Optimizer,” Proc. Fourth Int’l Conf Genetic Algorithms, Morgan Kaufmann, San Mateo, Calif., 1991, pp. 279-288. 10. J.D. Schaffer et al., “A Study of Control Parameters Affecting On-line Performance of Genetic Algorithms for Function Optimization,” Proc. Third Znt’l Con$ Genetic Algorithms, Morgan Kdufmann, San Mateo, Calif., 1989, pp. 51-60. 1. M. Srinivas and L.M. Patnaik, “Adaptive Probabilities of Crossover and Mutation in Genetic Algorithms,” ZEEE Trans. Systems, Man, and Cybernetics, Apr. 1994. 2. M. Srinivas and L.M. Patnaik, “Binomially Distributed Populations for Modeling Genetic Algorithms,” Proc. Fifth Int’l Con$ Genetic Algorithms, Morgan Kaufmann, San Mateo, Calif., 1993, pp. 138- 145. 13. D. Whitley and T. Starkweather, “Genitor- 11: A Distributed Genetic Algorithm,” J. Experimental Theoretical Artificial Intelligence, Vol. 2,1990, pp. 189-214. 14. L. Davis, ed., Handbook of Genetic Algorithms, Van Nostrand Reinhold, New York, 1991. 15. Z. Michalewicz, Genetic Algorithms + Data Structures = Evolutionary Programs, Springer-Verlag, Berlin, 1992. 16. K.A. DeJong and W.M. Spears, “An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms,” Proc. First Workshop Parallel Problem Solving from Nature, Springer- Verlag, Berlin, 1990, pp. 38-47. M. Srinivas is employed at Motorola India Electronics Ltd., where his research interests are in theory and design of genetic algorithms, neural networks, stochastic optimization, and optimization in VLSI CAD algorithms. He has also worked for the Centre for Development of Advanced Computing. Srinivas is a PhD candidate in computer science and automation at the Indian Institute of Science, Bangalore. Lalit M. Patnaik is a professor in the Electrical Sciences Division of the Indian Institute of Science, where he directs a research group in the Microprocessor Applications Laboratory. His teaching, research, and development interests are in parallel and distributed computing, computer architecture, computer-aided design of VLSI systems, computer graphics, theoretical computer science, real-time systems, neural computing, and genetic algorithms. In the areas of parallel and distributed computing and neural computing, he has been a principal investigator for government-sponsored research projects and a consultant to industry. Patnaik received his PhD for work in realtime systems in 1978 and his DSc in computer systems and architectures in 1989, both from the Indian Institute of Science. He is a fellow of the IEEE, Indian National Science Academy, Indian Academy of Sciences, National Academy of Sciences, and Indian National Academy of Engineering. For the last two years, he has served as chair of the IEEE Computer Society chapter, Bangalore section. Srinivas can be reached at Motorola India Electronics Ltd., No. 1, St. Marks Road, Bangalore 560 001, India; e-mail: msriniemaster. miel.mot.com. Patnaik can be contacted at the Microprocessor Applications Laboratory, Indian Institute of Science, Bangalore 560 012, India; e-mail: lalit@micro.iisc.ernet.in. COMPUTER Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:26:34 UTC from IEEE Xplore. Restrictions apply.\"],\n",
    "[3,1,1,\"IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS, VOL. SMC-16, NO, 1, JANUARY/FEBRUARY 1986 Optimization of Control Parameters for Genetic Algorithms JOHN J. GREFENSTETTE, MEMBER, IEEE Abstract-The task of optimizing a complex system presents at least two levels of problems for the system designer. First, a class of optimization algorithms must be chosen that is suitable for application to the system. Second, various parameters of the optimization algorithm need to be tuned for efficiency. A class of adaptive search procedures called genetic algorithms (GA) has been used to optimize a wide variety of complex systems. GA's are applied to the second level task of identifying efficient GA's for a set of numerical optimization problems. The results are validated on an image registration problem. GA's are shown to be effective for both levels of the systems optimization problem. I. INTRODUCTION THE PROBLEM of dynamically controlling a complex process often reduces to a numerical function optimization problem. Each task environment for the process defines a performance response surface which must be explored in general by direct search techniques in order to locate high performance control inputs (see Fig. 1). If the response surface is fairly simple, conventional nonlinear optimization or control theory techniques may be suitable. However, for many processes of interest, e.g. computer operating systems or system simulation programs, the response surface is difficult to search, e.g., a high-dimensional, multimodal, discontinuous, or noisy function of the control inputs. In such cases, the choice of optimization technique may not be obvious. Even when an appropriate class of optimization algorithms is available, there are usually various parameters that must be tuned, e.g., the step size in a variable metric technique. Often the choice of parameters can have significant impact on the effectiveness of the optimization algorithm [8]. The problem of tuning the primary algorithm represents a secondary, or metalevel, optimization problem (see Fig. 2). This work attempts to determine the optimal control parameters for a class of global optimization procedures called genetic algorithms (GA's). The class of GA's is distinguished from other optimization techniques by the use of concepts from population genetics to guide the search. However, like other classes of algorithms, GA's differ from one another with respect to several parameters and strategies. This paper describes experiments that search Manuscript received March 21, 1984; revised August 28, 1985. This work was supported in part by a Fellowship from the Vanderbilt University Research Council and by the National Science Foundation under Grant MCS-8305693. The author is with the Computer Science Department, Vanderbilt University, Nashville, TN 37235, USA. IEEE Log Number 8406073. Environment E Performance Measure u Control Input c Feedback f Adaptive Strategy Fig. 1 One-level adaptive system model. a parameterized space of GA's in order to identify efficient GA's for the task of optimizing a set of numerical functions. This search is performed by a metalevel GA. Thus GA's are shown to be suitable for both levels of the system optimization problem. The remainder of this paper is organized as follows: Section II contains a brief overview of GA's and a summary of previous work. Section III describes the design of experiments which test the performance of GA's as meta-level optimization strategies. The experimental results appear in Section IV. A validation study is presented in Section V. The conclusions are summarized in Section VI. II. OVERVIEW OF GENETIC ALGORITHMS Suppose we desire to optimize a process having a response surface u, which depends on some input vector x. It is assumed that no initial information is available concerning the surface u, but that a black box evaluation procedure can be invoked to compute the scalar function u(x). The state of the art in such situations is to perform some sort of random search, perhaps combined with local hillclimbing procedures [5], [9]. Genetic algorithms are global optimization techniques that avoid many of the shortcomings exhibited by local search techniques on difficult search spaces. A GA is an iterative procedure which maintains a constant- size population P(t) of candidate solutions. During each iteration step, called a generation, the structures in the current population are evaluated, and, on the basis of those evaluations, a new population of candidate solutions is formed (see Fig. 3.) The initial population P(O) can be chosen heuristically or at random. The structures of the population P(t + 1) are chosen from P(t) by a randomized selection procedure that ensures that the expected number of times a structure is chosen is approximately proportional to that structure's 0018-9472/86/0100-0122$01.00 (©1986 IEEE 122 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:26:09 UTC from IEEE Xplore. Restrictions apply. GREFENSTETTE: GENETIC ALGORITHMS FOR MULTILEVEL ADAPTIVE SYSTEMS Parameters x _-------------- St Fig. 2 Two-lev t <- 0; initialize P(t); -- P(t) is the population at time t evaluate P(t); while (termination condition not satisfied) do begin t <- t+l; select P(t); recombine P(t); evaluate P(t); end; Fig. 3 Skeleton of a genetic algorithm. performance relative to the rest of the population. In order to search other points in the search space, some variation is introduced into the new population by means of idealized genetic recombination operators. The most important recombination operator is called crossover. Under the crossover operator, two structures in the new population exchange portions of their internal representation. For example, if the structures are represented as binary strings, crossover can be implemented by choosing a point at random, called the crossover point, and exchanging the segments to the right of this point. Let xl = 100:01010 and x2= 010:10100, and suppose that the crossover point has been chosen as indicated. The resulting structures would be Yi = 100:10100 and Y2 = 010:01010. Crossover serves two complementary search functions. First, it provides new points for further testing within the hyperplanes already represented in the population. In the above example, both xl and Yi are representatives of the hyperplane 100# # # # #, where the # is a `don't care` symbol. Thus, by evaluating Yl, the GA gathers further knowledge about this hyperplane. Second, crossover introduces representatives of new hyperplanes into the population. In the previous example, Y2 is a representative of the hyperplane #1001.###, which is not represented by either parent structure. If this hyperplane is a high-performance area of the search space, the evaluation of Y2 will lead to further exploration in this subspace. Each evaluation of a structure of length L contributes knowledge about the performance of the 2L hyperplanes represented by that structure. The power of GA's derives largely from their ability to exploit efficiently this vast amount of accumulating knowledge by means of relatively simple selection mechanisms [17]. Termination of the GA may be triggered by finding an acceptable approximate solution, vel adaptive system model. by fixing the total number of structure evaluations, or some other application dependent criterion. For a more thorough introduction to GA's [7], [17]. As stated above, GA's are essentially unconstrained search procedures within the given representation space. Constraints may be handled indirectly through penalty functions. A more direct way to incorporate constraints has been proposed by Fourman [12], who treats the structures in the population as lists of consistent constraints for VLSI layout problems. Genetic algorithms have not enjoyed wide recognition, possibly due to a misconception that GA's are similar to early `evolutionary programming` techniques [11], which rely on random mutation and local hill-climbing. The basic concepts of GA's were developed by Holland [17] and his students [1], [2], [4], [6], [13], [15], [19]. These studies have produced the beginnings of a theory of genetic adaptive search. For example, an application of gambler's ruin theory to the allocation of trials to the hyperplanes of the search space shows that genetic techniques provide a nearoptimal heuristic for information-gathering in complex search spaces [6], [17]. Bethke [1] provides theoretical characterizations of problems which may be especially wellsuited or especially difficult for GA's [1]. In addition, a number of experimental studies show that GA's exhibit impressive efficiency in practice. While classical gradient search techniques are more efficient for problems which satisfy tight constraints, GA's consistently outperform both gradient techniques and various forms of random search on more difficult (and more common) problems, such as optimizations involving discontinuous, noisy, high-dimensional, and multimodal objective functions. GA's have been applied to various domains, including combinatorial optimization [12], [16], image processing [10], pipeline control systems [15], and machine learning [2], [18], [25]. III. EXPERIMENTAL DESIGN We now describe experiments which attempted to optimize the performance of GA's on a given set of function optimization problems. These experiments were designed to search the space of GA's defined by six control parameters, and to identify the optimal parameter settings with 123 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:26:09 UTC from IEEE Xplore. Restrictions apply. IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS, VOL. SMC-16, NO. 1, JANUARY/FEBRUARY 1986 respect to two different performance measures. The searches for the optimal GA's were performed by GA's, which demonstrates the efficiency and power of GA's as metalevel optimization techniques. A metalevel GA could similarly search any other space of parameterized optimization procedures. A. The Space of Genetic Algorithms Holland [17] describes a fairly general framework for the class of GA's. There are many possible elaborations of GA's involving variations such as other genetic operators, variable sized populations, etc. This study is limited to a particular subclass of GA's characterized by the following six parameters. 1) Population Size (N): The population size affects both the ultimate performance and the efficiency of GA's. GA s generally do poorly with very small populations [22], because the population provides an insufficient sample size for most hyperplanes. A large population is more likely to contain representatives from a large number of hyperplanes. Hence, the GA's can perform a more informed search. As a result, a large population discourages premature convergence to suboptimal solutions. On the other hand, a large population requires more evaluations per generation, possibly resulting in an unacceptably slow rate of convergence. In the current experiments, the population size ranged from 10 to 160 in increments of 10. 2) Crossover Rate (C): The crossover rate controls the frequency with which the crossover operator is applied. In each new population, C * N structures undergo crossover. The higher the crossover rate, the more quickly new structures are introduced into the population. If the crossover rate is too high, high-performance structures are discarded faster than selection can produce improvements. If the crossover rate is too low, the search may stagnate due to the lower exploration rate. The current experiments allowed 16 different crossover rates, varying from 0.25 to 1.00 in increments of 0.05. 3) Mutation Rate (M): Mutation is a secondary search operator which increases the variability of the population. After selection, each bit position of each structure in the new population undergoes a random change with a probability equal to the mutation rate M. Consequently, approximately M * N * L mutations occur per generation. A low level of mutation serves to prevent any given bit position from remaining forever converged to a single value in- the entire population. A high level of mutation yields an essentially random search. The current experiments allowed eight values for the mutation rate, increasing exponentially from 0.0 to 1.0. 4) Generation Gap (G): The generation gap controls the percentage of the population to be replaced during each generation. That is N * (1 G) structures of P(t) are chosen (at random) to survive intact in P(t + 1). A value of G = 1.0 means that the entire population is replaced during each generation. A value of G = 0.5 means that half of the structures in each population survive into the next generation. The current experiments allowed G to vary between 0.30 and 1.00, in increments of 0.10. 5) Scaling Window (W): When maximizing a numerical function f(x) with a GA, it is common to define the performance value u(x) of a structure x as u(x) = f(x) - fmin, where fmin is the minimum value that f(x) can assume in the given search space. This transformation guarantees that the performance u(x) is positive, regardless of the characteristics of f(x). Often, fmin is not available a priori, in which case it is reasonable to define u(x) = f(x) - f(xmin), where f(xmin) is the minimum value of any structure evaluated so far. Either definition of u(x) has the unfortunate effect of making good values of x hard to distinguish. For example, suppose fmin = 0. After several generations, the current population might contain only structures x for which 105 <f(x) < 110. At this point, no structure in the population has a performance which deviates much from the average. This reduces the selection pressure toward the better structures, and the search stagnates. One solution is to define a new parameter fm,1in with a value of say, 100, and rate each structure against this standard. For example, if f(xi)= 110 and f(x;) - 105, then U(Xi) = f(xi) - fmin = 10, and u(xj) = f(xj) - fmin 5; the performance of xi now appears to be twice as good as the performance of xi. Our experiments investigated three scaling modes, based on a parameter called the scaling window W. If W = 0, then scaling was performed as follows: fmin was set to the minimum f(x) in the first generation. For each succeeding generation, those structures whose evaluations were less than fmin were ignored in the selection procedure. The fmin was updated whenever all the structures in a given population had evaluations greater than fm. If 0 < W < 7, then we set fmin to the least value of f(x) which occurred in the last W generations. A value of W = 7 indicated an infinite window (i.e., no scaling was performed). 6) Selection Strategy (S): The experiments compared two selection strategies. If S = P, a pure selection procedure was used, in which each structure in the current population is reproduced a number of times proportional to that structure's performance. If S = E, an elitist strategy was employed. First, pure selection is performed. In addition, the elitist strategy stipulates that the structure with the best performance always survives intact into the next generation. In the absence of such a strategy, it is possible for the best structure to disappear, due to sampling error, crossover, or mutation. We denote a particular GA by indicating its respective values for the parameters N, C, M, G, W, and S. Early work by De Jong [6] suggests parameter settings which have been used in a number of implementations of genetic algorithms. Based on De Jong's work, we define the standard GA as GAS = GA(50, 0.6,, 0.001, 1.0, 7, E). The Cartesian product of the indicated ranges for the six parameters (N, C, M, G, W, S) defines a space of 218 GA's. In some cases, it is possible to predict how variations of a single parameter will affect the performance of the GA's, assuming that all other parameters are kept fixed [6]. 124 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:26:09 UTC from IEEE Xplore. Restrictions apply. GREFENSTETTE: GENETIC ALGORITHMS FOR MULTILEVEL ADAPTIVE SYSTEMS However, it is difficult to predict how the various parameters interact. For example, what is the effect of increasing the population size, while lowering the crossover rate? The analytic -optimization of this space is well beyond our current understanding of GA's. It is also clear that an exhaustive enumeration of the space is infeasible. Our approach was to apply a metalevel GA's to the problem of identifying high-performance GA's. Each structure in the population of the meta-level GA consisted of an 18-bit vector which identified a particular GA. The performance of each GA was measured during the performance of a series of function optimization tasks. The meta-level GA used this information to conduct a search for high-performance algorithms. B. Task Environment Each GA was evaluated by using it to perform five optimization tasks, one for each of five carefully selected numerical test functions. As a results of these optimization tasks, the GA was assigned a value according to one of the performance measures explained below. The functions comprising the task environment have been studied in previous studies of GA's [3], [6] and included functions with various characteristics, including discontinuous, multidimensional, and noisy functions. Table I gives a brief description of the test functions. C. Performance Measures Two performance metrics for adaptive search strategies were considered, online performance and offline performance [6]. The on-line performance of a search strategy s on a response surface e is defined as follows: Ue(s, T) = avet(Ue(t)), t = 0,1, *, T where ue(t) is the performance of the structure evaluated at time t. That is, online performance is the average performance of all tested structures over the course of the search. The offline performance of a search strategy s on a response surface e is defined as follows: Ue*(s, T) = avej(u*(t)), t = 0, 1, T where u*(t) is the best performance achieved in the time interval [0, t]. Offline performance is the relevant measure when the search can- be performed offline (e.g., via a simulation model), while the best structure so far is used to control the online system. In order to measure global robustness, corresponding performance measures are defined for the entire set of response surfaces E: UE(s, T) = 100.0* avee(Ue(s, T)/Ue(rand, T)), e in E UE (S, T) = 1000* avee(Ue*(s, T)/Ue*(rand, T)), e in E where U, (rand, T) and U,*(rand, T) are on-line and offline performance, respectively, of pure random search on TABLE I FUNCTIONS COMPRISING THE TEST ENVIRONMENT Function Dimensions Size of Space Description fl 3 1.0 x 109 parabola f2 2 1.7 x 106 Rosenbrock's saddle [23] f3 5 1.0 X 1015 step function f4 30 1.0 X 1072 quartic with noise f5 2 1.6 X 1010 Shekel's foxholes [241 response surface e. As normalized, UE and UE for random search will be 100.0, while UE and UE* for more effective search strategies will be correspondingly lower (for minimization problems). D. Experimental Procedures Two experiments were performed, one to optimize on-line performance and one to optimize offline performance. For each experiment, the procedure for obtaining the optimum GA was as follows. 1) One thousand GA's were evaluated, using a metalevel GA to perform the search through the space of GA's defined by the six GA parameters. Each evaluation comprised running one GA against each of the five test functions for 5000 function evaluations and normalizing the result with respect to the performance of random search on the same function. The metalevel GA started with a population of 50 randomly chosen GA's and used the standard parameter settings, i.e., GA(50, 0.6, 0.001, 1.0, 7, E). Past experience has shown that these parameters yield a fairly good search for a variety of problems, and so this was the natural choice for the meta-level. 2) Since GA's are randomized algorithms, the performance of a GA during a single trial in the metalevel experiment represents a sample from a distribution of performances. Therefore, it was decided that the GA's showing the best performances during step 1 would be subjected to more extensive testing. Each of the 20 best GA's in step 1 was again run against the task environment, this time for five trials for each test function, using different random number seeds for each trial. The GA which exhibited the best performance in this step was declared the winner of the experiment. IV. RESULTS A. Experiment 1-Online Performance The first experiment was designed to search for the optimal GA with respect to online performance on the task environment. Fig. 4 shows the average online performance for the 50 GA's in each of the 20 generations of experiment 1. Recall that the overall scores for random search on the task environment is 100.0. From the initial data point in Fig. 4, we can estimate that the average online performance of all GA's in our search space is approximately 56.6, or about 43.4 percent better than random search. Fig. 4 shows that the final population of GA's had significantly better performance than the average GA. 125 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:26:09 UTC from IEEE Xplore. Restrictions apply. IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS, VOL. SMC-16, NO. 1, JANUARY/FEBRUARY 1986 so 50 ¢D uJ zzaCl:; a: 040 U- tr: hi aL hi Z30 -J z0 hi 020 aa:: 10 250k z c: 200 a: 0CC U- 06150 z -J U- IL100 C: a: i 5 at: -0 4 e 12 GENERRTIONS 16 20 -0 4 8 12 16 20 GENERAT I ONS Fig. 4 Experiment 1. This experiment identified GA1 = GA(30, 0.95, 0.01, 1.0, 1, E) as the optimal GA with respect to online performance. In an extended comparison, GA1 showed a 3.09 percent improvement (with respect to the baseline performance of random search) over GAs on the task environment. This represents a small but statistically significant improvement over the expected online performance of GAS. The performance improvement between GAS and GA1 can be attributed to an interaction among a number of factors. First, GA1 uses a smaller population, which allows many more generations within a given number of trials. For example, on functions in the task environment, GA1 iterated through an average of twice as many generations as GAS. Second, GAS uses an infinite window, i.e., no scaling is performed. GA1 uses a small window (one generation), which resulted in a more directed search. These two factors are apparently balanced by the significantly increased crossover rate and mutation rate in GA1. A higher crossover rate tends to disrupt the structures selected for reproduction at a high rate, which is important in a small population, since high performance individuals are more likely to quickly dominate the population. The higher mutation rate also helps prevent premature convergence to local optima. B. Experiment 2-Offline Performance The second experiment was designed to search for the optimal GA with respect to offline performance on the task environment. Fig. 5 shows that the average offline performance of all GA's (214.8) appears to be much worse than the average offline performance of random search (100.0). This finding verifies the experience of many practitioners that GA's can prematurely converge to suboptimal solutions when given the wrong control parameters. For example, if a very small population size is used (i.e., N = 10), the number of representatives from any given hyperplane is so small that the selection procedure has insufficient information to properly apportion credit to the hyperplanes Fig. 5 Experiment 2. represented in the population. As a result, a relatively good structure may overrun the entire population in a few generations. Unless the mutation rate is high, the GA will quickly converge to a suboptimal solution. In contrast, random search will usually locate at least one high performance point within the first thousand trials, leading to relatively good offline performance. That is, random search is a fairly tough competitor for search strategies when the goal is good offline performance. It is encouraging that many GA's perform significantly better than random search with respect to the offline performance measure. This experiment identified GA2= GA(80, 0.45, 0.01, 0.9, 1, P) as the optimal GA with respect to offline performance. In an extended comparison, GA2 showed a 3.0 percent performance improvement over GAS on the task environment. Because of the high variance shown by GA's with respect to offline performance, this does not represent a statistically significant difference between GA2 and GAS. There are several interesting difference between GA2 and GA'. With a larger population and higher mutation rate, the population will tend to contain more variety, thus increasing the random aspects of the GA. The slightly lower generation gap also tends to reduce the effects of selection, resulting in a less focused search. These aspects are balanced by the lower crossover rate and the small scaling window which tend to enhance the selective pressure. C. General Observations Besides suggesting optimal GA's, the above experiments also provide performance data for 2000 GA's with various parameter settings. Given that these are not independent samples from the space of GA's, it is rather difficult to make valid statistical inferences from this data. Nevertheless the data does suggest some regularities that might warrant further studies. The experimental data confirms several observations first made by De Jong on the basis of a relatively small number of experiments [6]. For example, mutation rates above 0.05 are generally harmful with respect to online performance, 126 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:26:09 UTC from IEEE Xplore. Restrictions apply. GREFENSTETTE: GENETIC ALGORITHMS FOR MULTILEVEL ADAPTIVE SYSTEMS 30~ -20 -j 10 -u n_3Au . Au0 so u 120 IS POPULATION SIZE Fig. 6 Average online performance of various population sizes according to Experiment 1. with performance approaching that of random search with rates above 0.1 regardless of the other parameter settings. The absence of mutation is also associated with poorer performance, which suggests that mutation performs an important service in refreshing lost values. Best on-line performance can be obtained with a population size in the range of 30-100 structures as shown by Fig. 6, which plots the average online performance as a function of population size, ignoring those GA's with mutation rates above 0.05. A similar graph can be obtained for offline performance except that range for the best population size is 60-110 structures. A large generation gap generally improves performance, as does the elitist selection strategy. The performance data also suggests other regularities that have not been previously noted. First, the adoption of a small scaling window (1-5 generations) is associated with a slight improvement in both online and offline performance. This is reasonable since scaling enhances the pressure of selection in later stages of the search. In small populations (20 to 40) structures, good online performance is associated with either a high crossover rate combined with a low mutation rate or a low crossover rate combined with a high mutation rate. For mid-sized populations (30 to 90 structures), the optimal crossover rate appears to decrease as the population size increases. For example, among the best 10 percent of all GA's with population size 30, the average crossover rate was 0.88. The best crossover rate decreases to 0.50 for population size 50 and to 0.30 for population size 80. This is reasonable since, in smaller populations, crossover plays an important role in preventing premature convergence. In summary, the performance of GA's appear to be a nonlinear function of the control parameters. However, the available data is too limited to confirm or disconfirm the existence of discontinuities or multiple local optima in the performance space of GA's. It would be interesting to compare the performance of other nonlinear search techniques in optimizing the performance of GA's. V. VALIDATION In order to validate the experimental results, the three algorithms GAS, GA1 and GA2 were applied to an optimization problem which was not included in the experimental -j 0i a: GA1 0 100oo O2000 300-0 4000 5000 TRIALS Fig. 7 Online performance of GA1 and GAS on an image registration task. 8 zLu CC 6 w - - G u.i GAS c: 10 1000 2000 3000 4000 5000 TRIALS Fig. 8 Offline performance of GA2 and GAs on an image registration task. task environment. The validation task was the following image registration problem: In order to compare'two graylevel images of a scene taken at different times or from different vantage points, it is often necessary to determine a transformation, or registration, which will map one image, the original image, into another, the target image. This registration problem is important in such diverse fields as aerial photography and medical imaging [20], [21]. One approach to the image registration problem [10] is to define a parameterized class of transformations and to apply a GA to the task of searching this class for an optimal transformation, i.e., a transformation which maps the original image into the target image. The response surface for a given pair of images is a function of the transformation and corresponds to the average gray-level differences between corresponding pixels in the transformed original image and the target image. This class of problems appears to be difficult for conventional nonlinear programming algorithms due to the inevitable presence of many local minima [14]. Experiments were conducted to compare the effectiveness of the algorithms GAs, GA1, and GA2 for a sample registration problem consisting of a pair of carotid artery images in which patient motion produced significant mo- 127 )DI, 'IO, -0. - 10- -0-. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:26:09 UTC from IEEE Xplore. Restrictions apply. IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS, VOL. SMC-16, NO. 1, JANUARY/FEBRUARY 1986 tion artifacts. Each experiment consisted of five runs of a GA on the pair of images, each run evaluating 5000 candidate transformations. Fig. 7 compares the online performance of GAS with GA1. GA1 shows a small but statistically significant improvement in online performance over GAS on this problem. Fig. 8 shows that GA2 produces no significant difference in offline performance over GAS. These results are consistent with the results in Experiments 1 and 2, and indicate that those results may be generally applicable to other optimization problems. VI. CONCLUSION Experiments were performed to search for the optimal GA's for a given set of numerical optimization problems. Previous experiments show that the standard GA, GAS, outperforms several classical optimization techniques on the task environment. Thus, one goal of our experiments was to identify GA's which are at least as good as GAS. Both experiments succeeded in identifying control parameters settings that optimize GA's with respect to the described performance metrics. The experimental data also suggests that, while it is possible to optimize GA control parameters, very good performance can be obtained with a range of GA control parameter settings. The present approach is limited in several ways. First, it was necessary to choose a particular parameterized subclass of GA's to explore. In particular, we have neglected other recombination operators such as multipoint crossover and inversion and other strategies such as the inclusion of a `crowding factor` [6]. Second, the GA's we considered were essentially unconstrained optimization procedures. As previously noted, there are ways to incorporate constraints into GA's but it remains for future research to determine how the presence of constraints affects the optimal control parameters. Finally, the metalevel experiments represent a sizable number of CPU hours. It is encouraging that the results appear to be applicable to a wide class of optimization problems. An alternative approach to the optimization of GA's would be to enable the GA to modify its own parameters dynamically during the search. However, for many optimization problems the number of evaluations which can be performed in a reasonable amount of time would not allow the GA enoughb evaluations to modify its search techniques to any significant degree. Therefore, the experiments described above are important in that they identify approximately optimal parameter settings for the two performance measures considered. The data also suggests several new tradeoffs among the control parameters which may lead to further theoretical insights concerning the behavior of genetic algorithms. REFERENCES [1] A. D. Bethke, `Genetic algorithms as function optimizers,` Ph. D. thesis, Dept. Computer and Communication Sciences, Univ. of Michigan, 1981. [2] L. B. Booker, `Intelligent behavior as an adaptation to the task environment,` Ph. D. thesis, Dept. Computer and Communication Sciences, Univ. of Michigan, Feb. 1982. [3] A. Brindle, `Genetic algorithms for function optimization,` Ph. D. thesis, Computer Science Dept., Univ. of Alberta, 1981. [4] D. J. Cavicchio, `Adaptive search using simulated evolution,` Ph.D. thesis, Dept. Computer and Communication Sciences, Univ. of Michigan, 1970. [5] L. Cooper and D. Steinberg, Methods of Optimization. Philadelphia: W. B. Saunders, 1970. [6] K. A. DeJong, A nalysis of the behavior of a class of genetic adaptive systems, Ph.D. thesis, Dept. Computer and Communication Sciences, Univ. of Michigan, 1975. [7] , `Adaptive system design: A genetic approach,` IEEE Trans. Syst., Man, Cyber. vol. SMC-10, no. 9, pp. 566-574, Sept. 1980. [8] L. C. W. Dixon, `The choice of step length, a crucial factor in the performance of variable metric algorithms,` in Numerical Methods for Nonlinear Optimization, F. A. Lootsma, Ed. New York: Academic, 1972. [9] W. Farrell, Optimization Techniques for Computerized Simulation. Los Angeles: CACI, 1975. [10] J. M. Fitzpatrick, J. J. Grefenstette, and D. Van Gucht, `Image registration by genetic search,` in Proc. of IEEE Southeastcon '84, pp. 460-464, Apr. 1984. [11] L. J. Fogel, A. J. Owens, and M. J. Walsh, Artificial Intelligence Through Simulated Evolution. New York: Wiley and Sons, 1966. [12] M. P. Fourman, `Compaction of symbolic layout using genetic algorithms,` Proc. Intl. Conf. on Genetic Algorithms and their Applications, pp. 141-153, July 1985. [13] D). R. Frantz, Non-linearities in genetic adaptive search, Ph.D. thesis, Dept. Computer and Communication Sciences, Univ. of Michigan, 1972. [14] W. Frei, T. Shibata, and C. C. Chen, `Fast matching of non-stationary images with false fix protection,` Proc. 5th Intl. Conif. Patt. Recog., vol. 1, pp. 208-212, IEEE, 1980. [15] D. Goldberg, `Computer-aided gas pipeline operation using genetic algorithms and rule learning,` Ph.D. thesis, Dept. Civil Eng., Univ. of Michigan, 1983. [16] J. J. Grefenstette, R. Gopal, B. J. Rosmaita, and D. Van Gucht, `Genetic algorithms for the traveling salesman problem,` in Proc. Intl. Conf. Genetic Algorithms and their Applications, pp. 160-168, July 1985. [17] J. H. Holland, Adaptation in Natural and Artificial Systems, Univ. Michigan, Ann Arbor, MI, 1975. [18] `Escaping brittleness,` in Proc. Int. Machine Learning Workshop, pp. 92-95, June 1983. [19] R. B. Hollstien, Artificial Genetic Adaptation in Computer Control Systems, Ph.D. Thesis, Dept., Computer and Communication Sci ences, Univ. of Michigan, 1971. [20] R. A. Kruger and S. J. Riederer, Basic Concepts of Digital Subtraction Angiography. Boston: G. K. Hall, 1984. [21] James J. Little, `Automatic registration of landsat MSS images to digital elevation models,` in Proc. IEEE Workshop Computer Vision: Representation and Control, pp. 178-184, 1982. [22] E. Pettit and K. M. Swigger, `An analysis of genetic-based pattern tracking,' in Proc. National Conf. on Al, AAAI 83, pp. 327-332, 1983. [23] H. H. Rosenbrock, `An automatic method for finding the greatest or least value of a function,` Computer J., vol. 3, pp. 175-184, Oct. 1960. [24] J. Shekel, `Test functions for multimodal search techniques,` in Fifth Ann. Princeton Conf. Inform. Sci. Syst., 1971. [25] S. F. Smith, `Flexible learning of problem solving heuristics through adaptive search,` in Proc. of 8th IJCAI 1983. 128 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:26:09 UTC from IEEE Xplore. Restrictions apply.\"],\n",
    "[4,2,1,\"An Evolution Strategy for Multiobjective Optimization Lino Costa and Pedro Oliveira Departamento de Produsk e Sistemas Escola de Engenharia, Universidade do Minho Campus de Gualtar, 4710 Braga, Portugal { 1aqpno)Qdps. uminho.pt Abstract - Almost all approaches to multiobjective optimization are based on Genetic Algorithms, and implementations based on Evolution Strategies (ESs) are very rare. In this paper, a new approach to multiobjective optimization, based on ESs, is presented. The comparisons with other algorithms indicate a good performance of the Multiobjective Elitist Evolution Strategy. I. INTRODUCTION Solving multiobjective engineering problems is a very difficult task due to, in general, for this class of problems, the objectives conflict across a high-dimensional problem space. Thus, the interaction between the multiple objectives gives rise to a set of efficient solutions, known as the Pareteoptimal solutions. During the past decade, the application of evolutionary algorithms to multiobjective optimization has been investigated by several authors, such as Schaffer [lo], Fonseca and Fleming [5], Horn et al. (41, Srinivas and Deb [12] and Zitzler and Thiele [14]. Almost all approaches are based on Genetic Algorithms (GAS) [2] which were extended in order to track multiobjective problems. On the other hand, implementations based on Evolution Strategies (ESs) [8] are very rare, such as the algorithm proposed by Knowles and Corne [7]. However, the latter approach does not use some traditional features of ESs, namely, the real coding of decision variables and the adaptation of step sizes for mutation. Thus, it is crucial to investigate how to extend ESs to multiobjective optimization, since, in the past, they prove to be powerful single objective optimizers. In this paper, a new approach to multiobjective optimization, based on ESs, is presented. In the new algorithm, an effort was made in order to maintain the main features of traditional ESs as single objective optimizers. Several mechanisms, like elitism, have been introduced in order to improve the algorithm performance, as previously suggested by Zitzler et al. [15] and Van Veldhuizen and Lamont [13]. In section 2, a short introduction to ES is presented. Section 3 describes the Multiobjective Elitist Evolution Strategy (MEES) implemented. Next, the results of the application to several problems are presented. Finally, some conclusions and future work are addressed. Figure I The (p + A) Evolution Strategy Current Next Generation .Generation p Parents 1 Offspring p+1 Offspring p Parents 4 11. EVOLUTION STRATEGIES Evolution Strategies are search procedures that mimic the natural evolution of the species in the natural systems. They were first reported by Rechenberg [8][9] and later by Schwefel [ll]. ESs were developed to solve single objective optimization problems. Like GAS, they work with populations of candidate solutions, requiring only data based on the objective function and constraints, and no derivatives or other auxiliary knowledge. However, ESs work directly with the real representation of the decision variables and the transitions rules are deterministic (in particular, selection is a deterministic procedure). In spite of, traditionally, the search of new points was based on one single operator, the mutation operator, more recently, a recombination operator was introduced. One of the most promising features of ESs is that they use adaptive step sizes for mutation. Figure I illustrates the (p+X)-ES. The (p, X)-ES is similar differing, basically, on the selection procedure. Thus, in (p + X)-ES, at a given generation, there are p parents, and X offspring generated by mutation. Mutation creates new points by adding random normal distributed quantities with mean zero and variance u:. It is important to note that, for each decision variable, an individual standard deviation ui is used (controlling the step sizes). Then, the p+A members are sorted according to their objective function values. Finally, the best p of all the p+X members become the parents of the next generation (i.e., the selection takes place between the p+X members). On the other hand, in (p,X)-ES, the p best of the X members generated become the parents of the next generation (i.e., the selection takes place between the X members). 0-7803-7282402/$10.W0 2 2 IE EE 97 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:32:51 UTC from IEEE Xplore. Restrictions apply. For many problems, X/p x 7 is suggested. During the search, the step sizes for mutation are adapted. Several self-adaptation schemes are possible. One possibility is to actualize the standard deviations oi (for each decision variable) according to the equation: where zi N N(0, Ao2), z N N(0, Ad2) and Ao and Ad are parameters of the algorithm. Schwefel [ll] has reported a remarkable acceleration in the search process, as well as, the facilitation of selfadaptation of parameters by introducing a recombination operator. Basically, the recombination operator consists on, before mutation, to recombine a set of ch+ sen parents to find a new solution. A given number p (1 5 p 5 p) of parents are randomly chosen for recombination. When p = 1 then there is no recombination. Thus, the nomenclature for ESs can now be extended, and ESs with recombination are usually referred as (p/p + X)-ES or (p/p, X)-ES. Two types of recombination are, mainly, considered: intermediate and discrete recombination. In the intermediate recombination, the components of the offspring are obtained by calculating the average of the corresponding components of parents (randomly selected from the population). In the discrete recombination, each component of the offspring is chosen from one of the p parents at random. This procedure allows different combinations of the values of the decision variables from existing solutions in the population. 111. A MULTIOBJECTIVE ELITIST EVOLUTION STRATEGY The MEES approach to multiobjective optimization differs from conventional ESs with respect to the selection operator emphasizing the non-domination of solutions. Non-domination is tested at each generation in the selection phase, thus defining an approximation to the Pareto optimal set. On the other hand, a sharing method is used to distribute the solutions in the population over the Pareto-optimal region. The usual deterministic selection was also modified in order to track multiobjective optimization. The real representation of the decision variables, mutation and recombination operators remain as usual. The step sizes for mutation were adapted with a non-isotropic self-adaptation scheme as in equation 1. A. Fitness Assignment For each generation, all non-dominated solutions of the X or p + X solutions will constitute the 1st front. To these solutions a fitness value of 1 is assigned. In order to maintain diversity, a sharing scheme is then applied to the fitness values of these solutions [l]. Thus, the fitness value of each solution is divided by a quantity, called niche count, proportional to the number of solutions having a distance inferior to a parameter, the u&ore. All distances are measured in objective space. Thereafter, the solutions of the 1st front are ignored temporarily, and the rest of solutions are processed. To the second level of non-dominated solutions is assigned a fitness value equal to 1 plus the worst computed fitness value from the solutions in 1st front. Next, the fitness value of each solution in the 2nd front is divided by the respective niche count value. This process is repeated till all the X or p+X solutions are assigned a fitness value. This fitness assignment process will emphasize the non-domination of solutions, since the fitness values of all solutions in the 1st front will have a value inferior to all the fitness values of solutions in the 2nd front, and so on. Moreover, the co-existence of multiple non-dominated solutions is encouraged by the sharing scheme. B. Selection Operator In the simplest form, at each generation, only p from the X or p + X solutions are selected for next generation. Two situations were considered: if the number of solutions in 1st front, 711, is not greater than p, then a deterministic selection is performed; Otherwise, if nl is greater than p, then a tournament selection is performed. The deterministic selection consists on, after sorting the X or p + X offspring according to their fitness values, to select the p best (the ones with lower fitness values). This selection is obviously similar to the traditional selection of ESs, in the sense that only the best individuals will be present on the next generation. On the other hand, when the number of solutions in the 1st front is high (greater than p) then a selection scheme guaranteing that all nondominated solutions have a possibility of being present in the next generation is adopted. This selection consists on, after sorting the X or p + X offspring, performing a tournament between solutions of the 1st front. The tournament consists on picking two individuals from the offspring and then the best one is selected. C. Elitist Scheme The elitist technique is based on a separate population, the secondary population (SP) composed of all (or a part of) potential Pareto optimal solutions found so far during the search process. In this sense, SP is completely 0-7803-72824OZ$l0.00Q 2002 IEEE 98 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:32:51 UTC from IEEE Xplore. Restrictions apply. independent of the main population and, at the end of the entire search, it contains the set of all non-dominated solutions generated so far. A parameter 0 is introduced in order to control the elitism level. This parameter states the maximum number of non-dominated solutions of SP, the so-called elite, that will be introduced in main population. These nondominated solutions will effectively participate in the search process. If the number of solutions in SP (nsp) is greater or equal than 8, then 0 non-dominated solutions are randomly selected from SP to constitute the elite. Otherwise, only nsp non-dominated solutions are selected from SP to constitute the elite. In the latter case, the elite will only have nsp members. In its simplest form, for all generations, the new potential Pareto optimal solutions found are stored in SP. The SP update implies the determination of Pareto optimality of all solution stored so far, in order to eliminate those that became dominated. As the size of SP grows, the time to complete this operation may become significant. So, in order to prevent the growing computation times, in general, a maximum SP size is imposed. Thus, the algorithm consists on, for all generations, to store, in SP, each Pareto optimal solution 2nd found in the main population if 1. all solutions in SP are different of 2nd; 2. none of the solutions in SP dominates xnd. Next, all solutions in SP that became dominated are eliminated. As mentioned, as the size of SP increases, the execution time and memory requirements also increase. So, it is convenient to keep relatively small sizes of SP. In this sense, the previous algorithm can be modified accordingly. A new parameter d is introduced, stating the minimum desirable distance in objective space between potential Pareto optimal solutions in SP. So, the algorithm is modified by the introduction of the following step: 3. the distance from 2nd to any of the non-dominated solutions in SP is greater than d (euclidean distance measured on objective space). IV. RESULTS Several experiments were carried out in order to study the effect of the parameters of the elitist scheme, as well as, to compare its performance with some other evolutionary multiobjective approaches. ZDT6 (n = 10) x, E [O, 11 i = 1, ..., n A. Test Problems The multiobjective problems were chosen from Zitzler et al. [15]. All problems have two objective functions, no fl(z) = 1 - exp(-4x1)sin6(4xx1) TABLE I MULTIOBJECTIVE PROBLEMS I Problem 1 Objective functions I ZDTl (n = 30) I constraints and the Pareto-optimal solutions are known. Table I describes these problems, showing the number of variables and their bounds. The MEES was applied to each problem with a reasonable set of values for the parameters (no effort was made in finding the best parameter setting for each problem). The initial values for standard deviations (step sizes) and parameters for its self adaptation during the search were the suggested for ESs in single objective optimization. The points in the initial population were generated randomly. Several scenarios were considered in order to study the effects of the recombination operator, the selection mechanism, the elitism and d parameter. Thus, for each scenario all parameters values were kept constant except the feature under study (interaction between parameters was not studied in this phase). B. Metrics of Performance Comparing different multiobjective optimization algorithms is substantially more complex than for the case of single objective optimizers, because the optimization goal itself consists on finding a non-dominated set of solutions that is: a good approximation to the true Pareto optimal set (the distance between the approximation and the true sets should be minimized); e a well distributed set in the objective space. Several attempts can be found in literature to express the above statements by means of quantitative metrics. The metric here considered is described by Knowles and Corne [7] and is based on a statistical method proposed by Fonseca and Fleming [6]. For several executions of the algorithms, a statistical test based on the Mann-Whitney 0-7803-7282402/$10.00 02002 IEEE 99 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:32:51 UTC from IEEE Xplore. Restrictions apply. Figure I1 Results for ZDT2 problem with 5 and 30 variables 1 B \\\\ rank-sum test is applied to the previous collected data. The results of a comparison can be presented in a pair [a, b], where a is the percentage of the objective space on which algorithm A was found statistically superior to B, and b gives the similar percentage for algorithm B. Thus, a is the percentage of the objective space where algorithm A is 'unbeaten' and, b is the percentage of the objective space where algorithm B is 'unbeaten'. So, typically, if a x b x 100% then the algorithms A and B have similar results. For all results presented in the paper the statistical significance is at the 5% level and 1000 sampling lines were used. C. Influence of Recombination The MEES without the recombination operator seems to have difficulties in obtaining a well distributed set of non-dominated solutions when applied to multiobjective problems with a high number of variables. This is illustrated by Figure 11, which represents the non-dominated solutions obtained, in one single run, for ZDT2 problem with 5 and 30 variables for an (100+150)-ES without any recombination and, with a,ha,,=0.027, d = 0 and 8 = 0. The stopping criterion was to terminate the execution after 250 generations. It is clear that a good definition of the approximation to the Pareto-optimal set was obtained for the ZDT2 problem with 5 variables. However, for 30 variables, the results are poor, in the sense, that the solutions are far from the true Pareto-optimal front and, they are not uniformly distributed in the objective space. Since MEES without recombination seems to perform poorly for large dimensional multiobjective problems, several scenarios of MEES with recombination were tested. Scenarios that combine the most popular recombination schemes were considered: without any recombination (NOrec scenario); intermediate recombination on variables and stan- TABLE I1 INFLUENCE OF RECOMBINATION (ZDT1 PROBLEM) Figure 111 Results for ZDT2 problem with and without recombination I i ')i 1 nbl dard deviations (IIrec scenario); intermediate recombination on variables and discrete recombination on standard deviations (IDrec scenario); discrete recombination on variables and intermediate recombination on standard deviations (DIrec scenario); discrete recombination on variables and standard deviations (DDrec scenario). For scenarios with recombination, an (100/100,25O)-ES was applied (obviously, an (100,25O)-ES was considered when no recombination exists) with a,ha,,=0.027, d = 0 and 8 = 0. As before, the stopping criterion was to terminate the execution after 250 generations. For each scenario, the MEES was executed 30 times. Table I1 presents the results obtained for all scenarios for the ZDTl problem. All scenarios were compared in pairs using the statistical technique as previously described. It is clear that the best results were obtained for DDrec scenario, i.e., when discrete recombination is applied to decision variables and standard deviations. The MEES with discrete recombination on variables and standard deviations (an (100/100,25O)-ES) can now be compared with the performance of MEES without any recombination for the ZDT2 problem. The comparison is illustrated by Figure 111, which represents the non-dominated solutions obtained in one single run after 250 generations. The approximation to the Pareto-optimal front obtained with the MEES with recombination, was far better than the obtained without any recombination. 0-7803-7282402/$10.00 02002 IEEE 100 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:32:51 UTC from IEEE Xplore. Restrictions apply. TABLE 111 INFLUENCE OF ELITISM (ZDT1 PROBLEM) ZDTl HLGA VEGA NSGA MEESo [100,8.3] [lOO,ll.4] [100,1.3] HLGA - [75.2,77.5] [8.8,100] VEGA [ l Z . O , l O O ] NSGA S P E A Figure IV Results for ZDTZ problem for 0 = 0 and 0 = 10 .___ .____ LDRPm` (1wloollWKs 0- = QSQ7 d V oax . O B X = SPEA MEESlo [1.7,100] [1.7,100] [8.3,100] [8.3,100] [11.3,100] (11.2,100] [1.3,100] [1.3.100] II inn1 u ` ` I*) -. ZDT2 MEESo HLGA VEGA NSGA SPEA D. Influence of Elitism In order to study the influence of the elitism level, an (100/100,25O)-ES with discrete recombination on variables and standard deviations was applied to the ZDTl problem. The same values for the parameters were considered with the exception of 8, which was varied from 0 to 100 as in Table 111. The d parameter was fixed equal to 0 in order to guarantee that in SP all non-dominated solutions found during the search are present. This table shows that for increasing values of 8 there is a degradation of the performance of the algorithm, due to the lack of diversity in main population. However, it is also clear that, for the values of 8 tested, the best results were obtained with elitism. Furthermore, consistently, the best results were obtained with 8 = 10. The comparison between different levels of elitism is illustrated by Figure IV, which represents the non-dominated solutions obtained in one single run, after 250 generations for the ZDT2 problem, with 8 = 0 and 8 = 10. It is clear that the approximation to the Pareto-optimal front obtained with 8 = 10 was far better than with 8 = 0. HLGA VEGA NSGA SPEA MEESln [100,16.3] [100,3.3] [100,3.5] 117.8.97.31 [1.5,100] -~ E. Comparison with other algorithms The elitist ES was compared with four algorithms for the test problems (ZDT1 to ZDT6 problems). These results were published by Ziztler et al. [15]. The algorithms considered here are: HLGA: Hajela and Link weighted-sum based a p VEGA: Vector Evaluated Genetic Algorithm [lo]; proach [3]; ZDT3 MEESo HLGA VEGA NSGA HLGA VEGA NSGA SPEA MEESln [100,13.7] [100,6.6] [100,2.4] [2.4,100] [2.7,100] [55.9,82.9] [14.3,100] [13.7,100] 115.6,lOOI [6.9,100] [6.6,100] [7.3,100] [2.4,100] [2.7,100] TABLE V COMPARISON BETWEEN ALGORITHMS (ZDTZ PROBLEM) ZDT4 HLGA VEGA NSGA SPEA MEES~ HLGA VEGA NSGA SPEA MEESln [87.6,47.4] [33.1,100] [33.1,100] [32.7,100] [82.2,100] [16.7,100] (13.7,lOOI [i00,32.7] [i00.9.8] [100,16.7] [ino,i3.7] [ioo,z7.9] [10.6,1001 [10.6,100] [9.8,100] NSGA: Nondominated Sorting Genetic Algorithm [12]; SPEA: Strength Pareto Evolutionary Algorithm [14]. For MEES, an (100/100,150)-ES with discrete recombination in variables and standard deviations was considered. The MEES was applied without and with elitism (MEESo and MEESlo, respectively). The d and gshare parameters were fixed equal to 0 and 0.027, respectively. The stopping criterion was to terminate the search after 100 generations. As described with more detail in [15], for algorithms HLGA, VEGA, NSGA and SPEA, the population size was 100 (for SPEA the population size was 80 with an external non-dominated set of 20 points). The crossover and mutation rates were 0.8 and 0.01, respectively. The maximum number of generations was 250. The niching parameter was fixed in 0.48862. All algorithms were executed 30 times for each test problem and, for each run, the set of all non-dominated solutions generated during the entire search was taken as the outcome of one optimization run (off-line performance). The number of objective function evaluations was the same for algorithms HLGA, VEGA, NSGA and SPEA (approximately, 25000 evaluations). The number of objective function evaluations required by MEES was inferior than the other algorithms (approximately, 15000 evaluations). Tables IV to VI11 present the results of compari- TABLE VI COMPARISON BETWEEN ALGORITHMS (ZDTI PROBLEM) I SPEA I - I I - I - [ [0.9,100] J 0-7803-7282-4/02/s10.00 02002 IEEE 101 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:32:51 UTC from IEEE Xplore. Restrictions apply. TABLE VI11 COMPARISON BETWEEN ALGORITHMS (ZDTG PROBLEM) TABLE IX GLOBAL PERFORMANCE son of these algorithms with MEES. Two classes of algorithms can be distinguished, those that do not use elitism (HLGA, VEGA NSGA and MEESo) and, those that use, explicitly, elitism in the search (SPEA and MEESlo) . Thus, considering only the results obtained with non elitist algorithms, the best results were obtained by MEESo for all test problems considered. Moreover, MEESo has outperformed the elitist approaches SPEA in two problems (ZDT4 and ZDTG problems) and MEESlo in one problem (ZDT4 problem). However, MEESlo has beaten SPEA in all the problems considered. Table IX resumes the results obtained by different algorithms for all problems. The values in the table, for each algorithm, are the median percentage of the objective space that is ’unbeaten’ when compared with the remaining algorithms. From this table, it is clear that, in general, the elitism is useful in guiding the search. The best non elitist algorithm seems to be MEESo. SPEA, globally, outperformed MEESlo on ZDTG problem. V. CONCLUSIONS AND FUTURE WORK In this work, a new Elitist Evolution Strategy for multiobjective optimization was presented. This approach incorporates the main features of traditional single objective Evolution Strategies, like real representation of the decision variables and self-adaptation of step sizes. The algorithm was tested on several test problems in order to investigate the influence of some factors on its performance, as well as, to compare its performance with other multiobjective evolutionary approaches. As expected, the results indicated that recombination and elitism are essential for obtaining good approximations to the Pareto-optimal front. The Multiobjective Elitist Evolution Strategy without elitism (MEESo) outperformed the other non elitist approaches (HLGA, VEGA and NSGA) for all the test problems considered. The results of the Multiobjective Elitist Evolution Strategy with elitism (MEESlo) and SPEA were similar. It should be noted that the number of function evaluations required by MEES is inferior than the others algorithms being compared. Future work will concentrate on the study of parameters like population sizes, initial step sizes and selfadaptation schemes. The influence of the parameter that controls the density of points in the approximation set to the Pareto-optimal front will also be investigated. References [l] K. Deb and D. Goldberg. ”An investigation of niche and species formation in genetic function optimization”. Proceedings of the Third International Conference on Genetic Algorithms, pp. 42-50, 1989. D. Goldberg. Genetic Algorithms in Search,Optimization, and Machine Learning. Addison-Wesley,l989. P. Hajela and C.-Y. Lin. ”Genetic search strategies in multicriterion optimal design”. Structural Optimization, 4, pp. J. Horn, N. Nafploitis and D. Goldberg. “A niched Pareto genetic Algorithm for multi-objective optimization”. Proceedings of the First IEEE Conference on Evolutionary Computation, pp. 82-87, 1994. [5] C. M. Fonseca and P. J. Fleming. ”Genetic Algorithms for multi-objective optimization: Formulation, discussion and generalization”. Proceedings of the Fifth International Conference on Genetic Algorithms, s. Forrest, Ed. San Mateo, CA: Morgan Kauffman, pp. 416423, 1993. C. M. Fonseca and P. J. Fleming. ”On the Performance Assessment and Comparison of Stochastic Multiobjective Op timizers” Parallel Problem Solving from Nature IV, H.-M. Voigt, W. Ebeling, I. Rechenberg, H.-P. Schwefel, Eds Berlin: Germany: Springer, pp. 584-593, 1995. J. D. Knowles and D. W. Corne. ”Approximating the Nondominated Front Using the Pareto Archived Evolution Strategy”. Evolutionary Computation, Vol. 8(2), pp. 149-172, 2OOO. I. Rechenberg. Evolutionsstrategie ’94. Fkommann-Holzboog, Stuttgart, 1994. I. Rechenberg. Cybernetic Solution Path of an Experimental Problem. Royal Aircraft Establishment, Library Tkanslation No. 1122, Farnborough, England, 1964. [lo] J. D. SchafTer. ”Multiple objective optimization with vector evaluated genetic algorithms”. Proceedings of the First International Conference on Genetic Algorithms, J. J Grefensttete, Ed. Hillsdale, pp.93-100, 1985. [ll] H-P. Schwefel. Evolution and Optimal Seeking. John Wiley and Sons, 1995. [12] N. Srinivas and K. Deb. ”Multi-Objective function optimization using non-dominated sorting genetic algorithms”. Evolutionary Computation, Vol. 2, pp. 221-248, 1995. [13] D. Van Veldhuizen and G. Lamont. ”MultiObjective Evolutionary Algorithms: Analysing the State-of-the-Art”. Evolutionary Computation, Vol. 8(2), pp. 125-147, 2000. [14] E. Zitzler and L. Thiele. ”Multiobjective evolutionary algorithms: a comparative case study and the strength pareto a p proxh” . IEEE %nsactions on Evolutionary Computation, 3(4), pp. 257-271, 1999. [15] E. Zitzler, K. Deb and L. Thiele. ”Comparison of multiobjective evolutionary algorithms: Empirical results”. Evolutionary Computation, Vol. 8, pp. 173-195, 2000. (21 [3] 99-107, 1992. [4] [6] [7] [8] [9] 0-7803-7282-4/02/$10.00 02002 IEEE 102 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:32:51 UTC from IEEE Xplore. Restrictions apply.\"],\n",
    "[5,2,1,\"1171 D-3 Arithmetical crossover A Evolution Strategy : A New Time-Variant Mutation for Fine Local Tuning M.M.A. Hashem, Keigo Watanabe and Kiyotaka Izumi Department of Production and Control Technology Faculty of Engineering Systems and Technology Saga University, 1-Honjomachi, Saga 840, .Japan E-mail : wat anab e@me .saga-u .ac. j p Abstract: The inherent strength of the Evolution Strategy (ES) algorithm lies in the mutation operation. In this paper, a new Time-Variant Mutation (TVh4) operator is proposed for fine local tuning and high precision solutions. Its action depends upon the age of the populations and its performance is quite different from the Uniform Mutation (UM) operation. The proposed approach makes it possible that the size of mutation steps (i.e., standard deviations) can be dynamically adjusted to the so called `Evolution Window`. The efficacy of the proposed TVM operator is illustrated by solving the dynamic linear-quadratic control problem (a discrete time optimal control model). Key Words: Evolution Strategy; Evolutionary Computation; Linear-quadratic Control Problem; Discrete-Time Optimal Control; Mutation; Arithmetical Crossover; Intelligent Systems. 1. Introduction The Evolution Strategy (ES) imitates the effects of natural evolution as to solve parameter optimization problems numerically. The basic difference between evolution strategy and Genetic Algorithm (GL4) lies in their domains (i.e., the representation of individuals). ES generally uses to represent an individual as float-valued vectors instead of a binary string. The main objective behind such implementations is to move the algorithm closer to the problem space. Such a move forces, but also allows, the operators to be more problem specific - by utilizing some specific characteristics of real space. Moreover, this type of representation reduces the burden of converting genotype to phenotype during evolution process. ESs started with integer variables as an experimental optimum-seeking method, but turned to real variables when used on computers I). solutions for a particular problem. TJnlike GA, the ES produces a higher precision However, Generation of I initial population I 1 End I Fig. 1 The structure of an evolution strategy both ES and GA are evolution programs. They could be termed as probabilistic algorithms which maintain a population of individuals, II(t) = SICE '97 July 29-31, Tokushima - 1099 PR0001-3/97/0000-1099 $4.00 0 1997 SICE Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:32:30 UTC from IEEE Xplore. Restrictions apply. {+;, . . . , +ct`,} for generation t. Each individual represents a potential solution to the problem at hand, and, in any evolution program, is implemented as some (possibly complex) data structure 6 (chromosome representation). Each solution +F is evaluated to produce some measure of its 'Yitness`. Then, a new population (generation t + 1) is formed by selection (reproduction), crossover and mutation (as only recombination operator). The members of the new population undergo transformations by means of `genetic` operators to form new solutions. There are higher order transformations C; (crossover type), which create new individuals ([; : <X [ H 6) and a unary transformations, [i (mutation type), which create new individuals with a small change by zero-mean Gaussian noise in a single individual ([: : [ ~--r [). After some number of generations the program converges - it is hoped that the best individual represents a near optimum (reasonable) solution. The structure of an evolution strategy is shown in Fig. 1. However, further studies on various factors affecting the ability of evolutionary techniques to solve optimization problems are necessary '1. Generally, a Uniform Mutation (UM) is used throughout the evolutionary process. But the frequency of success and the degree of progress of the search by ES largely depends upon the mutatuon steps. By the study on UM steps (standard deviations); the authors observed that with the larger mutation steps cause to the fast convergence with low precision results, and the smaller mutation steps take too much generations to converge and sometimes it falls into the local minima. So, it is very difficult task to set up an optimum mutation step for the problem at hand. However, the theory of ES led to the discovery of the so called `Evolution Window`. The meaning of this term is that changes (mutational jumps) lead to evolutionary progress only when they lie within a narrowly confined and calculable step-width band. Mutation steps and recombination steps which fall outside the evolution window are ineffective '1. Thus, it is apparent that an optimal compromise must exists between the frequency of success and the degree of progress in the mutation of the evolutionary process. Therefore, the choice of a suitable mutation step, especially in the presence of many variables, becomes a decisive factor for the convergence of the evolution strategy 3). The objective of this paper is to find a suitable dynamic mutation strategy which can make a balance between the success and progress of the ES search. Therefore, we proposed a Tinie- Variant Mutation (TVM) whose performance is quite different from the UM. Its action depends upon the age of the populations. Moreover, it can produce such a mutation step (standard deviation) that falls within the evolution window. This type of mutation operator causes to search the problem space uniformly initially and very locally at the later stages. Thus, it possesses the fine local tuning property and produces very high precision solutions. This immensely important approach makes it possible that, in the process of optimization, the size of the mutations (standard deviations) can be dynamically adjusted to the evolution window. To test the efficacy of the proposed TVM operator, we solved the dynamic linear-quadratic control problem (a discrete time optimal control model) 2, for different controlled steps. The simulation results indicate that the proposed TVM operator outperforms the UM in convergence and fine local tuning within specified generations. 2. The Evolution Strategy 2.1 Initial Population The initial population will be generated by using a uniform random numbers (URN). If we would like to generate a variable u1 within the rage -100 5 u1 5 100, then we must use the URN[-100,100]. Thus, for other variable 212 with an altenative rage, we should use other URN. Thus, the first individual for the population = [u12,1 2,. . . , un]* is generated. Similarly, the remaining individuals for the population $2, - , +, are generated using the same manner, where p denotes the number of individuals in the population. 1100 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:32:30 UTC from IEEE Xplore. Restrictions apply. Optimal 212 t Subpopulation 1 a- point Fig. 2 Max-mean arithmetical crossover with subpopulation’s elite 2.2 Subpopulation- based Arithmetical Crossover The authors already have developed a method in which the competing subpopulation’s (subgroup’s) elite and the mean strength of that subpopulation except the elite are used in the crossover technique ‘). This technique has a very strong directivity to the elite as shown in Fig. 2. Among these competing subpopulations (subgroups), there are different directivity towards the optimum. So, there is less possibility to be trapped into local minima to attain the optimal point. The method is described below vividly. We devide the p populations into the equal sized 1 competing subpopulations. We define as an elite individual that maximizes a coljt function within the j-th subpopulation and 4, as a mean strength of the j-th subpopulation except the +,,max, we define the crossover for the competing subpopulation j as follows : c1 = Q+J” + (1 - Q)4] c z = (1 - Q)+],“ + Q& (1) (‘2) where Q is selected from URN[O,l]. Note that the children are generated for each subpopulation by using ( I t I)-ES. 2.3 Mutation ‘The mutation; recombination operator, plays a significant role in global search and fine tuning for the ESs. It is observed that the smaller changes occur more often than larger ones in biological evolutions. According to this observation, ES uses Gaussian noise with zero-mean to perturb all object variables of a child. The mutation for a child i, is made as where N ( . ) is a zero-mean Gaussian random number vector and a denotes the standard deviation, a = [ L T ~, . . . ,a,] T . When a is kept fixed throughout the evolution process, we call this type of mutation as uniform mutation. 2.4 Evaluation After mutation operation, each child is evaluated to its cost function (fitness) for a possible solution in each generation. These evaluations are preserved for creating a new generation. 2.5 Alternation of Generation In the alternation of generation, (p + p)-ES is used. That is, among p parents, which were evaluated at the former generation, and p children which are evaluated in the current generation, the p t p individuals are ordered in proportional to the amount of the cost function, and the best p individuals are selected for the next generation. 3.. The Time-Variant Mutation The inherent strength of the ES algorithm, towards convergence and high precision results, lies in the choice of the mutation steps (standard deviations). A special dynamic Time-Variant Mutation (TVM) operator is proposed aiming at both improving the fine local tuning and reducing the disadvantage of uniform mutation in the ES algorithm. The TVM is defined for a child i, as e: = c; + iv(o,~z(t)) (4) where a( t )= [a( t ) ,.. . , a(t)lTi s the time-variant standard deviation vector at the generation t , and g ( t ) is defined as - 1101 - Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:32:30 UTC from IEEE Xplore. Restrictions apply. Fig. 3 Characteristics of the a(t) with y = 6.0 and T = '200 where T is selected from the URN[O,l], T is the maximal generation number, y is a real-valued parameter determining the degree of dependency on the progress and success of the evolution. Normally, the value of y should be selected as greater than unity. However, using the value of y more than unity can produce high precision final results in presence of many variables. With the value of y less than unity, it may not be able to converge to the final results in presence of many variables and this may cause to decrease the success and progress of the evolution process. The function a(t) returns a value in the range [O: 11: which falls within the evolution tvindow such that the probability of a(t) being close to 0 increases as age of the population, t increases. This property of a(t) causes to search the problem space uniformly initially (when t is small) and very locally at t larger stages. Thus, it increases the probability of generating the new mutation step very closer to its successor mutation step than a uniform mutation choice for the whole evolution process. The generation of a typical time-variant standard deviation with y = 6.0 is shown in Fig. 3. 4. Simulations In general, it is very difficult task to design and implement the algorithms for the solution of optimal control problems, especially for nonlinear or large-scale systems. There exists some algorithms in the literature for these class of problems. However, these algorithiiis break down on problems of moderate size and complexity, suffering from what is called `the curse of diniensionality`. Optimal control problems are quite difficult to deal with numerically, even though the system is linear. So, the following linear discrete-time optimal control model has been chosen as the test bed for our proposed method. 4.1 The Test Bed For the siiiiulation, we have selected the following dynamic one-dimensional linear-quadratic control problem '). The problem is to niinimize the following cost function: N-I iv=o subject to ~ k + 1 U X +~ 6 ~ k , k = 0,1 , .. . , (iV - 1) (7) where 20 is a given initial state, a, 6, q, s, T are given constants, Xk E R is the state, and uk E R is the control input of the system. iV is the total number of control steps involved in the system. The optimal value of (6) subject to (7) can be analytically expressed as where K k is the solution of the Riccati equation: In what follows, the problem (6) subject to (7) will be solved for the sets of parameters given in the Table 1. The exact solutions with these sets of parameters are 16179.775281, 16180.339850, 16180.339887 and 16180.339887 for the control step sizes, iW = 5, 10, 15 and 20 respectively. - 1102 - Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:32:30 UTC from IEEE Xplore. Restrictions apply. Table 1 The simulation cases 4. '2 Imp 1 em en t a t i o n 'The proposed algorithm is implemented by using a population size of 60. The competing subpopulation size, I is selected as 6. An individual is represented by + = [U(O), U( l), . . . , U ( N - 1)) (10) The chromosomes, the control inputs U , are represented by floating-point vectors. Each element of the chromosome is initially generated randomly but within desired domain, and the operators are carefully designed to preserve this domain during evolution process. For the comparison with TVM, the mutation steps (standard deviations) for UrVl is chosen as 1.0 and 0.1. To observe the effects of the degree of dependency on the evolution process, the parameter, y is selected as '2.0,4 .0 and 6.0. The maximal generation number, T is selected as 300, 700, 1000 and 1500 for the control steps, N = 5, 10, 15 and 20 respectively. When the cost (fitness) function (6) subject to (7) is minimized, the desired control inputs, U are found. 4.3 Results The linear-quadratic control problem is solved for different control steps, N = 5, 10, 15 and 20 using the proposed TVM operator as well as UM operator with competing subpopulation size. 6. The presented results are the outcome of the average of the 10 independent runs with different sample paths. These results are shown in Figs. 4-7. In all the test cases, we used G = 1.0 and 0.1 for the UM. The intuitive reason behind it was, to observe the progress and success of TVM against the UP1 operator. The results, in Figs. L. Ll I I I 0 100 200 300 Generations Fig. 4 The evolution history with N = 5 4-7, indicate that with B = 0.1. the UM operator can't cause to converge within specified generations but when B = 1.0, it causes to converge merely with poor results which is undesirable for the high precisioned problems. 0 therwise, the TVM operator causes to converge successfully with very accurate results within specified generations. With the increasing control step sizes, the behavior of the TVM operator is almost same but the UM operator produces much more poor results, compared to the TVSI. As expected earlier, the TVM operator causes to search very globally initially and very locally at the final stages. The evolution histories (Figs. 4-7) shows that the effects on the selection of the value of y is obvoius. When y is selected as '2.0 and -1.0,it took more generations to converge but when y is selected as 6.0, it converged relatively faster to the optimal point. Moreover, with the increasing value of y causes to search the problem space uniformly initially and very locally at the final stages. For the most optimaization problems. the time necessary for an algorithm to converge to the optimum depends on the number of decision variables. The evolution histories (Figs. 4-71 of the problem indicate that the TVM operator causes to converge before 300, 700, 1000 and 1.500 generations for the control step sizes. N = 5. 10, 1.5 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:32:30 UTC from IEEE Xplore. Restrictions apply. € 1 I I I I I 1 I lo--$ 1 t l I I I I I I 0 200 400 600 Generations Fig. 5 The evolution history with A T = 10 E w I ' 3 Fig. 6 The evolution history with N = 1.5 and 20 respectively. This type of computation efforts are quite reasonable for the problem in the test bed. 5. Conclusions In this paper, we have studied a new mutation operator, TVM to improve the fine local tuning capabilities of the ES algorithm. Its performance is evaluated with the discrete-time optimal control problem against the UM. The simulation results indicate that the ES algorithm using the TVM operator clearly outperforms the Ghl with respect to the accuracy of the found F I I I 0 500 1000 1500 Generations Fig. 7 The evolution history with fV = 20 optimal solution. Moreover, it also converged reasonably faster to that solution indicating the capacity of local fine tuning. So, an encouraging agreement was found between the success and progress of the evolution process. Thus, the inherent strength of the ES algorithm is increased substantially. References 1) Schwefel, Hans-Paul, `On the Evolution of Evo- 1 u t ion ary Comp ut at ion ,` In Computational Intelligence: Imitating Life, IEEE Press, New York, pp.116-124, 1994. Michalewicz, Zbigniew, `Genetic Algorithms + Data Structures = Evolution Programs,`-3rd. rev.. and extended edition, Springer-Verlag, Berlin, 1996. 2) 3 ) Rechenberg, Ingo, `Evolution Strategy,` In Computational Intelligence: Imitating Life, IEEE Press, New York, pp.147-159, 1994. Izumi, K.,H ashem, M.M.A. and Watanabe, K., `An Evolution Strategy with Competing Subpopulations,` Submitted in 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation (CIRA '97), Itlonterey, California, USA, 1997. 4)\"],\n",
    "[6,2,1,\"This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. IEEE TRANSACTIONS ON CYBERNETICS 1 Evolution Strategy-Based Many-Objective Evolutionary Algorithm Through Vector Equilibrium Kai Zhang, Zhiwei Xu , Shengli Xie , Fellow, IEEE, and Gary G. Yen , Fellow, IEEE Abstract.In recent years, numerous many-objective evolutionary algorithms (MaOEAs) have been developed to search for well-diversified and well-converged Pareto optimal solutions for high-dimensional many-objective optimization problems (MaOPs). However, existing MaOEAs have to tackle some daunting challenges, including the emergence of dominance resistance solutions, effective diversity preservation scheme, management of a large population size, extremely high computational complexity, sensitivity to the shape of Pareto front (PF), and overly relying on high-quality reference points. In this article, we present an evolution strategy (ES) for solving MaOPs, called MaOES, which can solve these challenges efficiently and effectively. Inspired by the Vector Equilibrium phenomenon in magnetic fields, isotropic magnetic particles would automatically repel from each other, keep the uniform distance from the nearest neighbors, and extend the entire magnetic fields as far as possible, all at the same time. In the proposed algorithm, an efficient self-adaptive Precision-Controllable Mutation operator is designed for individuals to explore and exploit the decision space. In addition, the Maximum Extension Distance strategy, which emulates the isotropic magnetic particle behavior in a magnetic field, is developed to guide individuals to keep uniform distance and extension to approximate the entire PF. As a result, the MaOES can obtain a well-converged and well-diversified PF with much less population size and far lower computational complexity. The larger the number of individuals, the sharper the contour the resulting approximate PF will be. Finally, the proposed algorithm is evaluated by the scalable MaOPs test suites on DTLZ and WFG. The experimental results have been demonstrated to provide a competitive and oftentimes better performance when compared against some chosen state-of-the-art MaOEAs. Manuscript received August 5, 2019; accepted December 12, 2019. This work was supported by the National Natural Science Foundation of China under Grant 61472293, Grant 61702383, Grant U1803262, and Grant 61602350. This article was recommended by Associate Editor Q. Zhang. (Corresponding author: Gary G. Yen.) Kai Zhang is with the School of Computer Science and Technology, Wuhan University of Science and Technology, Wuhan 430065, China (e-mail: zhangkai@wust.edu.cn). Zhiwei Xu is with the Hubei Province Key Laboratory of Intelligent Information Processing and Real-Time Industrial System, Wuhan University of Science and Technology, Wuhan 430065, China (e-mail: kenxucn95@gmail.com). Shengli Xie is with the Key Laboratory of Intelligent Detection and The Internet of Things in Manufacturing, Ministry of Education, Guangzhou 510006, China, and also with the Guangdong-Hong Kong-Macao Joint Laboratory for Smart Discrete Manufacturing, Guangdong University of Technology, Guangzhou 510006, China (e-mail: shlxie@gdut.edu.cn). Gary G. Yen is with the School of Electrical and Computer Engineering, Oklahoma State University, Stillwater, OK 74078 USA (e-mail: gyen@okstate.edu). Color versions of one or more of the figures in this article are available online at http://ieeexplore.ieee.org. Digital Object Identifier 10.1109/TCYB.2019.2960039 Index Terms.Evolution strategy (ES), many-objective evolutionary algorithm (MaOEAs), many-objective optimization problem (MaOPs). I. INTRODUCTION MANY real-world problems involve multiple conflicting objectives that need to be optimized simultaneously. In the last decades, some of the carefully crafted multiobjective evolutionary algorithms (MOEAs), such as NSGA-II [1] and SPEA2 [2], have shown an extraordinary ability to search for a set of well-converged and welldiversified nondominated solutions in two- or three-objective optimization problems. However, these effective MOEAs fail in tackling real-world applications involving four or many more objectives optimization, such as engineering design [3], air traffic control [4], auto controller [5], and nursing staff scheduling [6]. In recent years, some state-of-the-art designs on manyobjective evolutionary algorithms (MaOEAs) have been proposed for solving many-objective optimization problems (MaOPs), including improved diversity-based approaches (e.g., GrEA [7]), enhanced dominance-based approaches (e.g., FD-NSGA-II [8]), decomposition-based approaches (e.g., NSGA-III [9], MOEA/DD [10]), indicator-based approaches (e.g., HypE [11]), objective reduction-based approaches (e.g., PCSEA [12]), and evolution strategy (ES)-based approaches (e.g., S3-CMA-ES [13]). However, when the number of objectives increases, an enormously large number of solutions becomes nondominated. These dominance resistance solutions seriously weaken the selection pressure toward the Pareto front (PF), and the convergence ability of most of the MaOEAs quickly deteriorates. In order to maintain a set of well-diversified solutions which approximates the entire PF, some of the MaOEAs need to increase the number of reference points or the number of search directions, or keep the extreme solutions to extend the boundary. However, without any priori PF shape knowledge, it is very difficult to generate high-quality reference solutions or identify the boundary solutions for high-dimensional MaOPs. ES has been proven for years to be a simple, yet powerful approach for the optimization problems in particular because of its self-adaptation mechanisms [14], [15]. The ES has been widely applied for solving various multiobjective optimization problems. In order to obtain well-diversified solutions, many diversity approaches have been well integrated into ES, such as niching [16], crossover-like mutation [17], clustering [18], 2168-2267 \u0002c 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:33:17 UTC from IEEE Xplore. Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. 2 IEEE TRANSACTIONS ON CYBERNETICS and archiving [19], [20], to name a few. However, most of these designs still need to maintain a large population size for sorting, selection, and reproduction. In addition, the excessive success of the genetic algorithms (GAs) has inadvertently led to the lost attention on the powerful ES. In this article, we propose a novel ES for solving MaOPs, called MaOES. In 1917, Richard Buckminster Fuller discovered the significance of vector symmetry and called it the Vector Equilibrium in 1940 [21], [22]. Inspired by the Vector Equilibrium phenomenon in magnetic fields, isotropic magnetic particles would repel each other and extend to the entire magnetic fields as far as possible and automatically. Under this spirit, the proposed MaOES can obtain a wellconverged and well-diversified PF with much less population size and far lower computational complexity. The larger the number of individuals, the sharper the contour the resulting approximate PF will be. The proposed algorithm uses mutation and selection for individual self-adaptation. The Precision Controllable Mutation operator is designed for individuals to explore and exploit the decision space efficiently. The Maximum Extension Distance strategy is tailored to guide the individuals to keep uniform distance among particles in the population and to facilitate the extension to approximate the entire PF automatically. The remaining sections complete the presentation of this article. Section II provides a comprehensive analysis of the existing MaOEAs. The proposed ES for MaOPs and MaOES is then detailed in Section III. In Section IV, we elaborate on the experimental results given selected benchmark test problems. Finally, conclusions are drawn in Section V along with pertinent observations that are identified. II. LITERATURE REVIEW The mathematical model of an MaOP can be formulated as follows: min f (x) = min\u0002f1(x), f2(x), . . . , fM(x)\u0003T , f (x) ¸ RM (1) where x = [x1, x2, . . . , xN]T ¸ \u0002, and x consists of N decision variables, \u0002 is the search space. f (x) consists of M objective functions, fi (x), i = 1, . . . ,M and M > 3. RM denotes the objective space. In recent years, a number of novel and effective algorithms for MaOPs have been proposed, such as particle swarm optimization with a balanceable fitness estimation for many-objective optimization [23], manyobjective optimization using differential evolution with variable-wise mutation restriction [24], MaOEA using a oneby- one selection strategy [25], and set-based GA for interval many-objective optimization problems [26]. Existing strategies of MaOEAs can be broadly classified into several different categories, including improved diversity approaches, enhanced dominance approaches, decomposition-based approaches, indicator-based approaches, objective reduction approaches, and ES-based approaches. A. Existing Strategies for MaOEAs 1) Improved Dominance or Diversity Approaches: The enhanced dominance approaches, such as FD-NSGA-II [8] and ƒÃ-MOEA [27], replace the exact Pareto dominance with some relax dominance definitions, which can enhance the selection pressure toward the PF. The improved diversity approaches, including GrEA [7] and SPEA2 + SDE [28], attempt to improve the performance of MaOEAs by reducing the adverse impact of diversity maintenance. However, the delicate balance between convergence and diversity is indeed difficult to maintain throughout the evolution process for these MaOEAs. Excessive aggressive selection pressure may result in degraded diversity maintenance. On the other hand, excessive diversity selection may deteriorate the convergence performance. 2) Decomposition-Based Approaches: The decompositionbased MaOEAs are characterized by systematically generating uniformly distributed normalized weight vectors or reference points. These approaches search for Pareto optimal solutions along each reference vector or reference point. The representative designs include MOEA/DD [10], NSGA-III [9], and RPD-NSGAII [29]. However, the decomposition-based MaOEAs have to maintain an exponentially increasing number of search directions given the increasing number of objectives. The decompositionbased approaches often show high sensitivity to the shape of PF [30], especially for those MaOPs with degenerative PFs. In addition, there exists an infinite number of possible shapes of PF surface in high-dimensional objective space. Without a priori knowledge of PF hyper-surface, it is very difficult to generate high quality reference solutions for MaOPs. 3) Indicator-Based Approaches: The indicator-based approaches adopt accurate or estimated indicator values to guide the search process for solving MaOPs. This category includes HypE [11] and IBEA [31]. However, the indicator-based MaOEAs do have their own issues. First, the computational cost is very expensive for the exact hypervolume calculation. Second, the indicator-based MaOEAs encounter difficulties to generate a set of uniformly distributed solutions. Third, it remains a challenge to select the most appropriate reference points for various indicator-based MaOEAs. 4) Objective Reduction Approaches: The objective reduction approaches consist of finding the relevant objectives and eliminating the redundant objectives. Typical representatives are PCSEA [12], L-PCA, and NLMVU-PCA [32]. However, these methods can reduce the computational load but potentially lose some information as a result of the reduced objectives. Moreover, such techniques are only applicable to problems having a moderate number of conflicting objectives. 5) Evolution Strategy-Based Approaches: The ES approaches apply the self-adaptive mutation mechanism for solving MaOPs. Specifically, niching or archive maintenance methods are integrated to maintain well diverse solutions. Some popular designs in this category are S3-CMA-ES [13], PAES [20], and SMES [33]. In particular, ES is simpler to implement, it is easier to scale in a distributed setting, and it has fewer hyperparameters to control. This outcome is no surprise because ES resembles simple hill climbing in Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:33:17 UTC from IEEE Xplore. Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. ZHANG et al.: ES-BASED MaOEA THROUGH VECTOR EQUILIBRIUM 3 a high-dimensional space-based only on finite differences along a few random directions at each step [34]. However, most of these approaches still need to maintain a large population size for sorting, selection, and reproduction, and, as a result, suffer from extremely high computational complexity largely due to the excessive successes of the GA applications; evolutionary strategy has lost its early attention in the community. B. Motivation Generally, when the number of objectives increases, existing MaOEAs have to face the following challenges. 1) The number of incomparable, nondominated solutions increases enormously in the dominance resistance phenomenon [35], [36]; these solutions seriously weaken the selection pressure toward the PF. 2) In order to obtain a set of well-diversified solutions that approximates the entire PF, some of the MaOEAs need to keep the extreme solutions to extend the boundary. However, without any prior PF shape knowledge, it is very difficult to identify the extreme or boundary solutions [37]. 3) In order to approximate the high-dimensional PF surface, some of the MaOEAs have to maintain a very large population size, which inadvertently led to expensive computational cost [38]. 4) Some of the MaOEAs show high sensitivity to the shape of PF (e.g., decomposition-based approaches are sensitive to degenerate PF [30]). 5) Some of the MaOEAs heavily rely upon the availability of high-quality reference points. However, without a priori position and shape knowledge of PF hypersurface, it is very difficult, if not impossible, to generate high quality reference solutions for a given MaOPs. Inspired by the Vector Equilibrium phenomenon in magnetic fields, isotropic magnetic particles would automatically repel each other, naturally preserve uniform distance from the nearest neighbors, and extend the entire magnetic fields as far as possible. In this article, we present an ES for solving MaOPs, called MaOES, which can address these challenges listed above efficiently and effectively. The proposed algorithm imitates the Vector Equilibrium phenomenon of isotropic magnetic particles, which apply a self-adaptive mutation mechanism to guide the individuals to keep uniform distance and extension to approximate the entire PF automatically. The MaOES can obtain a well-converged and well-diversified PF with much less population size, as shown in Fig. 1. As can be seen from Fig. 1, given different population sizes (30, 50, and 100), MaOES can obtain well-distributed solutions to cover the entire PF, and each individual keeps uniform distance from nearest neighbor solutions. The larger the number of individuals, the sharper the contour of the resulted approximate PF will be preserved. III. PROPOSED ALGORITHM In this section, the details of the proposed ES MaOEA are presented. Two main procedures are iteratively run for each Fig. 1. Resulted approximate PFs by MaOES with different population sizes (30, 50, and 100) on three-objective WFG1. Fig. 2. (a) SBX distributions with different parameter n. (b) Gaussian distributions with different parameter ƒÐ. individual, specifically the Precision-Controllable Mutation operator and the Maximum Extension Distance strategy. The Precision-Controllable Mutation operator is designed for both exploration and exploitation given the designated precision. The Maximum Extension Distance strategy imitates the isotropic magnetic particles in magnetic fields which guide individuals to maintain uniform distance and extension to approximate the entire PF automatically. The computational complexity of the proposed algorithm is O(MP2), where M denotes the number of objectives, and P is the population size. This is comparable to most state-of-the-art MaOEA designs. A. Precision-Controllable Mutation Operator Let x = [x1, x2, . . . , xN]T be one of the individuals, and the xi is the ith decision variable of x. It is important to exploit the local region near x, and explore the global region distant away from x as well. Traditional mutation operator [39] adopts Gaussian perturbation to generate the mutated candidate solution x\u0004 i, as shown in (2) x\u0004 i = xi + N(0, ƒÐ). (2) However, the Gaussian distribution is similar to simulated binary crossover (SBX) [40], which generates the offspring near their parent with a high probability, which is only effective for local search, as shown in Fig. 2. In addition, the Gaussian probability density function has the parameter ƒÐ, which is very difficult to be assigned an optimal value.Within limited iteration steps, larger ƒÐ value can assure a higher search precision, but slower convergence. On the other hand, smaller ƒÐ value can facilitate a quick convergence, but lower search precision. For the WFG instances, all the decision variables are in different ranges xi ¸ [0, 2i], which Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:33:17 UTC from IEEE Xplore. Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. 4 IEEE TRANSACTIONS ON CYBERNETICS poses a much difficult task to assign an optimal value of ƒÐ specifically suited for all the decision variables simultaneously. In this article, a simple and efficient Precision-Controllable Mutation operator is proposed for exploration and exploitation, as shown in (3).(8) x\u0004 i = xi + \u0005ƒ¿ (3) x\u0004 i = xi . \u0005ƒ¿ (4) where \u0005ƒ¿ = 1 10Random(p)+1 ~ (Random(9) + 1) x\u0004 i = xi + \u0005ƒÀ (5) x\u0004 i = xi . \u0005ƒÀ (6) where \u0005ƒÀ = xi ~ \u0005ƒ¿.xi x\u0004 i = xi + \u0005ƒÁ (7) x\u0004 i = xi . \u0005ƒÁ (8) where \u0005ƒÁ = xi € \u0005ƒ¿.xi. Equations (3) and (4) are designed for exploitation, while (5) to (8) are intended for exploration. The variable p is the parameters to control the search precision in the decision space. Function Random(p) can generate a pseudorandom number in the range of 0 to p . 1. If the required search precision is 0.001, the parameter p can be set to be 3. The value of random number Random(p) should be in the set of {0, 1, 2}, then the corresponding value of [1/(10Random(p)+1)] will be in the set of {0.1, 0.01, 0.001}. The value of (Random(9) + 1) can be regarded as a random coefficient from 1 to 9. Obviously, these mutation equations can generate all the neighboring solutions within the required minimum precision of 0.001. As an illustrated example, we randomly generate the mutated solution x\u0004 i about 5000 times to test the exploitation ability of (3) and (4). For example, let the original value of xi be 10, the frequency distribution of mutated xi is shown in Fig. 3(a) and (b), given the parameter p is set to be 1 and 2, respectively. Please note the addition and subtraction operators are chosen at equal probability. The frequency distribution of newly generated mutated solutions is shown in Fig. 3. In Fig. 3(a) the parameter p is set to be 1, Random(p) produces a 0, (3) and (4) can generate mutated individual from 9.1 to 10.9 with precision 0.1 of uniform frequency. In Fig. 3(b) the parameter p is set to be 2, Random(p) should be 0 or 1, (3) and (4) can generate uniform spreading mutated solutions near original xi = 10 in precision 0.1 and 0.01. Equations (5).(8) can generate new individuals greater than \u0005ƒÀ times or less than \u0005ƒÁ times of the original xi, which could be considered exploration the decision space far away from xi. For example, let xi be 10, and the parameter p is set to be 1, Random(p) should be 0. We generate mutated x\u0004 i 5,000 times randomly, the frequency distribution is shown in Fig. 4. Equations (7) and (8) can generate mutated solutions with uniform frequency, the value of mutated x\u0004 i become 1 to 10 time more than or less than xi. Fig. 3. Frequency distribution of mutated x\u0004iwith different parameter p. (a) xi = 10, p = 1, new generated solution x\u0004 i with (3) and (4). (b) xi = 10, p = 2, new generated solution x\u0004 i with (3) and (4). Fig. 4. Value distribution of mutated x\u0004 i with (7) and (8). Apparently, addition and subtraction mutation operators can generate a small variation \u0005ƒ¿ from its original value, which is useful for local search. On the other hand, division and multiplication mutation operators could generate greater than \u0005ƒÀ times or less than \u0005ƒÁ times away from the original value, which is useful for global search to jump out of the local optimal. The pseudocode of the Precision-Controllable Mutation is given in Algorithm 1. B. Maximum Extension Distance Strategy Since the PF of MaOPs is a high-dimension hypersurface, without any priori PF shape knowledge, it is very difficult to identify boundary solutions for MaOPs. In 1917, Buckminster Fuller discovered the significance of the full vector symmetry in the magnetic fields and called it, the Vector Equilibrium in 1940 [21], [22]. Inspired by the Vector Equilibrium phenomenon, isotropic magnetic particles would repel each other and extend the entire magnetic fields as far as possible. From an energy perspective, the Vector Equilibrium represents the ultimate and perfect condition wherein every isotropic Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:33:17 UTC from IEEE Xplore. Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. ZHANG et al.: ES-BASED MaOEA THROUGH VECTOR EQUILIBRIUM 5 Algorithm 1 Precision-Controllable Mutation Input: P(i) t , p=10 Output: NewP(i) t , x\u0004 i is the ith decision variable of NewP(i) t 1: for i = 1 to N 2: x\u0004 i = xi 3: r = Random(6) 4: \u0005ƒ¿ = 1 10Random(p)+1 ~ (Random(9) + 1) 5: \u0005ƒÀ = xi ~ \u0005ƒ¿.xi 6: \u0005ƒÁ = xi € \u0005ƒ¿.xi 7: if r = 0 then xTemp = xi + \u0005ƒ¿ 8: else if r = 1 then xTemp = xi . \u0005ƒ¿; 9: else if r = 2 then xTemp = xi + \u0005ƒÀ; 10: else if r = 3 then xTemp = xi . \u0005ƒÀ; 11: else if r = 4 then xTemp = xi + \u0005ƒÁ; 12: else if r = 5 then xTemp = xi . \u0005ƒÁ; 13: end if 14: if xTemp is feasible then x\u0004 i = xTemp 15: end for (a) (b) (c) Fig. 5. Isotropic magnetic particles repel each other and expand the boundary automatically. (a) Initial state. (b) Repel each other. (c) Equilibrium state. magnetic particle keeps uniform distance from the nearest neighbors, as shown in Fig. 5. In our algorithm, we propose the Maximum Extension Distance strategy to imitate the isotropic magnetic particles behaving in magnetic fields, which guide individuals to preserve uniform distance and extension to approximate the entire PF automatically. The Maximum Extension Distance is defined in (9) MED\u0004P(i) t \u0005 = NearDist\u0004P(i) t \u0005 ~ TotalDist\u0004P(i) t \u0005 (9) where NearDist\u0004P(i) t \u0005 = min j,j\u0005=i M \u0006 m=1 \u0007\u0007\u0007f ( i) m . f (j) m \u0007\u0007\u0007 . TotalDist\u0004P(i) t \u0005 = P \u0006 j=1 M \u0006 m=1 \u0007\u0007\u0007 f (i) m .f (j) m \u0007\u0007\u0007 . In this equation, P(i) t is the ith individual in Population Pt at the tth generation. TotalDist(P(i) t ) calculates the summation of Manhattan distance (MD) between P(i) t and P(j) t . A greater value of TotalDist(P(i) t ) implies the solution P(i) t has moved away from other individuals. NearDist(P(i) t ) calculates the minimum MD between P(i) t and P(j) t . A greater value of NearDist(P(i) t ) implies a better individual diversity. The proposed Maximum Extension Distance strategy is the Fig. 6. Maximum Extension Distance to expand the boundary solution. Fig. 7. Maximum Extension Distance to keep good diversity. product of NearDist() and TotalDist(). The greater the maximum extension distance implies an individual has extended the overall boundary, and an individual has obtained a better diversity. For example, the Maximum Extension Distance can expand the boundary solution. Let A be an individual in the population, and Anew is the new candidate solution after mutation operation, as shown in Fig. 6. The Maximum Extension Distance is calculated as follows: MED(A) = NearDist(A) ~ TotalDist(A) NearDist(A) = MD(AB) TotalDist(A) = MD(AB)+MD(AC)+MD(AD)+MD(AE) +MD(AF) MED(Anew) = NearDist(Anew) ~ TotalDist(Anew) NearDist(Anew) = MD(AnewB) TotalDist(Anew) = MD(AnewB)+MD(AnewC)+MD(AnewD) +MD(AnewE)+MD(AnewF) MED(Anew) > MED(A). Compared with the original solution A, both NearDist() and TotalDist() of Anew are greater than those of A. Since the Maximum Extension Distance, MED(Anew), is greater than the value of MED(A), the individual A should be replaced by the new candidate solution Anew. Obviously, Anew extends the whole population boundary and extends individual distance far away from other individuals. For another example, B is an individual in the population, and Bnew is a new candidate solution after mutation operation, as shown in Fig. 7. The Maximum Extension Distance is calculated as follow: MED(B) = NearDist(B) ~ TotalDist(B) Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:33:17 UTC from IEEE Xplore. Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. 6 IEEE TRANSACTIONS ON CYBERNETICS Algorithm 2 Maximum Extension Distance Input: the index of mutation individual idx Output: maximum extension distance MED 1: TotalDist = 0; 2: NearDist = +8; 3: for i = 1 to P 4: if i = idx then continue; 5: Dist = 0; 6: for j = 1 to M 7: Dist = Dist + abs(f (i) j . f (idx) j ); 8: end for 9: TotalDist = TotalDist + Dist; 10: if Dist < NearDist then NearDist = Dist; 11: end for 12: MED = NearDist ~ TotalDist; NearDist(B) = MD(BC) TotalDist(B) = MD(BA)+MD(BC)+MD(BD)+MD(BE) +MD(BF) MED(Bnew) = NearDist(Bnew) ~ TotalDist(Bnew) NearDist(Bnew) = MD(BnewA) TotalDist(Bnew) = MD(BnewA)+MD(BnewC)+MD(BnewD) +MD(BnewE)+MD(BnewF) MED(Bnew) < MED(B). Although TotalDist() of Bnew are greater than B, NearDist() of Bnew is far less than that of B. Smaller NearDist() means B has moved closer to other individuals, the diversity would be degenerated. Since the Maximum Extension Distance MED(Bnew) is less than the value of MED(B), individual B should not be replaced by the candidate solution Bnew. The time complexity of Maximum Extension Distance strategy is O(MP). The pseudocode is given in Algorithm 2. C. Overall Algorithm For a given MaOP, a population of randomly sampled individuals are generated. At each generation, the proposed ES adopts the Precision-Controllable Mutation operator to every individual in each decision variant. Let P(i) t be the ith original solution in population Pt, and NewP(i) t be the mutated new solution of P(i) t . If NewP(i) t dominate P(i) t , the new mutated solution would replace the original one. If NewP(i) t is dominated by P(i) t , the new mutated solution would be neglected. If NewP(i) t and P(i) t are nondominated with respect to each other, our algorithm would compare the numbers which other solutions in the population that dominate P(i) t or NewP(i) t . Let DomCount(P(i) t ) be the function that calculate the number of other solutions dominate P(i) t , the computational complexity is O(MP). If the function value of NewP(i) t become smaller than the value of P(i) t , that is, less solutions can dominate NewP(i) t , the proposed algorithm would accept the new mutated solution. If the value of DomCount(NewP(i) t ) become equal to the value of P(i) t , our algorithm would continue to compare the value of Maximum Extension Distance. If the value of Maximum Extension Distance MED(NewP(i) t ) is greater than MED(P(i) t ), the new mutated Algorithm 3 Proposed MaOES Algorithm Input: Output: 1: Initialization Pt, t = 0 2: while ( t < maximum generation ) 3: for i = 1 to P 4: NewP(i) t = Precision-Controllable Mutation (P(i) t ) 5: Objective Function Calculation (NewP(i) t ) 6: if (NewP(i) t . P(i) t ) then 7: P(i) t = NewP(i) t 8: else if (NewP(i) t . P(i) t ) and (P(i) t . NewP(i) t ) then 9: if DomCount(NewP(i) t ) < DomCount (P(i) t ) then 10: P(i) t = NewP(i) t 11: elseif DomCount(NewP(i) t ) = DomCount (P(i) t ) then 12: if MED(NewP(i) t ) > MED(P(i) t ) then 13: P(i) t = NewP(i) t 14: end if 15: end if 16: end if 17: end for 18: end while solution is better than the original one, and would replace it. Otherwise, we should eliminate the new solution. The computational complexity of the overall algorithm is O(MP2). The pseudocode is given in Algorithm 3. IV. EXPERIMENTAL RESULTS In order to validate the proposed MaOES, we compare its performance with some state-of-the-art representatives from different categories of MaOEAs, including NSGA-III [9], MOEA/DD [10], GrEA [7], HypE [11], RPD-NSGAII [27], S3-CMA-ES [13], MyO-DEMR [24], NMPSO [23], and onebyone EA [25]. In the comparison, these algorithms are evaluated on 16 scalable benchmark instances in WFG [41] and DTLZ [42] suites. These MaOPs contain different problem characteristics, such as convex, concave, disconnected, linear, and degenerated. We perform 30 independent runs for each algorithm on each test instance and the maximum evaluation is set to 10 000. For the proposed MaOES, the SBX and the polynomial mutation have been adopted for real-coded GAs. For a fair comparison, the population size is set to 200, SBX distribution index is set to 20, polynomial mutation distribution index is set to 20, crossover probability is set to 1.0, and mutation probability is set to 1/N, where N denotes the number of decision variables. The other variable parameters of the compared algorithms are adopted as suggested in the original papers, including [7], [9].[11], [13], [23].[25], and [27]. For each benchmark instance, 10 000 true PF solutions (i.e., PF true), are generated by PlatEMO [43] to evaluate the inverted generational distance (IGD) [44]. The IGD indicator measures the distance between the true PF and the closest individual in the obtained solutions. The indicator can be expressed as IIGD = \u0004\b |PF| i=1 d2 i \u00051/2 |PF| (10) Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:33:17 UTC from IEEE Xplore. Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. ZHANG et al.: ES-BASED MaOEA THROUGH VECTOR EQUILIBRIUM 7 TABLE I AVERAGE IGD VALUES OVER 30 RUNS ON UNCONSTRAINT DTLZ AND WFG BENCHMARK INSTANCES (POPULATION SIZE 200), WHERE THE BEST MEAN FOR EACH INSTANCE IS SHOWN WITH A GRAY BACKGROUND where di is the Euclidean distance between the ith solution in the true PF and the closest individual in the obtained solutions. As a matter of fact, the lower the IGD value is, the better approximate solution set is obtained. To facilitate the experiments, we have implemented the algorithm in the object Pascal language and developed a graphical user interface (GUI) under the Delphi XE7 platform. The GUI and MaOES algorithm source code are available Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:33:17 UTC from IEEE Xplore. Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. 8 IEEE TRANSACTIONS ON CYBERNETICS Fig. 8. Experimental results for three-objectives benchmark problems on unconstraint DTLZ suite by MaOES (population sizes 100). (a) DTLZ1. (b) DTLZ2. (c) DTLZ3. (d) DTLZ4. (e) DTLZ5. (f) DTLZ6. (g) DTLZ7. in the following website: https://github.com/MaOEA/MaOES. Interested readers can retrieve the code and validate the performance on his/her own. A. Performance on DTLZ Problems The IGD values obtained by the ten competing algorithms are given in Table I. One can easily notice that MaOES performs significantly better than those of competing MaOEAs on DTLZ1, DTLZ6, and DTLZ7 with respect to all considered numbers of objectives. For DTLZ2, MOEA/DD obtains a better IGD value on the eight-dimension instances while MaOES shows better results with the other DTLZ2 instances. For DTLZ4, MOEA/DD and onebyone EA obtain better IGD values on the 3-objective instance and 5-objective instance, respectively, and MaOES has the best performance when the number of objectives is more than 5. For DTLZ5, S3-CMAES and onebyone EA outperform other MaOEAs in almost all DTLZ5 instances, while MaOES wins 3. From 28 test instances of the DTLZ benchmark problems, it is clear that MaOES is the best optimizer as it wins in 22 instances. Thus, from the empirical results on DTLZ1.DTLZ7 test problems, we find that MaOES outperforms other compared MaOEAs. The results demonstrate the ability of the proposed MaOES to deal with MAOPs characterized by linear, concave, degenerated, and discontinues PFs. For example, given the threeobjective DTLZ instances, 100 nondominated solutions over 5,000 generations can be seen in Fig. 8. Fig. 9 shows the corresponding parallel coordinates of the solutions of MaOES on 10-D DTLZ test instances. In comparison with parallel coordinates of true PF sampled by PlatEMO. As can be seen in Fig. 9, MaOES has a good convergence on DTLZ1-DTLZ4, and DTLZ7. However, MaOES have a poor population convergence on ten-dimension DTLZ5 and DTLZ6. In the DTLZ6 of Fig. 8, some individuals already have obtained good convergence and distribution PF ranging from 0 to 1. Please note that DTLZ5 has a nondegenerate part of the PF, although DTLZ5 has often been used as MaOP with degenerate PFs. Because of the effectiveness of Maximum Fig. 9. Solution sets obtained by MaOES and true PF on the ten-objective DTLZ test suite through parallel coordinates. Extension Distance strategy, MaOES obtain good coverage for all DTLZ test instances, and almost uniformly contribution to all ten objectives. B. Performance on WFG Problems As can be seen from Table I, MaOES has better IGD means values than the other MaOEAs in all WFG1, WFG3, and WFG4 instances. For WFG2, WFG5, and WFG8, NMPSO obtains better IGD values on the 10-objective instances and MaOES shows better results with all the remaining instances. For WFG7 and WFG9, GrEA obtains better IGD values on the 8-objective instances, while NMPSO has the best performance on the 10-objective instances. For WFG6, GrEA obtains better IGD values on the 8-objective and 10-objective instances, and MaOES shows better results with the other WFG6 instances. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:33:17 UTC from IEEE Xplore. Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. ZHANG et al.: ES-BASED MaOEA THROUGH VECTOR EQUILIBRIUM 9 Fig. 10. Experimental results for three-objectives benchmark problems on unconstraint WFG suite by MaOES (population sizes 100). (a) WFG1. (b) WFG2. (c) WFG3. (d) WFG4. (e) WFG5. (f) WFG6. (g) WFG7. (h) WFG8. (i) WFG9. From the 36 WFG problem instances, MaOES wins in 27 instances which clearly show the ability of the proposed MaOES to deal with MaOPs characterized by mixed convex and concave, disconnected convex, degenerated and concave PFs. For example, given the three-objective WFG instances, the final nondominated solutions over 5,000 generations can be seen in Fig. 10. Fig. 10(a) shows the proposed algorithm can find well-convergent and well-diversified solutions on mixed convex and concave PF for WFG1. As can be seen from Fig. 10(b), our algorithm obtains a proper distribution among solutions for WFG2 with disconnected convex PF. Fig. 10(c) shows our MaOES can obtain quality convergence and uniform solutions that lie on the degenerated WFG3. Fig. 10(d).(j) show that the MaOES obtain sets of good distributed solutions that cover the whole concave PFs for WFG4-WFG9. Fig. 11 shows the corresponding parallel coordinates of the solutions of MaOES on ten-objectives WFG test instances. In comparison with parallel coordinates of true PF sampled by PlatEMO. As can be seen in Fig. 11, MaOES has a good convergence on eight WFG test instances (i.e., WFG1 and WFG3-WFG9). In addition, our proposed algorithm obtains a uniform coverage for each objective, largely due to the effective design of Maximum Extension Distance strategy. However, MaOES fails to cover the region on the first three objectives for WFG2, and there are no solution distributed in the second objective ranging from 2 to 4 for WFG8. Fig. 11. Solution sets obtained by MaOES and true PF on the ten-objective WFG test suite through parallel coordinates. Fig. 12 shows the corresponding parallel coordinates of the solutions of five state-of-the-art MaOEAs along with the PFs on ten-dimension DTLZ1, DTLZ7, WFG3, and WFG4 test instances, which characterized by linear, discontinues, degenerated, and concave PFs. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:33:17 UTC from IEEE Xplore. Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. 10 IEEE TRANSACTIONS ON CYBERNETICS Fig. 12. Solution sets obtained by MaOES, GrEA, MOEA/DD, NSGAIII, HypE, and true PF on the ten-objective DTLZ1, DTLZ7, WFG3, and WFG4 through parallel coordinates. As can be seen in Fig. 12(a), the true DTLZ1 PF ranging from 0 to 0.5. MaOES has a good convergence PF ranging from 0 to 1, the other four algorithms show an inferior convergence, with its solution set ranging from 0 to around 180, 50, and 80, respectively. The results of the discontinues DTLZ7 instance is shown in Fig. 12(b), MaOES performs well on convergence and coverage within the DTLZ7 PF. However, MOEA/DD and HypE fail to cover the region on almost all ten objectives, and GrEA and NSGA-III obtain poor diversity on the first nine objectives. From Fig. 12(c), MaOES and HypE perform well on convergence and coverage, and MaOES obtains more uniform distribution in all ten objectives. The other three algorithms show an inferior convergence on the degenerated WFG3 PF. In Fig. 12(d), all the algorithms have good convergence on the WFG4 concave PF. The MaOES, GrEA and NSGA-III can reach on all the objectives, however, MOEA/DD and HypE fail to cover the region on two different objectives and five different objectives, respectively. Moreover, the solutions of MaOES and GrEA can spread over the whole range for each objective. In contrast, MOEA/DD and NSGAIII obtain very few lines distributed around the middle section on all the objectives. From 64 test instances of the DTLZ and WFG benchmark problems, it is clear that MaOES is the best optimizer as it wins 56 instances against NMPSO, wins 59 instances against S3-CMA-ES and onebyoneEA, wins 60 instances against GrEA, wins 61 instances against HypE, wins 62 instances against MyO-DEMR and RPD-NSGAII, and wins 63 instances against NSGA-III. Thus, from the empirical results on DTLZ and WFG test problems, we find that MaOES outperforms other compared MaOEAs. C. Performance on Different Population Size The MaOES need not maintain a large population size, since the Maximum Extension Distance can guide the individuals to maintain uniform distances from the nearest neighbors and extend the entire objective space automatically. In fact, the MaOES can efficiently obtain quality converged and diversified solutions which cover the entire true PF with Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:33:17 UTC from IEEE Xplore. Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. ZHANG et al.: ES-BASED MaOEA THROUGH VECTOR EQUILIBRIUM 11 Fig. 13. Experimental results for selected benchmark functions with different population sizes by MaOES (i.e., 4, 10, 30, and 50). various population sizes (i.e., 4, 10, 30, and 50) as shown in Fig. 13. V. CONCLUSION In this article, we proposed an ES for solving MaOPs based on ES. The algorithm imitates the isotropic magnetic particles, which automatically repel each other and extend the entire magnetic fields as far as possible. In the proposed algorithm, an efficient self-adaptive Precision Controllable Mutation operator was designed for individuals to explore and exploit the decision space. In addition, the Maximum Extension Distance strategy was designed to guide individuals to keep uniform distances and extension to approximate the entire PF automatically. The MaOES can obtain a well-converged and well-diversified PF with much less population size and far lower computational complexity. The larger the number of individuals, the sharper the contour of the approximate PF will be. The performance is compared against nine different categories of state-of-the-art MaOEAs, including GrEA, NSGA-III, MOEA/DD, HypE and RPD-NSGAII, S3-CMAES, MyO-DEMR, NMPSO, and onebyone EA. The experimental results show that MaOES provides the best IGD measure, and the performance of the proposed algorithm is significantly better than the chosen competing MaOEAs on DTLZ and WFG with 3.10 objectives. The results demonstrate the ability of our MaOES to deal with the problems characterized by linear, concave, mixed convex and concave, disconnected convex, and degenerated MaOPs. Compared with the existing MaOEAs, our algorithm has satisfactorily addressed several challenges. 1) Since every mutated new solution xf needs only compare with its original individual x, there is no selection pressure and dominance resistance problem in our algorithm. 2) In our algorithm, individuals like isotropic magnetic particles, would repel from each other, and keep uniform distance from the nearest neighbors. Our MaOES can obtain well-diversified solutions without explicit diversity preservation scheme. 3) The proposed Maximum Extension Distance strategy is the product of NearDist() and TotalDist(). The greater the maximum extension distance implies an individual has extended the overall boundary and better individual diversity. Our MaOES can extend to approximate the entire PF automatically without explicitly identifying or keeping the boundary or extreme solutions. 4) The overall computational complexity of one generation of MaOES is equal to O(MP2). In addition, the experimental results show that the MaOES needs only very little population size to obtain a well-converged and well-diversified PF. The larger the number of individuals, the sharper the contour of the approximate PFs will be. 5) MaOES requires no reference points or sensitive parameters, and the experimental results show that MaOES is very robust to deal with linear, concave, mixed convex and concave, disconnected convex, and degenerated MaOPs. 6) Given the required search precision, the Precision- Controllable Mutation operator can generate a new mutated solution for both exploration and exploitation efficiently. The Precision-Controllable Mutation operator can improve the efficiency and eliminate unnecessary computational cost. REFERENCES [1] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan, gA fast and elitist multiobjective genetic algorithm: NSGA-II,h IEEE Trans. Evol. Comput., vol. 6, no. 2, pp. 182.197, Apr. 2002. [2] E. Zitzler, M. Laumanns, and L. Thiele, gSPEA2: Improving the strength Pareto evolutionary algorithm,h in Proc. Evol. Methods Design Optim. Control Appl. Ind. Problems (EUROGEN), Athens, Greece, 2001, pp. 95.100. [3] P. J. Fleming, R. C. Purshouse, and R. J. Lygoe, gMany-objective optimization: An engineering design perspective,h in Proc. Int. Conf. Evol. Multi Criterion Optim., 2005, pp. 14.32. [4] J. G. Herrero, A. Berlanga, and J. M. M. Lopez, gEffective evolutionary algorithms for many-specifications attainment: Application to air traffic control tracking filters,h IEEE Trans. Evol. Comput., vol. 13, no. 1, pp. 151.168, Feb. 2009. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:33:17 UTC from IEEE Xplore. Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. 12 IEEE TRANSACTIONS ON CYBERNETICS [5] K. Narukawa and T. Rodemann, gExamining the performance of evolutionary many-objective optimization algorithms on a real-world application,h in Proc. 6th Int. Conf. Genet. Evol. Comput., Kitakushu, Japan, 2012, pp. 316.319. [6] A. Sulflow, N. Drechsler, and R. Drechsler, gRobust multi-objective optimization in high dimensional spaces,h in Proc. Int. Conf. Evol. Multi Criterion Optim., Matsushima, Japan, 2007, pp. 715.726. [7] S. X. Yang, M. Q. Li, X. H. Liu, and J. H. Zheng, gA grid-based evolutionary algorithm for many-objective optimization,h IEEE Trans. Evol. Comput., vol. 17, no. 5, pp. 721.736, Oct. 2013. [8] Z. He, G. G. Yen, and J. Zhang, gFuzzy-based Pareto optimality for many-objective evolutionary algorithms,h IEEE Trans. Evol. Comput., vol. 18, no. 2, pp. 269.285, Apr. 2014. [9] K. Deb and H. Jain, gAn evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, Part I: Solving problems with box constraints,h IEEE Trans. Evol. Comput., vol. 18, no. 4, pp. 577.601, Aug. 2014. [10] K. Li, K. Deb, Q. Zhang, and S. Kwong, gAn evolutionary manyobjective optimization algorithm based on dominance and decomposition,h IEEE Trans. Evol. Comput., vol. 19, no. 5, pp. 694.716, Oct. 2015. [11] J. Bader and E. Zitzler, gHypE: An algorithm for fast hypervolume-based many-objective optimization,h Evol. Comput., vol. 19, no. 1, pp. 45.76, 2011. [12] H. K. Singh, A. Isaacs, and T. Ray, gA Pareto corner search evolutionary algorithm and dimensionality reduction in many-objective optimization problems,h IEEE Trans. Evol. Comput., vol. 15, no. 4, pp. 539.556, Aug. 2011. [13] H. Chen, R. Cheng, J. Wen, H. Li, and J. Weng, gSolving large-scale many-objective optimization problems by covariance matrix adaptation evolution strategy with scalable small subpopulations,h Inf. Sci., vol. 509, no. 1, pp. 457.469, 2020. [14] I. Rechenberg, Evolution Strategy: Optimizing Technical Systems With Principles of Biological Evolution (in German). Stuttgart, Germany: Frommann-Holzboog, 1973. [15] H. Schwefel, Numerical Optimization of Computer Models Using Evolutionary Strategy. Basel, Switzerland: Birkhauser Verlag, 1977. [16] J. Zhang, X. Yuan, Z. Zeng, B. P. Buckles, C. Koutsougeras, and S. Amer, gNiching in an ES/EP context,h in Proc. Congr. Evol. Comput., Washington, DC, USA, 1999, pp. 1426.1433. [17] O. Takahashi and S. Kobayashi, gAn adaptive neighboring search using crossover-like mutation for multi modal function optimization,h in Proc. IEEE Int. Conf. Syst. Man Cybern., Tucson, AZ, USA, 2001, pp. 261.267. [18] O. Aichholzer et al., gEvolution strategy and hierarchical clustering,h IEEE Trans. Magn., vol. 38, no. 2, pp. 1041.1044, Mar. 2002. [19] C.-H. Im, H.-K. Kim, H.-K. Jung, and K. Choi, gA novel algorithm for multimodal function optimization based on evolution strategy,h IEEE Trans. Magn., vol. 40, no. 2, pp. 1224.1227, Mar. 2004. [20] J. Knowles and D. Corne, gThe Pareto archived evolution strategy: A new baseline algorithm for Pareto multiobjective optimisation,h in Proc. Congr. Evol. Comput., Washington, DC, USA, 1999, pp. 98.105. [21] R. Fuller, Synergetics.Explorations in the Geometry of Thinking, vol. 1. New York, NY, USA: Macmillan, 1975. [22] R. Fuller, Synergetics.Explorations in the Geometry of Thinking, vol. 2. New York, NY, USA: Macmillan, 1979, [23] Q. Lin et al., gParticle swarm optimization with a balanceable fitness estimation for many-objective optimization problems,h IEEE Trans. Evol. Comput., vol. 22, no. 1, pp. 32.46, Feb. 2018. [24] R. Denysiuk, L. Costa, and I. E. Santo, gMany-objective optimization using differential evolution with variable-wise mutation restriction,h in Proc. 15th Annu. Conf. Genet. Evol. Comput., Amsterdam, The Netherlands, 2013, pp. 591.598. [25] Y. Liu, D. Gong, J. Sun, and Y. Jin, gA many-objective evolutionary algorithm using a one-by-one selection strategy,h IEEE Trans. Cybern., vol. 47, no. 9, pp. 2689.2702, Sep. 2017. [26] D. Gong, J. Sun, and Z. Miao, gA set-based genetic algorithm for interval many-objective optimization problems,h IEEE Trans. Evol. Comput., vol. 22, no. 1, pp. 47.60, Feb. 2018. [27] K. Deb, M. Mohan, and S. Mishra, gEvaluating the ƒÃ-domination based multi-objective evolutionary algorithm for a quick computation of Pareto-optimal solutions,h Evol. Comput., vol. 13, no. 4, pp. 501.525, 2005. [28] M. Li, S. Yang, and X. Liu, gShift-based density estimation for Paretobased algorithms in many-objective optimization,h IEEE Trans. Evol. Comput., vol. 18, no. 3, pp. 348.365, Jun. 2014. [29] M. Elarbi, S. Bechikh, A. Gupta, L. B. Said, and Y.-S. Ong, gA new decomposition-based NSGA-II for many-objective optimization,h IEEE Trans. Syst., Man, Cybern., Syst., vol. 48, no. 7, pp. 1191.1210, Jul. 2018. [30] H. Ishibuchi, Y. Setoguchi, H. Masuda, and Y. Nojima, gPerformance of decomposition-based many-objective algorithms strongly depends on Pareto front shapes,h IEEE Trans. Evol. Comput., vol. 21, no. 2, pp. 169.190, Apr. 2017. [31] E. Zitzler and S. Kunzli, gIndicator-based selection in multiobjective search,h in Proc. 8th Int. Conf. Parallel Problem Solving Nat., Birmingham, U.K., 2004, pp. 832.842. [32] D. K. Saxena, J. A. Duro, A. Tiwari, K. Deb, and Q. Zhang, gObjective reduction in many-objective optimization: Linear and nonlinear algorithms,h IEEE Trans. Evol. Comput., vol. 17, no. 1, pp. 77.99, Feb. 2013. [33] E. Mezura-Montes and C. A. Coello Coello, gA simple evolution strategy to solve constrained optimization problems,h in Proc. Genet. Evol. Comput. Conf., Chicago, IL, USA, 2003, pp. 640.641. [34] T. Salimans, J. Ho, X. Chen, and I. Sutskever, gEvolution strategies as a scalable alternative to reinforcement learning,h arXiv preprint arXiv:1703.03864, 2017. [Online]. Available: https://arxiv.org/abs/1703.03864v2 [35] R. C. Purshouse and P. J. Fleming, gOn the evolutionary optimization of many conflicting objectives,h IEEE Trans. Evol. Comput., vol. 11, no. 6, pp. 770.784, Dec. 2007. [36] J. Knowles and D. Corne, gQuantifying the effects of objective space dimension in evolutionary multiobjective optimization,h in Proc. Int. Conf. Evol. Multi Criterion Optim., Matsushima, Japan, 2007, pp. 757.771. [37] H. Ishibuchi, N. Tsukamoto, and Y. Nojima, gEvolutionary manyobjective optimization: A short review,h in Proc. IEEE Congr. Evol. Comput. (CEC), Hong Kong, 2008, pp. 2419.2426. [38] B. Li, J. Li, K. Tang, and X. Yao, gMany-objective evolutionary algorithms: A survey,h ACM Comput. Surveys, vol. 48, no. 1, pp. 1.35, 2015. [39] H.-G. Beyer and H.-P. Schwefel, gEvolution strategies: A comprehensive introduction,h Nat. Comput., vol. 1, no. 1, pp. 3.52, 2002. [40] K. Deb and R. B. Agrawal, gSimulated binary crossover for continuous search space,h Complex Syst., vol. 9, no. 2, pp. 115.148, 1995. [41] S. Huband, L. Barone, L. While, and P. Hingston, gA scalable multiobjective test problem toolkit,h in Proc. Int. Conf. Evol. Multi Criterion Optim., Guanajuato, Mexico, 2005, pp. 280.295. [42] K. Deb, L. Thiele, M. Laumanns, and E. Zitzler, gScalable test problems for evolutionary multiobjective optimization,h in Evolutionary Multi-Objective Optimization, Advanced Information and Knowledge Processing. London, U.K.: Springer, 2005, pp. 105.145. [43] Y. Tian, R. Cheng, X. Zhang, and Y. Jin, gPlatEMO: A MATLAB platform for evolutionary multi-objective optimization [educational forum],h IEEE Comput. Intell. Mag., vol. 12, no. 4, pp. 73.87, Nov. 2017. [44] P. Czyzak and A. Jaszkiewicz, gPareto simulated annealing-a metaheuristic technique for multiple-objective combinatorial optimization,h J. Multi Criteria Decis. Anal., vol. 7, no. 1, pp. 34.47, 1998. Kai Zhang received the Ph.D. degree in system analyses and integration from the Huazhong University of Science and Technology, Wuhan, China, in 2008. He was a Postdoctoral Research Fellow with the School of Electronics Engineering and Computer Science, Peking University, Beijing, China, from 2008 to 2010. He is currently a Professor with the School of Computer Science and Technology, Wuhan University of Science and Technology, Wuhan. His research focuses on evolutionary computation and multicriteria decision making. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:33:17 UTC from IEEE Xplore. Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. ZHANG et al.: ES-BASED MaOEA THROUGH VECTOR EQUILIBRIUM 13 Zhiwei Xu received the B.S. degree in information security from the Wuhan University of Science and Technology, Wuhan, China, in 2017, where he is currently pursuing the Ph.D. degree with the School of Computer Science and Technology. His research interests include evolutionary computation and many-objective optimization. Shengli Xie (Fellow, IEEE) received the M.S. degree in mathematics from Central China Normal University, Wuhan, China, in 1992, and the Ph.D. degree in control theory and applications from the South China University of Technology, Guangzhou, China, in 1997. He is currently a Full Professor and the Head of the Key Laboratory of Intelligent Detection and The Internet of Things in Manufacturing, Guangdong University of Technology, Guangzhou. He has authored or coauthored 2 books and over 200 scientific papers in journals and conference proceedings. His research interests include wireless networks, automatic control, and blind signal processing. Prof. Xie was a recipient of the Second Prize in Chinafs State Natural Science Award in 2009 for his research on blind source separation and identification. Gary G. Yen (Fellow, IEEE) received the Ph.D. degree in electrical and computer engineering from the University of Notre Dame, Notre Dame, IN, USA, in 1992. He was with the Structure Control Division, U.S. Air Force Research Laboratory, Albuquerque, NM, USA. In 1997, he joined Oklahoma State University, Stillwater, OK, USA, where he is currently a Regents Professor with the School of Electrical and Computer Engineering. His research interests include intelligent control, computational intelligence, conditional health monitoring, and signal processing and their industrial/defense applications. Dr. Yen received the Andrew P Sage Best Transactions Paper Award from the IEEE Systems, Man and Cybernetics Society in 2011 and the Meritorious Service Award from the IEEE Computational Intelligence Society in 2014. He was an Associate Editor of the IEEE Control Systems Magazine; the IEEE TRANSACTIONS ON CONTROL SYSTEMS TECHNOLOGY; Automatica; Mechantronics; the IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS.PART A; the IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS.PART B; and the IEEE TRANSACTIONS ON NEURAL NETWORKS. He is currently serving as an Associate Editor for the IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION and the IEEE TRANSACTIONS ON CYBERNETICS. He served as the General Chair for the 2003 IEEE International Symposium on Intelligent Control, Houston, TX, USA, and the 2006 IEEE World Congress on Computational Intelligence, Vancouver, BC, Canada. He served as the Vice President for the Technical Activities from 2005 to 2006 and the President from 2010 to 2011 of the IEEE Computational intelligence Society. He is the Founding Editor-in-Chief of IEEE Computational Intelligence Magazine from 2006 to 2009. He is a fellow of IET.\"],\n",
    "[7,3,1,\"4 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 15, NO. 1, FEBRUARY 2011 Differential Evolution: A Survey of the State-of-the-Art Swagatam Das, Member, IEEE, and Ponnuthurai Nagaratnam Suganthan, Senior Member, IEEE Abstract.Differential evolution (DE) is arguably one of the most powerful stochastic real-parameter optimization algorithms in current use. DE operates through similar computational steps as employed by a standard evolutionary algorithm (EA). However, unlike traditional EAs, the DE-variants perturb the currentgeneration population members with the scaled differences of randomly selected and distinct population members. Therefore, no separate probability distribution has to be used for generating the offspring. Since its inception in 1995, DE has drawn the attention of many researchers all over the world resulting in a lot of variants of the basic algorithm with improved performance. This paper presents a detailed review of the basic concepts of DE and a survey of its major variants, its application to multiobjective, constrained, large scale, and uncertain optimization problems, and the theoretical studies conducted on DE so far. Also, it provides an overview of the significant engineering applications that have benefited from the powerful nature of DE. Index Terms.Derivative-free optimization, differential evolution (DE), direct search, evolutionary algorithms (EAs), genetic algorithms (GAs), metaheuristics, particle swarm optimization (PSO). I. Introduction TO TACKLE complex computational problems, researchers have been looking into nature for years.both as model and as metaphor.for inspiration. Optimization is at the heart of many natural processes like Darwinian evolution itself. Through millions of years, every species had to adapt their physical structures to fit to the environments they were in. A keen observation of the underlying relation between optimization and biological evolution led to the development of an important paradigm of computational intelligence.the evolutionary computing techniques [S1].[S4] for performing very complex search and optimization. Evolutionary computation uses iterative progress, such as growth or development in a population. This population is then selected in a guided random search using parallel processing to achieve the desired end. The paradigm of evolutionary computing techniques dates back to early 1950s, when the idea Manuscript received September 17, 2009; revised March 9, 2010 and June 10, 2010; accepted June 12, 2010. Date of publication October 14, 2010; date of current version February 25, 2011. This work was supported by the Agency for Science, Technology, and Research, Singapore (A.Star), under Grant #052 101 0020. S. Das is with the Department of Electronics and Telecommunication Engineering, Jadavpur University, Kolkata 700 032, India (e-mail: swagatamdas19@ yahoo.co.in). P. N. Suganthan is with the School of Electrical and Electronic Engineering, Nanyang Technological University, 639798, Singapore (e-mail: epnsugan@ntu.edu.sg). Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org. Digital Object Identifier 10.1109/TEVC.2010.2059031 to use Darwinian principles for automated problem solving originated. It was not until the sixties that three distinct interpretations of this idea started to be developed in three different places. Evolutionary programming (EP) was introduced by Lawrence J. Fogel in the USA [S5],1 while almost simultaneously. I. Rechenberg and H.-P. Schwefel introduced evolution strategies (ESs) [S6], [S7] in Germany. Almost a decade later, John Henry Holland from University of Michigan at Ann Arbor, devised an independent method of simulating the Darwinian evolution to solve practical optimization problems and called it the genetic algorithm (GA) [S8]. These areas developed separately for about 15 years. From the early 1990s on they are unified as different representatives (¡°dialects¡±) of one technology, called evolutionary computing. Also since the early nineties, a fourth stream following the same general ideas started to emerge.genetic programming (GP) [S9]. Nowadays, the field of nature-inspired metaheuristics is mostly constituted by the evolutionary algorithms [comprising of GAs, EP, ESs, GP, differential evolution (DE), and so on] as well as the swarm intelligence algorithms [e.g., ant colony optimization (ACO), particle swarm optimization (PSO), Bees algorithm, bacterial foraging optimization (BFO), and so on [S10].[S12]]. Also the field extends in a broader sense to include self-organizing systems [S13], artificial life (digital organism) [S14], memetic and cultural algorithms [S15], harmony search [S16], artificial immune systems [S17], and learnable evolution model [S18]. The DE [72], [73], [88].[90] algorithm emerged as a very competitive form of evolutionary computing more than a decade ago. The first written article on DE appeared as a technical report by R. Storn and K. V. Price in 1995 [88]. One year later, the success of DE was demonstrated at the First International Contest on Evolutionary Optimization in May 1996, which was held in conjunction with the 1996 IEEE International Conference on Evolutionary Computation (CEC) [89]. DE finished third at the First International Contest on Evolutionary Optimization (1st ICEO), which was held in Nagoya, Japan. DE turned out to be the best evolutionary algorithm for solving the real-valued test function suite of the 1st ICEO (the first two places were given to non-evolutionary algorithms, which are not universally applicable but solved the test-problems faster than DE). Price presented DE at the Second International Contest on Evolutionary Optimization in 1Due to space limitation the supplementary reference list cited as [Sxxx] will not be published in print, but as an on-line document only. 1089-778X/$26.00 \u0001c 2010 IEEE DAS AND SUGANTHAN: DIFFERENTIAL EVOLUTION: A SURVEY OF THE STATE-OF-THE-ART 5 1997 [72] and it turned out as one of the best among the competing algorithms. Two journal articles [73], [92] describing the algorithm in sufficient details followed immediately in quick succession. In 2005 CEC competition on real parameter optimization, on 10-D problems classical DE secured 2nd rank and a self-adaptive DE variant called SaDE [S201] secured third rank although they performed poorly over 30-D problems. Although a powerful variant of ES, known as restart covariance matrix adaptation ES (CMA-ES) [S232, S233], yielded better results than classical and self-adaptive DE, later on many improved DE variants [like improved SaDE [76], jDE [10], opposition-based DE (ODE) [82], DE with global and local neighborhoods (DEGL) [21], JADE [118], and so on that will be discussed in subsequent sections] were proposed in the period 2006.2009. Hence, another rigorous comparison is needed to determine how well these variants might compete against the restart CMA-ES and many other real parameter optimizers over the standard numerical benchmarks. It is also interesting to note that the variants of DE continued to secure front ranks in the subsequent CEC competitions [S202] like CEC 2006 competition on constrained real parameter optimization (first rank), CEC 2007 competition on multiobjective optimization (second rank), CEC 2008 competition on large scale global optimization (third rank), CEC 2009 competition on multiobjective optimization (first rank was taken by a DE-based algorithm MOEA/D for unconstrained problems), and CEC 2009 competition on evolutionary computation in dynamic and uncertain environments (first rank). We can also observe that no other single search paradigm such as PSO was able to secure competitive rankings in all CEC competitions. A detailed discussion on these DE-variants for optimization in complex environments will be provided in Section V. In DE community, the individual trial solutions (which constitute a population) are called parameter vectors or genomes. DE operates through the same computational steps as employed by a standard EA. However, unlike traditional EAs, DE employs difference of the parameter vectors to explore the objective function landscape. In this respect, it owes a lot to its two ancestors namely.the Nelder-Mead algorithm [S19], and the controlled random search (CRS) algorithm [S20], which also relied heavily on the difference vectors to perturb the current trial solutions. Since late 1990s, DE started to find several significant applications to the optimization problems arising from diverse domains of science and engineering. Below, we point out some of the reasons why the researchers have been looking at DE as an attractive optimization tool and as we shall proceed through this survey, these reasons will become more obvious. 1) Compared to most other EAs, DE is much more simple and straightforward to implement. Main body of the algorithm takes four to five lines to code in any programming language. Simplicity to code is important for practitioners from other fields, since they may not be experts in programming and are looking for an algorithm that can be simply implemented and tuned to solve their domain-specific problems. Note that although PSO is also very easy to code, the performance of DE and its variants is largely better than the PSO variants over a wide variety of problems as has been indicated by studies like [21], [82], [104] and the CEC competition series [S202]. 2) As indicated by the recent studies on DE [21], [82], [118] despite its simplicity, DE exhibits much better performance in comparison with several others like G3 with PCX, MA-S2, ALEP, CPSO-H, and so on of current interest on a wide variety of problems including unimodal, multimodal, separable, non-separable and so on. Although some very strong EAs like the restart CMAES was able to beat DE at CEC 2005 competition, on non-separable objective functions, the gross performance of DE in terms of accuracy, convergence speed, and robustness still makes it attractive for applications to various real-world optimization problems, where finding an approximate solution in reasonable amount of computational time is much weighted. 3) The number of control parameters in DE is very few (Cr, F, and NP in classical DE). The effects of these parameters on the performance of the algorithm are wellstudied. As will be discussed in the next section, simple adaptation rules for F and Cr have been devised to improve the performance of the algorithm to a large extent without imposing any serious computational burden [10], [76], [118]. 4) The space complexity of DE is low as compared to some of the most competitive real parameter optimizers like CMA-ES [S232]. This feature helps in extending DE for handling large scale and expensive optimization problems. Although CMA-ES remains very competitive over problems up to 100 variables, it is difficult to extend it to higher dimensional problems due to its storage, update, and inversion operations over square matrices with size the same as the number of variables. Perhaps these issues triggered the popularity of DE among researchers all around the globe within a short span of time as is evident from the bibliography of DE [37] from 1997 to 2002. Consequently, over the past decade research on and with DE has become huge and multifaceted. Although there exists a few significant survey papers on EAs and swarm intelligence algorithms (e.g., [S21].[S25]), to the best of our knowledge no extensive review article capturing the entire horizon of the current DE-research has so far been published. In a recently published article [60], Neri and Tirronen reviewed a number of DE-variants for single-objective optimization problems and also made an experimental comparison of these variants on a set of numerical benchmarks. However, the article did not address issues like adapting DE to complex optimization environments involving multiple and constrained objective functions, noise and uncertainty in the fitness landscape, very large number of search variables, and so on. Also it did not focus on the most recent engineering applications of DE and the developments in the theoretical analysis of DE. This paper attempts to provide a comprehensive survey of the DE algorithm. its basic concepts, different structures, and variants for 6 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 15, NO. 1, FEBRUARY 2011 Fig. 1. Main stages of the DE algorithm. solving constrained, multiobjective, dynamic, and large-scale optimization problems as well as applications of DE variants to practical optimization problems. The rest of this paper is arranged as follows. In Section II, the basic concepts related to classical DE are explained along with the original formulation of the algorithm in the real number space. Section III discusses the parameter adaptation and control schemes for DE. Section IV provides an overview of several prominent variants of the DE algorithm. Section V provides an extensive survey on the applications of DE to the discrete, constrained, multiobjective, and dynamic optimization problems. The theoretical analysis of DE has been reviewed in Section VI. Section VII provides an overview of the most significant engineering applications of DE. The drawbacks of DE are pointed out in Section VIII. Section IX highlights a number of future research issues related to DE. Finally, the paper is concluded in Section X. II. Differential Evolutions: Basic Concepts and Formulation in Continuous Real Space Scientists and engineers from all disciplines often have to deal with the classical problem of search and optimization. Optimization means the action of finding the best-suited solution of a problem within the given constraints and flexibilities. While optimizing performance of a system, we aim at finding out such a set of values of the system parameters for which the overall performance of the system will be the best under some given conditions. Usually, the parameters governing the system performance are represented in a vector like \u0002X = [x1, x2, x3, ..., xD]T . For real parameter optimization, as the name implies, each parameter xi is a real number. To measure how far the ¡°best¡± performance we have achieved, an objective function (or fitness function) is designed for the system. The task of optimization is basically a search for such the parameter vector \u0002X., which minimizes such an objective function f (\u0002X)(f : \u0001 ¡ö \u0004D ¡æ \u0004), i.e., f (\u0002X.) < f(\u0002X) for all \u0002X ¡ô \u0001, where \u0001 is a non-empty large finite set serving as the domain of the search. For unconstrained optimization problems \u0001 = \u0004D. Since max \u0001f (\u0002X)\u0002 = .min \u0001.f (\u0002X)\u0002, the restriction to minimization is without loss of generality. In general, the optimization task is complicated by the existence of non-linear objective functions with multiple local minima. A local minimum f\u0002 = f (\u0002X\u0002) may be defined as ¢¤¥å > 0 ¢£\u0002X ¡ô \u0001 : \u0003\u0003 \u0002X . \u0002X\u0002\u0003\u0003 < ¥å ¢¡ f\u0002 ¡Â f (\u0002X), where \u000b",
    ".\u000b",
    " indicates any p-norm distance measure. DE is a simple real parameter optimization algorithm. It works through a simple cycle of stages, presented in Fig. 1. We explain each stage separately in Sections II-A.II-D. A. Initialization of the Parameter Vectors DE searches for a global optimum point in a D-dimensional real parameter space \u0004D. It begins with a randomly initiated population of NP D dimensional real-valued parameter vectors. Each vector, also known as genome/chromosome, forms a candidate solution to the multidimensional optimization problem. We shall denote subsequent generations in DE by G = 0, 1...,Gmax. Since the parameter vectors are likely to be changed over different generations, we may adopt the following notation for representing the ith vector of the population at the current generation: \u0002X i,G = [x1,i,G, x2,i,G, x3,i,G, ....., xD,i,G]. (1) For each parameter of the problem, there may be a certain range within which the value of the parameter should be restricted, often because parameters are related to physical components or measures that have natural bounds (for example if one parameter is a length or mass, it cannot be negative). The initial population (at G = 0) should cover this range as much as possible by uniformly randomizing individuals within the search space constrained by the prescribed minimum and maximum bounds: \u0002Xmin = {x1,min, x2,min, ..., xD,min } and \u0002X max = {x1,max, x2,max, ..., xD,max }. Hence we may initialize the jth component of the ith vector as xj,i,0 = xj,min + randi,j[0, 1] ¡¤ (xj,max . xj,min) (2) where randi,j[0, 1] is a uniformly distributed random number lying between 0 and 1 (actually 0 ¡Â randi,j[0, 1] ¡Â 1) and is instantiated independently for each component of the i-th vector. B. Mutation with Difference Vectors Biologically, ¡°mutation¡± means a sudden change in the gene characteristics of a chromosome. In the context of the evolutionary computing paradigm, however, mutation is also seen as a change or perturbation with a random element. In DE-literature, a parent vector from the current generation is called target vector, a mutant vector obtained through the differential mutation operation is known as donor vector and finally an offspring formed by recombining the donor with the target vector is called trial vector. In one of the simplest forms of DE-mutation, to create the donor vector for each ith target vector from the current population, three other distinct parameter vectors, say \u0002Xri1 , \u0002Xri2 , and \u0002Xri3 are sampled randomly from the current population. The indices ri1 , ri2 , and ri3 are mutually exclusive integers randomly chosen from the range [1, NP], which are also different from the base vector index i. These indices are randomly generated once for each mutant vector. Now the difference of any two of these three vectors is scaled by a scalar number F (that typically lies in the interval [0.4, 1]) and the scaled difference is added to the third one whence we obtain the donor vector \u0002Vi,G. We can express the process as \u0002V i,G = \u0002Xri1 ,G + F ¡¤ (\u0002Xri2 ,G . \u0002Xri3 ,G). (3) The process is illustrated on a 2-D parameter space (showing constant cost contours of an arbitrary objective function) in Fig. 2. DAS AND SUGANTHAN: DIFFERENTIAL EVOLUTION: A SURVEY OF THE STATE-OF-THE-ART 7 Fig. 2. Illustrating a simple DE mutation scheme in 2-D parametric space. C. Crossover To enhance the potential diversity of the population, a crossover operation comes into play after generating the donor vector through mutation. The donor vector exchanges its components with the target vector \u0002Xi,G under this operation to form the trial vector \u0002Ui,G = [u1,i,G, u2,i,G, u3,i,G, ..., uD,i,G]. The DE family of algorithms can use two kinds of crossover methods.exponential (or two-point modulo) and binomial (or uniform) [74]. In exponential crossover, we first choose an integer n randomly among the numbers [1,D]. This integer acts as a starting point in the target vector, from where the crossover or exchange of components with the donor vector starts. We also choose another integer L from the interval [1,D]. L denotes the number of components the donor vector actually contributes to the target vector. After choosing n and L the trial vector is obtained as uj,i,G = vj,i,G for j =... according to the following pseudo-code: L = 0; DO { L = L + 1; } WHILE ((rand(0, 1) ¡Â Cr) AND (L ¡Â D)). ¡°Cr¡± is called the crossover rate and appears as a control parameter of DE just like F. Hence in effect, probability (L = ¥ô) = (Cr)¥ô . 1 for any positive integer v lying in the interval [1, D]. For each donor vector, a new set of n and L must be chosen randomly as shown above. On the other hand, binomial crossover is performed on each of the D variables whenever a randomly generated number between 0 and 1 is less than or equal to the Cr value. In this case, the number of parameters inherited from the donor has a (nearly) binomial distribution. The scheme may be outlined as uj,i,G = \u0004 vj,i,G if (randi,j[0, 1] ¡Â Cr or j = jrand ) xj,i,G otherwise (5) Fig. 3. Different possible trial vectors formed due to uniform/binomial crossover between the target and the mutant vectors in 2-D search space. where, as before, randi,j[0, 1] is a uniformly distributed random number, which is called anew for each jth component of the ith parameter vector. jrand ¡ô [1, 2, ....,D] is a randomly chosen index, which ensures that \u0002Ui,G gets at least one component from \u0002Vi,G. It is instantiated once for each vector per generation. We note that for this additional demand, Cr is only approximating the true probability pCr of the event that a component of the trial vector will be inherited from the donor. Also, one may observe that in a 2-D search space, three possible trial vectors may result from uniformly crossing a mutant/donor vector \u0002Vi,G with the target vector \u0002Xi,G. These trial vectors are as follows. 1) \u0002Ui,G = \u0002Vi,G such that both the components of \u0002Ui,G are inherited from \u0002Vi,G. 2) \u0002U / i,G, in which the first component (j = 1) comes from \u0002V i,G and the second one (j = 2) from \u0002Xi,G. 3) \u0002U // i,G, in which the first component (j = 1) comes from \u0002X i,G and the second one (j = 2) from \u0002Vi,G. The possible trial vectors due to uniform crossover are illustrated in Fig. 3. D. Selection To keep the population size constant over subsequent generations, the next step of the algorithm calls for selection to determine whether the target or the trial vector survives to the next generation, i.e., at G = G + 1. The selection operation is described as \u0002X i,G+1 = \u0002Ui,G iff (\u0002Ui,G) ¡Â f (\u0002Xi,G) = \u0002Xi,G iff (\u0002Ui,G) > f(\u0002Xi,G) (6) where f (\u0002X) is the objective function to be minimized. Therefore, if the new trial vector yields an equal or lower value of the objective function, it replaces the corresponding target vector in the next generation; otherwise the target is retained in the population. Hence, the population either gets better (with respect to the minimization of the objective function) or remains the same in fitness status, but never deteriorates. Note that in (6) the target vector is replaced by the trial vector even if both yields the same value of the objective function.a feature that enables DE-vectors to move over 8 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 15, NO. 1, FEBRUARY 2011 Algorithm 1 Pseudo-code for the DE Algorithm with Binomial Crossover Step 1: Read values of the control parameters of DE: scale factor F, crossover rate Cr, and the population size NP from user. Step 2: Set the generation number G = 0 and randomly initialize a population of NP individuals PG = {\u0002X1,G, ......, \u0002XNP,G} with \u0002X i,G = [x1,i,G, x2,i,G, x3,i,G, ....., xD,i,G] and each individual uniformly distributed in the range [\u0002Xmin, \u0002Xmax], where \u0002Xmin = {x1,min, x2,min, ..., xD,min } and \u0002X max = {x1,max, x2,max, ..., xD,max } with i = [1, 2, ....,NP]. Step 3. WHILE the stopping criterion is not satisfied DO FOR i = 1 to NP //do for each individual sequentially Step 2.1 Mutation Step Generate a donor vector \u0002Vi,G = {v1,i,G, ......., } {vD,i,G} corresponding to the ith target vector \u0002X i,G via the differential mutation scheme of DE as: \u0002V i,G = \u0002Xri1 ,G + F ¡¤ (\u0002Xri2 ,G . \u0002Xri3 ,G). Step 2.2 Crossover Step Generate a trial vector \u0002Ui,G = {u1,i,G, ......., uD,i,G } for the ith target vector \u0002Xi,G through binomial crossover in the following way: uj,i,G = vj,i,G, if (randi,j[0, 1] ¡Â Cr or j = jrand ) xj,i,G, otherwise, Step 2.3 Selection Step Evaluate the trial vector \u0002Ui,G IF f (\u0002Ui,G) ¡Â f (\u0002Xi,G), THEN \u0002Xi,G+1 = \u0002Ui,G ELSE \u0002Xi,G+1 = \u0002Xi,G. END IF END FOR Step 2.4 Increase the Generation Count G = G + 1 END WHILE flat fitness landscapes with generations. Note that throughout this paper, we shall use the terms objective function value and fitness interchangeably. But, always for minimization problems, a lower objective function value will correspond to higher fitness. E. Summary of DE Iteration An iteration of the classical DE algorithm consists of the four basic steps.initialization of a population of search variable vectors, mutation, crossover or recombination, and finally selection. After having illustrated these stages, we now formally present the whole of the algorithm in a pseudo-code below. The terminating condition can be defined in a few ways like: 1) by a fixed number of iterations Gmax, with a suitably large value of Gmax depending upon the complexity of the objective function; 2) when best fitness of the population does not change appreciably over successive iterations; and alternatively 3) attaining a pre-specified objective function value. F. DE Family of Storn and Price Actually it is the process of mutation that demarcates one DE scheme from another. In the previous section, we have illustrated the basic steps of a simple DE. The mutation scheme in (3) uses a randomly selected vector \u0002Xr1 and only one weighted difference vector F ¡¤ (\u0002Xr2 . \u0002Xr3) to perturb it. Hence, in literature, the particular mutation scheme given by (3) is referred to as DE/rand/1. When used in conjunction with binomial crossover, the procedure is called DE/rand/1/bin. We can now have an idea of how the different DE schemes are named. The general convention used above is DE/x/y/z, where DE stands for ¡°differential evolution,¡± x represents a string denoting the base vector to be perturbed, y is the number of difference vectors considered for perturbation of x, and z stands for the type of crossover being used (exp: exponential; bin: binomial). The other four different mutation schemes, suggested by Storn and Price [74], [75] are summarized as ¡°DE/best/1 :\u000e\u000e \u0002Vi,G = \u0002Xbest,G + F ¡¤ (\u0002Xri1 ,G . \u0002Xri2 ,G) (7) ¡°DE/target . to . best/1 :\u000e\u000e \u0002Vi,G = \u0002Xi,G +F ¡¤ (\u0002Xbest,G . \u0002Xi,G) + F ¡¤ (\u0002Xri1 ,G . \u0002 Xri2 ,G) (8) ¡°DE/best/2 :\u000e\u000e \u0002Vi,G = \u0002Xbest,G + F ¡¤ (\u0002Xri1 ,G . \u0002Xri2 ,G) +F ¡¤ (\u0002Xri3 ,G . \u0002Xri4 ,G) (9) ¡°DE/rand/2 :\u000e\u000e \u0002Vi,G = \u0002Xri1 ,G + F ¡¤ (\u0002Xri2 ,G . \u0002Xri3 ,G) +F ¡¤ (\u0002Xri4 ,G . \u0002Xri5 ,G). (10) The indices ri1 , ri2 , ri3 , ri4 , and ri5 are mutually exclusive integers randomly chosen from the range [1, NP], and all are different from the base index i. These indices are randomly generated once for each donor vector. The scaling factor F is a positive control parameter for scaling the difference vectors. \u0002X best,G is the best individual vector with the best fitness (i.e., lowest objective function value for a minimization problem) in the population at generation G. Note that some of the strategies for creating the donor vector may be mutated recombinants, for example, (8) listed above basically mutates a two-vector recombinant \u0002Xi,G + F ¡¤ (\u0002Xbest,G . \u0002Xi,G). Storn and Price [74], [92] suggested a total of ten different working strategies for DE and some guidelines in applying these strategies to any given problem. These strategies were derived from the five different DE mutation schemes outlined above. Each mutation strategy was combined with either the ¡°exponential¡± type crossover or the ¡°binomial¡± type crossover. This yielded a total of 5¡¿2 = 10 DE strategies. In fact many other linear vector combinations can be used for mutation. In general, no single mutation method [among those described in (3), (7).(10)] has turned out to be best for all problems. DAS AND SUGANTHAN: DIFFERENTIAL EVOLUTION: A SURVEY OF THE STATE-OF-THE-ART 9 Nevertheless the various mutation schemes need further investigation to determine under which circumstances they perform well and on what kind of problems they yield poor results. Some initial work in this direction was undertaken by Mezura- Montes et al., who empirically compared eight different DEschemes over a test-suite of 13 benchmark problems in [56]. The authors took into account an interesting mutation scheme known as DE/rand/2/dir [26] that incorporates the objective function information to guide the direction of the donor vectors in the following way: \u0002V i,G = \u0002Xr1,G + F 2 ¡¤ (\u0002Xr1,G . \u0002Xr2,G . \u0002Xr3,G) (11) where \u0002Xr1,G, \u0002Xr2,G, and \u0002Xr3,G are distinct population members such that f (\u0002Xr1,G) ¡Â \u0001f (\u0002Xr2,G), f(\u0002Xr3,G)\u0002. The experiments performed by Mezura-Montes et al. indicate that DE/best/1/bin (using always the best solution to find search directions and also binomial crossover) remained the most competitive scheme, regardless the characteristics of the problem to be solved, based on final accuracy and robustness of results. The authors in [56] also mention that over unimodal and separable functions, DE/rand/2/dir achieved considerably good results. For unimodal and non-separable functions, DE/best/1/bin consistently yielded best performance. This variant was also successful in optimizing the multimodal and separable benchmarks. DE/rand/1/bin and DE/rand/2/dir provided performances of similar quality on this class of functions. However, on multimodal and non-separable functions DE/rand/2/dir remained most competitive and slightly faster to converge to the global optimum. G. DE and the Contemporary EAs: Conceptual Similarities and Differences In this section, we briefly discuss how DE relates to and differs from the contemporary EAs for real parameter optimization. Following the convention of ES, we shall use ¥ì to indicate the number of parent vectors and ¥ë (¡Ã ¥ì) to denote the size of the child population. 1) Mutation: In the context of GAs and EAs, mutation is treated as a random change of some parameter. Real valued EAs typically simulate the effects of mutation with additive increments that are randomly generated by a predefined and fixed probability density function (PDF). DE differs markedly from algorithms like ES and EP in consideration of the fact that it mutates the base vectors (secondary parents) with scaled population-derived difference vectors. As generations pass, these differences tend to adapt to the natural scaling of the problem. For example, if the population becomes compact in one variable but remains widely dispersed in another, the difference vectors sampled from it will be small in the former variable, yet large in the latter. This automatic adaptation significantly improves the convergence of the algorithm. In other words, ES and EP require the specification or adaptation of absolute step size for each variable over generations while DE requires only the specification of a single relative scale factor F for all variables. Although the difference vector based mutation is believed to be one of the main strength of DE [73], [92], the idea of using difference of population members in recombination of EAs is not completely new. Eshelman and Schaffer [S187] came up with an idea of a difference-based recombination operator [called blend crossover operator (BLX)] for real coded GAs, long back in 1992. Voigt et al. [S188] used a selection differential defined as the difference between the mean fitness of the selected parents and the mean fitness of the population to derive a design criteria for recombination operators and used it with the fuzzy recombination (FR). In 1995, Deb and Agrawal [S189] proposed a simulated binary crossover (SBX) that works with two parent solutions and creates two offspring solutions to simulate the working principle of the single-point crossover operator on binary strings. In SBX, the probability distribution used to create offspring depends on a spread factor that is defined as ratio of the absolute difference in children values to that of the parent values. Both BLX and SBX were analyzed in detail by Deb and Beyer in [8] and [24]. In Kita et al.¡¯s uniform normal distribution crossover (UNDX) [S190] and simplex crossover (SPX) [S191], operators generate ¥ì.1 direction vectors for ¥ì . 1 randomly chosen parents by taking the difference of each parent vector from their mean vector. Using the direction vectors, in UNDX the probability of creating the offspring away from the mean vector is reduced and a maximum probability is assigned at the mean vector. SPX assigns a uniform probability for creating any offspring within a restricted region (called the simplex). In Deb et al.¡¯s parent centric crossover (PCX) operator [S192] for each offspring one parent is chosen randomly and a difference vector is calculated between the parent and the mean of the chosen ¥ì parents. However, the use of scaled difference of any two distinct population members to perturb a third one, as done in DE, finds closest resemblance with the reflection operation of Nelder-Mead polyhedron search [S19] and Price¡¯s CRS algorithm [S20]. Although due to space limitations, it is not possible to discuss these two algorithms in sufficient details, for interested readers we would like to point out that unlike DE, the Nelder-Mead algorithm restricts the number of sample vectors (from which the difference vector is to be generated) to D + 1 (D corresponding to the dimensionality of the search space). This limitation becomes a drawback for complicated objective functions that require many more points to form a clear model of the surface topography. Also both Nelder-Mead¡¯s and CRS¡¯s reflection operations with difference vectors are a form of arithmetic recombination, while DE¡¯s difference vector based perturbation schemes more closely resemble a mutation operation [74, p. 29]. One of the most fundamental aspects of DE-type mutation is the fact that vector perturbations are generated from the NP ¡¤ (NP . 1) nonzero difference vectors of the population rather than employing a predetermined PDF. This leads to one of the main assets of DE: contour matching, a term coined and explained by Price et al. in [74]. Contour matching refers to the phenomena of adaptation of the vector population such that promising regions of the fitness landscape are investigated automatically once they are detected. One of the biggest advantages that the difference vectors afford is that both a mutation step¡¯s size and its orientation are automatically adapted to the objective function landscape. Price et al. claim that contour matching 10 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 15, NO. 1, FEBRUARY 2011 Fig. 4. Empirical distributions of candidate trial vectors for three different Cr values. (a) Cr = 0. (b) Cr = 0.5. (c) Cr = 1.0. also induces an important ingredient besides selection is the promotion of basin-to-basin transfer, where search points may move from one basin of attraction, i.e., a local minimum, to another one. In PSO also the stochastic attraction toward the personal best and neighborhood best positions are modeled by scaled difference vectors. The velocity update formula of PSO has similarities with the DE/target-to-best/1 scheme [see (8)] that generates a mutated recombinant. 2) Crossover: Both DE and ES employ crossover to create a single trial vector, while most GAs recombine two vectors to produce two trial vectors often by one-point crossover. Note that EP depends only on mutation to generate offspring and does not have any crossover operator associated with. One of the popular crossover techniques for real coded GAs is the n-point crossover where the offspring vector is randomly partitioned into (n + 1) blocks such that parameters in adjacent partitions are inherited from different parent vectors. Studies of n-point crossover [S193] indicate that an even number of crossover points reduces the representational bias (dependence of ordering of parameters within a vector) at the cost of increasing the disruption of parameters that are closely grouped. As analyzed by Price et al. [92, p. 93] DE¡¯s exponential crossover employs both one and two point crossover with an objective of reducing their individual biases. The representational bias inherent in n-point crossover can be eliminated if donors are determined by D independent random trials. This procedure is known as uniform crossover in EA literature [S3] and this is exactly what DE employs as discrete recombination or binomial crossover [see (5)] most often. 3) Selection: Selection can be applied to an evolutionary process in primarily two different stages.first stage being parent selection to decide which vectors from the current population will undergo recombination while the second is survivor selection to choose which vectors from the parent and offspring populations will survive to the next generation. Unlike GAs that select parents based on their fitness, both ES and DE gives all the individuals equal chance for being selected as parents. In ES, each individual has the same chance to be selected for mutation (and recombination). In DE also the base vectors are randomly picked up without any regard for their fitness values. When only the offspring vectors are allowed to advance (as done in some simple GAs [S3]) there is no guarantee that the best-so-far solution will not be lost. Retaining the best-so-far solution is called elitism and it plays an important role in bringing the convergence of the algorithm to the global optimum [S193] in long time limits. For this reason and because of the speed improvement it offers, most EAs including DE, EP, and some versions of ES take into account the current population while determining the membership of the next generation. The (¥ì, ¥ë) ES selects best ¥ì children to become parents in next generation. Alternatively, the (¥ì + ¥ë) ES populates the next generation with best ¥ì vectors from the combined parent and child populations. The survivor selection scheme of DE is closer in spirit to the elitist (¥ì + ¥ë) ES, however, instead of ranking the combined population, the former employs a oneto- one competition where each parent vector competes once only against its own offspring. Evidently, unlike the tournament selection in EP, DE¡¯s one-to-one selection holds only NP knock-out competitions between a parent and its offspring generated through mutation and recombination. Comparing each trial vector (offspring) to the best performing vectors at the same index ensures that DE retains the very bestso- far solution at each index. Parent-offspring competition has a superior ability to maintain population diversity when compared with ranking or tournament selection where elites and their offspring may dominate the population rapidly. III. Control Parameters of the Differential Evolution There are three main control parameters of the DE algorithm: the mutation scale factor F, the crossover constant Cr, and the population size NP. In this section, we focus on the effect of each of these parameters on the performance of DE as well as the state-of-the-art methods for tuning these parameters. A good volume of research work has been undertaken so far to improve the ultimate performance of DE by tuning its control parameters. Storn and Price in [88] have indicated that a reasonable value for NP could be chosen between 5-D and 10-D (D being the dimensionality of the problem), and a good initial choice of F was 0.5. The effective range of F is usually between 0.4 and 1. The parameter Cr controls how many parameters in expectation are changed in a population member. For low value DAS AND SUGANTHAN: DIFFERENTIAL EVOLUTION: A SURVEY OF THE STATE-OF-THE-ART 11 of Cr, a small number of parameters are changed in each generation and the stepwise movement tends to be orthogonal to the current coordinate axes. On the other hand, high values of Cr (near 1) cause most of the directions of the mutant vector to be inherited prohibiting the generation of axis orthogonal steps. This effect has been illustrated in Fig. 4 by showing for three values of Cr, an empirical distribution of candidate trial vectors obtained by running DE on a single starting population of ten vectors for 200 generations with selection disabled. It is interesting to note at this point that for algorithms like classic DE (DE/rand/1/bin) performance is rotationally invariant only when Cr = 1. At that setting, crossover is a vector-level operation that makes the trial vector a pure mutant, i.e., \u0002Ui,G = \u0002Xri1 ,G+F¡¤(\u0002Xri2 ,G .\u0002Xri3 ,G). The location (with respect to the function¡¯s topography) of mutant trial vectors will not change under coordinate rotation as long as Cr = 1 and F is a constant, or sampled from a distribution no more than once per trial vector. A low Cr value (e.g., 0 or 0.1) results in a search that changes each direction (or a small subset of directions) separately. This is an effective strategy for functions that are separable or decomposable [i.e., f (\u0002X) = \u0005D i=1 fi(xi)] Gamperle et al. [S26] evaluated different parameter settings for DE on the Sphere, Rosenbrock¡¯s, and Rastrigin¡¯s functions. Their experimental results revealed that the global optimum searching capability and the convergence speed are very sensitive to the choice of control parameters NP, F, and Cr. Furthermore, a plausible choice of the population size NP is between 3-D and 8-D, the scaling factor F = 0.6, and the crossover rate Cr is between [0.3, 0.9]. Recently, the authors in [85] state that typically 0.4 < F < 0.95 with F = 0.9 can serve as a good first choice. They also opine that Cr should lie in (0, 0.2) when the function is separable, while in (0.9, 1) when the function¡¯s parameters are dependent. As can be perceived from the literature, several claims and counter-claims were reported concerning the rules for choosing the control parameters and these can potentially confuse engineers, who may try to solve practical problems with DE. Further, most of these claims lack sufficient experimental justifications. Some objective functions are very sensitive to the proper choice of the parameter settings in DE [S27]. Therefore, researchers naturally started to consider some techniques such as self-adaptation to automatically find an optimal set of control parameters for DE [S28], [3], [10], [48], [76], [84]. Usually self-adaptation is applied to tune the control parameters F and Cr. Liu and Lampinen [48] introduced a Fuzzy adaptive differential evolution using fuzzy logic controllers whose inputs incorporate the relative function values and individuals of successive generations to adapt the parameters for the mutation and crossover operation. In this context, Qin et al. [76] came up with a SaDE algorithm, in which both the trial vector generation strategies and their associated control parameters F and Cr are gradually self-adapted by learning from their previous experiences of generating promising solutions. The parameter F, in SaDE, is approximated by a normal distribution with mean value 0.5 and standard deviation 0.3, denoted by N (0.5, 0.3). A set of F values are randomly sampled from such normal distribution and applied to each target vector in the current population. This way, SaDE attempts to maintain both exploitation (with small F values) and exploration (with large F values) power throughout the entire evolution process. SaDE gradually adjusts the range of Cr values for a given problem according to previous Cr values that have generated trial vectors successfully entering the next generation. Specifically, it is assumed that Cr obeys a normal distribution with mean value Crm and standard deviation Std = 0.1, denoted by N(Crm, Std), where Crm is initialized as 0.5. The Std should be set as a small value to guarantee that most Cr values generated by N(Crm, Std) are between [0, 1], even when Crm is near 0 or 1. Hence, the value of Std is set as 0.1. Note that the self-adaptive schemes like SaDE often themselves have parameters to be adjusted like the standard deviation in normal distribution. However, self-adaptive DE performs better than the standard DE because sensitive parameters in DE are replaced by less sensitive parameters in self-adaptive DE. In [3], a fitness-based adaptation has been proposed for F. A system with two evolving populations has been implemented. The crossover rate Cr has been fixed to 0.5 after an empirical study. Unlike Cr, the value of F is adaptively updated at each generation by means of the following scheme: F = .. . max \tlmin, 1 . otherwise (12) where lmin = 0.4 is the lower bound of f, fmin and fmax are the minimum and maximum objective function values over the individuals of the populations, obtained in a generation. Recently, Brest et al. [10] proposed a self-adaptation scheme for the DE control parameters. They encoded control parameters F and Cr into the individual and adjusted them by introducing two new parameters ¥ó1 and ¥ó2. In their algorithm (called ¡°jDE¡±), a set of F and Cr values was assigned to each individual in the population, augmenting the dimensions of each vector. The better values of these encoded control parameters lead to better individuals that in turn, are more likely to survive and produce offspring and, thus, propagate these better parameter values. The new control parameters for the next generation are computed as follows: Fi,G+1 = Fl + rand1 . Fu with probability¥ó1 = Fi,G else \f",
    " (13a) and Cri,G+1 = rand3 with probability¥ó2 = Cri,G else \f",
    " (13b) where Fl and Fu are the lower and upper limits of F and both lie in [0, 1]. In [10] and [S231], Brest et al. used ¥ó1 = ¥ó2 = 0.1. As Fl = 0.1 and Fu = 0.9, the new F takes a value from [0.1, 0.9] while the new Cr takes a value from [0, 1]. As Fi,G+1 and CRi,G+1 values are obtained before the mutation is performed, they influence the mutation, crossover, and selection operations for the new vector \u0002Xi,G+1. Zaharie [S29] proposed a parameter adaptation strategy for DE (ADE) based on the idea of controlling the population diversity, and implemented a multipopulation approach. Following the same line of thinking, Zaharie and Petcu [S30] 12 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 15, NO. 1, FEBRUARY 2011 designed an adaptive Pareto DE algorithm for multiobjective optimization and also analyzed its parallel implementation. Abbass [1] self-adapted the crossover rate Cr for multiobjective optimization problems, by encoding the value of Cr into each individual, simultaneously evolved with other search variables. The scaling factor F was generated for each variable from a Gaussian distribution N(0, 1). The upper limit of the scale factor F is empirically taken as 1. Although it does not necessarily mean that a solution is not possible with F > 1, however, until date, no benchmark function that was successfully optimized with DE required F > 1. Zaharie [112] derived a lower limit of F and the study [112] revealed that if F is sufficiently small, the population can converge even in the absence of selection pressure. With a few simplifying assumptions, Zaharie proved the following relation between the variance of the original population Px,t at time step G and the variance of the trial population Pu,t E(Var(Pu,t)) ... where pCr is the probability of crossover [Zaharie neglected the jrand part in (5) and then Cr became the absolute probability that a component of the target vector is exchanged with that of the donor vector; Zaharie used the notation pCr to denote this probability instead of Cr]. Consequently, the DE control parameter combinations that satisfy the equation 2.F2 . 2 NP + pCr NP = 0 (15) may be considered as critical since they result in a population whose variance remains constant except for random fluctuations. Thus, when the selection step is absent, according to (13), F will display a critical value Fcrit such that the population variance decreases when F < Fcrit and increases if F > Fcrit . Solving (13), we have Fcrit = \u000f\u00101 . pCr\u00112\u0012 NP . (16) Zaharie experimentally confirmed that Fcrit establishes a lower limit on the value of F in the sense that smaller values will induce convergence even on a flat objective function landscape (when all trial vectors are accepted, i.e., selection pressure is absent). Omran et al. [65] introduced a self-adaptive scaling factor parameter F. They generated the value of Cr for each individual from a normal distribution N(0.5, 0.15). This approach (called ¡°SDE¡±) was tested on four benchmark functions and performed better than other versions of DE. Besides adapting the control parameters F or Cr, some researcher also adapted the population size. Teo [102] proposed DE with self-adaptive population size NP (abbreviated as DESAP), based on self-adaptive Pareto DE proposed by Abbas [1]. Mallipeddi and Suganthan [50] empirically investigated the effect of population size on the quality of solutions and the computational effort required by DE with a set of five problems chosen from the test-suite of CEC 2005 Special Session on Real-Parameter Optimization [95]. In [11], the authors presented a method for gradually reducing population size of DE. The method improves the efficiency and robustness of the algorithm and can be applied to any variant of a DE algorithm. In [51], Mallipeddi and Suganthan proposed a DE algorithm with an ensemble of parallel populations, where the number of function evaluations (FEs) allocated to each population is self-adapted by learning from their previous experiences in generating superior solutions. Consequently, a more suitable population size along with its parameter settings can be determined adaptively to match different search/evolution phases. Apart from self-adaptation, frequently F has been made to vary randomly for improving the performances of DE. Price et al. [74] defined two new terms: jitter and dither in context to the randomization of F. The practice of generating a new value of F for every parameter is called jitter and it is signified by subscripting F with the parameter index, j. Alternatively, choosing F anew for each vector, or dithering, is indicated by subscripting F with the population¡¯s running index, i. Dithering scales the length of vector differentials because the same factor, Fi, is applied to all components of a difference vector. Das et al. used dither in [17] where F was made to vary randomly between 0.5 and 1 for each vector. In the same paper, they also suggested decreasing F linearly from 1.0 to 0.5 in their second scheme (called DETVSF: DE with time varying scale factor). This encourages the individuals to sample diverse zones of the search space during the early stages of the search (promoting exploration). During the later stages a decaying scale factor helps to adjust the movements of trial solutions finely so that they can explore the interior of a relatively small space in which the suspected global optimum lies (thus promoting exploitation). Recently in works like [S31] and [S32], chaotic sequences are combined with DE in order to enhance its population diversity and thus to avoid the state of stagnation, when a standard DE may occasionally stop proceeding toward the global optimum virtually without any obvious reasons [41]. That means although the population has not converged to a local optimum or any other point, the population is still remaining diverse, and occasionally, even new individuals may enter the population, but the algorithm does not progress by finding any better solutions. Chaos theory [S33] deals with the qualitative study of unstable aperiodic behavior in deterministic nonlinear dynamical systems. In chaotic DE the scale factor F is varied over generations by using the logistic map iterator, which is one of the simplest dynamic systems evidencing chaotic behavior, in the following way: FG = ¥ì ¡¤ FG.1 ¡¤ [1 . FG.1]. (17) IV. Important Variants of DE for Continuous Single-Objective Optimization Since its advent in 1995, DE has been attracting the attention of the researchers from diverse domains of knowledge, all over the world. This has resulted in a wealth of variants of the basic DE algorithm. Some of these variants are devised to tackle specific applications while others are generalized for numerical optimization. In this section, we shall undertake DAS AND SUGANTHAN: DIFFERENTIAL EVOLUTION: A SURVEY OF THE STATE-OF-THE-ART 13 an in-depth discussion of the most prominent DE-variants that were developed over the past decade and appeared to be competitive against the existing best-known real parameter optimizers. A. Differential Evolution Using Trigonometric Mutation Fan and Lampinen [25] proposed a trigonometric mutation operator for DE to speed up its performance. To implement the scheme, for each target vector, three distinct vectors are randomly selected from the DE population. Suppose for the ith target vector \u0002Xi,G, the selected population members are \u0002X r1,G, \u0002Xr2,G, and \u0002Xr3,G. The indices r1, r2, and r3 are mutually exclusive integers randomly chosen from the range [1, NP], which are also different from the index i. Now three weighting coefficients are formed according to the following equations: (18d) where f () is the function to be minimized. Let \u0007 be the trigonometric mutation rate in the interval (0, 1). Then the trigonometric mutation scheme may now be expressed as \u0002V i,G+1 = (\u0002Xr1 + \u0002Xr2 + \u0002Xr3)\u00113 + (p2 . p1).(\u0002Xr1 . \u0002Xr2)+ (p3 . p2) ¡¤ (\u0002Xr2 . \u0002Xr3) + (p1 . p3) ¡¤ (\u0002Xr3 . \u0002Xr1)if rand[0, 1] ¡Â \u0007 \u0002V i,G+1 = \u0002Xr1 + F ¡¤ (\u0002Xr2 . \u0002Xr3) else. (19) Thus, the scheme proposed by Fan et al. used trigonometric mutation with a probability of \u0007 and the mutation scheme of DE/rand/1 with a probability of (1 . \u0007). B. Differential Evolution Using Arithmetic Recombination The binomial crossover scheme, usually employed in most of the DE variants, creates new combinations of parameters; it leaves the parameter values themselves unchanged. Binomial crossover is in spirit same as the discrete recombination used in conjunction with many EAs. However, in continuous or arithmetic recombination, the individual components of the trial vector are expressed as a linear combination of the components from mutant/donor vector and the target vector. The common form of the arithmetic recombination between two vectors \u0002Xr1,G and \u0002Xr2,G adopted by most of the EAs [S3] may be put as \u0002W i,G = \b Xr1,G +ki ¡¤ (\u0002Xr1,G . \u0002Xr2,G). (20) The coefficient of combination ki can either be a constant or a random variable. Generally speaking, if this coefficient is sampled anew for each vector then the resulting process is known as line recombination. However, if the combination coefficient is elected randomly anew for each component of the vectors to be crossed, then the process is known as Fig. 5. Domains of the different recombinant vectors generated using discrete, line and random intermediate recombination. intermediate recombination and may be formalized for the jth component of the recombinants as wi,j,G = xr1,j,G + kj ¡¤ (xr1,j,G . xr2,j,G). (21) Fig. 5 schematically shows the regions searched by discrete, line and arithmetic recombination between donor vector \u0002Vi,G and the target vector \u0002Xi,G when the coefficient of combination is a uniformly distributed random number between 0 and 1. The two recombinant vectors occupy the opposite corners of a hypercube whose remaining corners are the trial vectors \u0002U/ i,G and \u0002U // i,G created by discrete recombination. Line recombination, as its name suggests, searches along the axis connecting the recombinant vectors, while the intermediate recombination explores the entire D-dimensional volume contained within the hypercube. As can be perceived from Fig. 5, both the discrete as well as the intermediate recombination are not rotationally invariant processes. If the coordinate system rotates through an angle, the corners of the hypercube are relocated, which in turn redefines the area searched by the intermediate recombination. On the other hand, the line recombination is rotationally invariant. To make the recombination process of DE rotationally invariant, Price proposed a new trial vector generation strategy ¡°DE/current-to-rand/1¡± [75], which replaces the binomial crossover operator with the rotationally invariant arithmetic line recombination operator to generate the trial vector \u0002Ui,G by linearly combining the target vector \u0002Xi,G and the corresponding donor vector \u0002Vi,G as follows: \u0002U i,G = \u0002Xi,G + ki ¡¤ (\u0002Vi,G . \u0002Xi,G). (22) Now incorporating (3) in (22) we have \u0002U i,G = \u0002Xi,G + ki ¡¤ (\u0002Xr1,G + F ¡¤ (\u0002Xr2,G . \u0002Xr3,G) . \u0002Xi,G) (23) which further simplifies to \u0002U i,G = \u0002Xi,G + ki ¡¤ (\u0002Xr1,G . \u0002Xi,G) + F \u000e ¡¤ (\u0002Xr2,G . \u0002Xr3,G) (24) where ki is the combination coefficient, which has been experimentally shown [74], [75] to be effective when it is chosen with a uniform random distribution from [0, 1] and F \u000e = ki ¡¤ F is a new constant parameter. 14 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 15, NO. 1, FEBRUARY 2011 Fig. 6. Change of the trial vectors generated through the discrete and random intermediate recombination due to rotation of the coordinate system. \u0002U R/ i,G and \u0002U R// i,G indicate the new trial vectors due to discrete recombination in rotated coordinate system. C. DE/rand/1/Either-Or Algorithm Price et al. [74, p. 118] proposed the state-of-the-art DE/rand/1/either-or algorithm, where the trial vectors that are pure mutants occur with a probability pF and those that are pure recombinants occur with a probability 1 . pF . This variant is shown to yield competitive results against classical DE-variants rand/1/bin and target-to-best/1/bin in a recent comparative study [21]. The scheme for trial vector generation may be outlined as \u0002U i,G = \u0002X r1,G + F.(\u0002X r2,G . \u0002Xr3,G) ifrandi(0, 1) < pF = \u0002Xr0,G + k.(\u0002Xr1 + \u0002Xr2 . 2.\u0002Xr0 ) otherwise \u0013 . (25) Price et al. recommended k = 0.5 ¡¤ (F + 1) as a good choice for the parameter k for a given F. The DE/rand/1/eitheror algorithm provides a simple way to implement the dual axis search in the k-F plane (k indicating the combination coefficient of the arithmetic crossover and F being the scale factor). The scheme provides efficient solutions for functions that are best minimized by either mutation-only (pF = 1) or recombination only (pF = 0), as well as generic functions that can be solved by randomly interleaving both operations (0 < pF < 1). Note that pF is a parameter of the algorithm and it determines the relative importance of the mutation and arithmetic recombination schemes. Price et al. recommend a value 0.4 for it. It is interesting to investigate whether it is possible to self-adapt pF so that the algorithm may be able to decide the optimal value of this parameter capturing some special properties of the objective function under test. D. Opposition-Based Differential Evolution The concept of opposition-based learning was introduced by Tizhoosh [S34] and its applications were introduced in [S34].[S36]. Rahnamayan et al. [82] have recently proposed an ODE for faster global search and optimization. The algorithm also finds important applications to the noisy optimization problems [S37]. The conventional DE was enhanced by utilizing opposition number based optimization concept in three levels, namely, population initialization, generation jumping, and local improvement of the population¡¯s best member. In the absence of a priori information about the actual optima, an EA usually starts with random guesses. We can improve our chance of starting with a better solution by simultaneously checking fitness of the opposite solution. By doing this, the fitter one (guess or opposite guess) can be chosen as an initial solution. As explained in [S34], according to probability theory, 50% of the time a guess may have lower fitness value than its opposite guess. Therefore, starting with the fitter of the two guesses has the potential to accelerate convergence. The same approach can be applied not only to initial solutions but also continuously to each solution in the current population. Also, when the population begins to converge into a smaller neighborhood surrounding an optimum, taking opposition moves can increase diversity of the population. In addition, when the population converges, the magnitude of difference vectors will become smaller. However, difference vectors generated by using parents that just underwent an opposite move will be large thereby resulting in larger perturbation in the mutant vector. Therefore, ODE possesses superior capability to jump out of local optima basins. Before discussing the steps of ODE, below we define opposite numbers. Definition 1: Let x be a real number defined in the closed interval [a, b], i.e., x ¡ô [a, b]. Then the opposite number \u0014x of x may be defined as \u0014x = a + b . x. (26) The ODE changes the classical DE using the concept of opposite numbers at the following three different stages. 1) Opposition based population initialization: first a uniformly distributed random population P(NP) is generated and then the opposite population OP(NP) is calculated. The kth opposite individual corresponding to the kth parameter vector of P(NP) is [following (26)], OPk,j = ak,j + bk,j . Pk,j , where k = 1, 2, ....,NP and j = 1, 2, ....,D, ak,j and bk,j denote the interval boundaries of j-th parameter of the kth vector, i.e., xk,j ¡ô [ak,j, bk,j ]. Finally, NP fittest individuals are selected from the set {P(NP),OP(NP)} as the initial population. 2) Opposition based generation jumping: in this stage, after each iteration, instead of generating new population by evolutionary process, the opposite population is calculated with a predetermined probability Jr(¡ô (0, 0.04)) and the NP fittest individuals may be selected from the current population and the corresponding opposite population. 3) Opposition based best individual jumping: in this phase, at first a difference-offspring of the best individual in the current population is created as \u0002X new best,G = \u0002Xbest,G + F \u000e ¡¤ (\u0002Xr1,G . \u0002Xr2,G) where r1 and r2 are mutually different random integer indices selected from {1, 2, ..., NP} and F \u000e DAS AND SUGANTHAN: DIFFERENTIAL EVOLUTION: A SURVEY OF THE STATE-OF-THE-ART 15 is a real constant. Next the opposite of offspring is generated as \u0002Xopp newbest,G. Finally, the current best member is replaced by the fittest member of the set {\u0002Xbest,G, \u0002Xnew best,G, \u0002Xopp newbest,G. E. DE with Neighborhood-Based Mutation The efficiency of most EAs depends on their extent of explorative and exploitative tendencies during the course of search. Exploitation means the ability of a search algorithm to use the information already collected and thus to orient the search more toward the goal while exploration is the process that allows introduction of new information into the population. Exploration helps the algorithm to quickly search the new regions of a large search-volume. Das et al. [21] proposed two kinds of topological neighborhood models for DE in order to achieve better balance between its explorative and exploitative tendencies. The resulting algorithm, called DEGL, was put forward as an improvement over the DE/target-to-best/1 scheme, which shows a poor performance on multimodal fitness landscapes, as noted in the studies of Mezura-Montes et al. [56] and Price et al.. [74, p. 156]. Suppose we have a DE population PG = [\u0002X1,G, \u0002X2,G, ...., \u0002X NP,G] at generation G. The vector indices are sorted only randomly (as obtained during initialization) in order to preserve the diversity of each neighborhood. Now for every vector \u0002X i,G we define a neighborhood of radius k (where k is a nonzero integer from 0 to (NP . 1)\u00112 as the neighborhood size must be smaller than the population size, i.e., 2k + 1 ¡Â NP), consisting of vectors \u0002Xi.k,G, ..., \u0002Xi,G, ..., \u0002Xi+k,G. We assume the vectors to be organized on a ring topology with respect to their indices, such that vectors \u0002XNP,G and \u0002X2,G are the two immediate neighbors of vector \u0002X1,G. For each member of the population a local donor vector is created by employing the best (fittest) vector in the neighborhood of that member and any two other vectors chosen from the same neighborhood. The model may be expressed as \u0002L i,G = \u0002Xi,G + ¥á ¡¤ (\u0002Xn besti,G . \u0002Xi,G) + ¥â ¡¤ (\u0002Xp,G . \u0002Xq,G) (27) where the subscript n besti indicates the best vector in the neighborhood of \u0002Xi,G and p, q ¡ô [i . k, i + k] with p \u0010= q \u0010= i. Similarly, the global donor vector is created as \u0002g i,G = \u0002Xi,G + ¥á ¡¤ (\u0002Xg best,G . \u0002Xi,G) + ¥â ¡¤ (\u0002Xr1,G . \u0002Xr2,G) (28) where the subscript g best indicates the best vector in the entire population at iteration G and r1, r2 ¡ô [1,NP] with r1 \u0010= r2 \u0010= i. ¥á and ¥â are the scaling factors. Now we combine the local and global donor vectors using a scalar weight w ¡ô (0, 1) to form the actual donor vector of the proposed algorithm \u0002V i,G = w ¡¤ \u0002gi,G + (1 . w) ¡¤ \u0002Li,G. (29) Clearly, if w = 1 and in addition ¥á = ¥â = F, the donor vector generation scheme in (30) reduces to that of DE/targetto- best/1. Hence, the latter may be considered as a special case of this more general strategy involving both global and local neighborhood of each vector synergistically. Note that DE/target-to-best/1, in its present form, favors exploitation only, since all the vectors are attracted toward the same best position found so far by the entire population, thereby converging faster toward the same point. In DEGL, a vector¡¯s neighborhood is the set of other parameter vectors that it is connected to; it considers their experience when updating its position. The graph of inter-connections is called the neighborhood structure. Generally, neighborhood connections are independent of the positions pointed to by the vectors. In the local model, whenever a parameter vector points to a good region of the search space, it only directly influences its immediate neighbors. Its second degree neighbors will only be influenced after those directly connected to them become highly successful themselves. Thus, there is a delay in the information spread through the population regarding the best position of each neighborhood. Therefore, the attraction to specific points is weaker, which reduces the chances of getting trapped in local minima. F. DE with Adaptive Selection of Mutation Strategies DE can encompass a number of trial vector generation strategies, each of which may be effective over certain problems but poorly perform over the others [26]. In [76], for the first time Qin et al. proposed a self-adaptive variant of DE (SaDE), where along with the control parameter values the trial vector generation strategies are also gradually self-adapted by learning from their previous experiences in generating promising solutions. Consequently, it is possible to determine a more suitable generation strategy along with its parameter settings adaptively to match different phases of the search process/evolution. In SaDE, four effective trial vector generation strategies namely the DE/rand/1/bin, DE/rand-to-best/2/bin, DE/rand/ 2/bin and finally DE/current-to-rand/1 were chosen to constitute a strategy candidate pool. The first three DE-variants are equipped with binomial type crossover while the last one uses arithmetic recombination (described in Section IV-B). In the SaDE algorithm, for each target vector in the current population, one trial vector generation strategy is selected from the candidate pool according to the probability learned from its success rate in generating improved solutions (that can survive to the next generation) within a certain number of previous generations, called the learning period (LP). The selected strategy is subsequently applied to the corresponding target vector to generate a trial vector. More specifically, at each generation, the probabilities of choosing each strategy in the candidate pool are summed to 1. These probabilities are initially equal (1/K for K total number of strategies in the pool) and are then gradually adapted during evolution, based on the Success and Failure Rates [76] over the previous LP generations. The adaptations of the probabilities take place in such a fashion that, the larger the success rate for the kth strategy in the pool within the previous LP generations, the larger is the probability of applying it to generate trial vectors at the current generation. The performance of SaDE was compared with the conventional DE and three adaptive DE-variants: ADE [S29], SDE [65], and jDE [10] (already discussed in Section III) over a 16 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 15, NO. 1, FEBRUARY 2011 suite of 26 bound constrained numerical optimization problems, and the authors reported that, SaDE was more effective in obtaining better quality solutions, with the relatively smaller standard deviations, and higher success rates. G. Adaptive DE with DE/Current-to-pbest Mutation In order to avoid the need for problem specific parameter tuning and also to improve the convergence characteristics of DE, an adaptive DE-variant, called JADE, was recently proposed [118]. The algorithm implements a new mutation strategy, referred by the authors as DE/current-to-pbest and uses an optional external archive to track the previous history of success and failure. It also updates the control parameters in an adaptive manner with generations. The DE/current-to-pbest strategy is a less greedy generalization of the DE/current-tobest/ strategy. Instead of only adopting the best individual in the DE/current-to-best/1 strategy, the current-to-pbest/1 strategy utilizes the information of other good solutions. Moreover, the recently explored inferior solutions are also incorporated in this strategy. The DE/current-to-pbest/1 without external archive generates the donor vector as \u0002V i,G = \u0002Xi,G + Fi ¡¤ (\u0002Xp best,G . \u0002Xi,G) + Fi ¡¤ (\u0002Xri1 ,G . \u0002Xri2 ,G) (30) where \u0002Xp best,G is randomly chosen as one of the top 100p% individuals of the current population with p ¡ô (0, 1]. Fi is the scale factor associated with the ith individual and it is updated dynamically in each generation. JADE can optionally make use of an external archive, which stores the recently explored inferior solutions. Let A denote the archive of inferior solutions and P denote the current population. Then DE/current-topbest/ 1 with external archive generates the donor vector as \u0002V i,G = \u0002Xi,G + Fi ¡¤ (\u0002Xp best,G . \u0002Xi,G) + Fi ¡¤ (\u0002Xri1 ,G . \u0002X \u000e ri2 ,G) (31) where \u0002Xi,G, \u0002Xp best,G, and \u0002Xri1 ,G are selected from P as before in (30), but \u0002X \u000e ri2 ,G is selected at random from the union P \u0014A, of the current population and archive. The archive operation is made very simple to avoid significant computation overhead. Initially the archive is empty. Then, after each generation, the parent solutions that fail in the selection process are added to the archive. If the archive size exceeds a certain threshold, then some solutions are randomly eliminated from the archive to keep the archive size fixed. H. Hybrid DE Algorithms Hybridisation, in context to metaheuristics, primarily refers to the process of combining the best features of two or more algorithms together, to form a new algorithm that is expected to outperform its ancestors over application-specific or general benchmark problems. Over the past few years, DE has been successfully hybridized with several other global optimization algorithms like PSO [S38], ant colony systems [S39], artificial immune systems (AIS) [S40], bacterial foraging optimization algorithm (BFOA) [S41], and simulated annealing (SA) [S42]. Also researchers attempted to embed different local search techniques in basic DE, to improve its exploitation abilities. In this section, we shall discuss the hybrid DE algorithms in two parts: first one will present the synergy between DE and other global search methods while the second one will review the blending of DE with local search algorithms. 1) Synergy of DE with Other Global Optimization Algorithms: The concept of particle swarms, although initially introduced for simulating human social behaviors, has become very popular these days as an efficient global search and optimization technique. The first synergy between DE and PSO was reported by Hendtlass, who proposed a combined swarm differential evolution algorithm [S43] serving as a hybrid optimizer based on PSO and DE. In this optimizer, particle positions are updated only if their offspring have better fitness. DE acts on the particles in the PSO swarm at specified intervals. Zang and Xie [119] proposed another popular hybrid algorithm called DEPSO, in which the original PSO algorithm and the DE operator alternate at the odd iterations and at the even iterations. DEPSO achieved better convergence results than both the original algorithms over certain constrained optimization problems. Das et al. [16] presented a tightly coupled synergy of PSO and DE, called particle swarm optimization with differentially perturbed velocity (PSO-DV). PSO-DV introduces a differential operator (borrowed from DE) in the velocity-update scheme of PSO. Further, unlike conventional PSO, a particle is actually shifted to a new location only if the new location yields a better fitness value, i.e., a DE-type selection strategy has been incorporated into the swarm dynamics. In [58], Moore and Venayagamoorthy proposed a new hybrid of DE and PSO, which is similar in spirit to the algorithm proposed in [119], but the DE and PSO in it are DE/rand/2/bin and a modified PSO with ¡°Ring¡± topology respectively. In [S44], Liu et al. proposed a similar DEPSO and used it to train artificial neural networks. Like the work reported in [119], the PSO in this hybrid optimizer is also based on Gbest model; however, the DE in it is DE/targetto- best/1/bin. In particular, this hybrid also adopts a chaotic local search to improve its local exploitation ability. In 2004, Kannan et al. [S45] proposed a distinctive DEPSO (named CPSO in [S45]). The DE algorithm in it is employed to select three control parameters on-line for PSO. In other words, DE serves as a meta-optimizer for the optimization of PSOs search behavior. Recently Hao et al. [S46] constructed a new hybrid optimizer, where DE and PSO are regarded as two operators to generate candidate solutions, and they act on the level of dimensional components of individuals. In [66], Omran et al. presented two hybrids of DE and PSO. The first DEPSO (named DEPSO-OES) is somewhat similar to the hybrid described in [S46]. The DE (DE/rand/1/bin) and PSO-cf (PSO with constriction factor) in it also alternate in a stochastic way, but both DE and PSO act on the level of a whole individual, that is to say, each individual at each generation has only one updating method (DE or PSO). Besides, the probability for controlling the selection of updating method and the scaling factor in DE are dynamic and adaptive. The second hybrid method combined the bare bones PSO proposed by Kennedy [S47] and DE in an embedding way. Xue et al. described another scheme of mixing DE operators with PSO in [S48]. DAS AND SUGANTHAN: DIFFERENTIAL EVOLUTION: A SURVEY OF THE STATE-OF-THE-ART 17 Das et al. [19] modified the selection mechanism of the classical DE family by using the concepts of SA such that the probability of accepting the inferior solutions may be dynamically altered with iterations. Biswas et al. [9] proposed a synergistic coupling of DE and BFOA. Foraging can be modeled as an optimization process where an animal seeks to maximize energy per unit time spent for foraging. BFOA emulates the foraging behavior of a group of Escherichia coli bacteria living in our intestine. In [9], the computational chemotaxis step of BFOA, which may also be viewed as a stochastic gradient search, has been coupled with DE type mutation and crossing over of the optimization agents leading to the new hybrid algorithm called chemotactic differential evolution (CDE). In [S49] and [S50], hybridizations of the DE with an ant colony optimizer are proposed. In [S51], He and Han propose a hybrid binary DE based on AIS for tackling discrete optimization problems. Kaelo and Ali [33] use the attraction-repulsion concept of electromagnetism-like algorithm to boost the mutation operation of the original DE. 2) Synergy of DE with Local Search Methods: Local search algorithms primarily explore a small neighborhood of a candidate solution in the search space until a locally optimal point is found or a time bound is elapsed. Noman and Iba [64] proposed a crossover based adaptive local search (LS) operation to improve the performance of the classical DE. Typically in LS, every candidate solution has more than one neighbour solution; the choice of which one to move to is taken using only information about the solutions in the neighbourhood of the current one, hence the name local search. If the choice of the neighbouring solution is done by taking the one locally maximizing the criterion, the metaheuristic takes the name hillclimbing. The authors in [64] proposed an LS, whose length of the search can be adjusted adaptively using a hill-climbing heuristic. The incorporation of a crossover-based local search (XLS) with adaptive length (adaptive length XLS, shortened as AHCXLS) in DE resulted into a DE-variant called by the authors: DEahcSPX, where SPX is the simplex-based crossover scheme proposed by Tsutsui et al. for real-coded GAs [S52]. The experimental results reported by Noman and Iba [64] indicated that DEahcSPX could outperform the classical DE (DE/rand/1/bin) in terms of convergence speed over a set of carefully chosen numerical benchmarks [95]. The overall performance of the adaptive LS scheme was reportedly better than the other crossover-based LS strategies and the overall performance of the newly proposed DE algorithm was shown to be superior to or at least comparable with some other memetic algorithms (MAs) [S53] selected from literature. Yang et al. [108] proposed a hybridization of DE with the neighborhood search, which appears as a main strategy underpinning EP [S54]. The resulting algorithm, known as NSDE, performs mutation by adding a normally distributed random value to each target-vector component in the following way: \u0002V i,G = \u0002Xri1 ,G + \u0015 \u0002di,G ¡¤ N(0.5, 0.5) if randi(0, 1) < 0.5 \u0002di,G ¡¤ ¥ä otherwise (32) where \u0002di,G = \u0002Xri2 ,G . \u0002Xri3 ,G is the usual difference vector and ¥ä denotes a Cauchy random variable with scale parameter t = 1. Recently Yang et al. [110] used a Self-adaptive NSDE in the cooperative coevolution framework that is capable of optimizing large-scale non-separable problems (up to 1000 dimensions). They proposed a random grouping scheme and adaptive weighting for problem decomposition and coevolution. Somewhat similar in spirit to this paper is the study by Yang et al. [S55] on self-adaptive DE with neighborhood search (SaNSDE). SaNSDE incorporates self-adaptation ideas from the Qin et al¡¯s SaDE [76] and proposes three self-adaptive strategies: self-adaptive choice of the mutation strategy between two alternatives, self-adaptation of the scale factor F, and self-adaptation of the crossover rate Cr. In contrast to Yang et al.¡¯s works on NSDE and SaNSDE, in the topological neighborhood-based mutation scheme proposed in [21], the authors keep the scale factor non-random and use a ring-shaped neighborhood topology (inspired by PSO [S56]), defined on the index graph of the parameter vectors, to derive a local neighborhood-based mutation model. Also instead of F and Cr, the weight factor that unifies two kinds of mutation models, have been made self-adaptive in one of the variants of the algorithms described in [21]. MAs represent one of the recent growing areas of research in evolutionary computation. The term MA is now widely used to denote a synergy of evolutionary or any populationbased approach with separate individual learning or local improvement procedures for problem search. Neri and Tirronen [59] proposed a DE-based MA, which employs within a self-adaptive scheme, two local search algorithms. The algorithm was referred by authors as the scale factor local search differential evolution. These local search algorithms aim at detecting a value of the scale factor F corresponding to an offspring with a higher fitness, while the generation is executed. The local search algorithms thus assist in the global search and generate offspring with a higher fitness, which are subsequently supposed to promote the generation of enhanced solutions within the evolutionary framework. In [S57], Tirronen et al. proposed a DE-based MA employing three local search algorithms coordinated by means of fitness diversity adaptation and a probabilistic scheme for designing digital filters, which aim at detecting defects of the paper produced during an industrial process. In [14], Caponio et al. incorporated PSO and two other LS algorithms (Nelder mead algorithm and Rosenbrock algorithm) in the framework of DE. The main idea is that initially PSO should quickly improve a solution having poor fitness and include it in the DE population. This solution (called by the authors as ¡°super-fit individual¡±) should therefore be the one leading the DE-search. The two local searchers are invoked within the main DE-search probabilistically. Although the paper reports improvement of the gross performance of DE, role of the LS algorithms are not much clear. 3) DE-Variants for Discrete and Binary Optimization: Although DE was devised mainly for real parameter optimization, over the years researchers have tried to modify it for tackling binary and discrete optimization problems as well. In the early days of DE research, Lampinen and Zelinka first focused in this direction through their conference article in MENDEL¡¯99 [40]. For handling of integer variables, they 18 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 15, NO. 1, FEBRUARY 2011 recommended truncating the parameter values for objective function evaluation such that the population of DE still works with floating-point values. They pointed out that although such truncation changes the effective objective function landscape from DE¡¯s point of view by introducing flat areas to the fitness landscape, DE¡¯s self-adaptive reproduction scheme is well able to move across to those flat areas. In the same paper, Lampinen and Zelinka also came up with a straightforward approach for optimizing discrete parameters that are limited to a set of standard values. For example, the thickness of a steel plate, the diameter of a copper pipe, the size of a screw, the size of a roller bearing, and so on, are often limited to a set of commercially available standard sizes. A discrete value is optimized indirectly so that DE actually works on an integer value (index) that points to the actual discrete value. First, the discrete set of available values is arranged to an ascending sequence, and then an index is assigned to refer each available value. DE works with these indices by optimizing the index like any integer variable. However, for objective function evaluation the actual discrete value pointed by the index is used. In [S212], Tasgetiren et al. presented a DE algorithm to solve the permutation flowshop scheduling problem with the makespan criterion. DE was a traditional continuous algorithm and the smallest position value rule was presented to convert the continuous vector to a discrete job permutation. In [S213], Onwubolu and Davendra presented a DE variant for solving scheduling problems. In [S58], Tasgetiren et al. proposed a discrete differential evolution algorithm (DDE) for the no-wait flowshop scheduling problem with total flow time criterion. In the DDE they proposed, a discrete version of DE based on a insert mutation and PTL crossover operator they offered are employed. In order to further improve the solution quality, a variable neighborhood descent local search is embedded in the DDE algorithm. A DDE algorithm was presented by Tasgetiren et al. [S214] for the total earliness and tardiness penalties with a common due date on a single-machine. In [S214], the same mutation and the PTL crossover operator were used in the binary context as well as a Bswap local search is employed to further improve the solution quality. A similar approach but working on a continuous domain was presented in Nearchou [S215] to solve the total earliness and tardiness penalties with a common due date on a single-machine. In [S215], the conversion of continuous vector was based on the fact that a value less than or equal to 0.5 in the string indicates that the corresponding job is early, otherwise the job is late. In [S216], Al-Anzi and Allahverdi proposed a selfadaptive differential evolution heuristic for two-stage assembly scheduling problem to minimize maximum lateness with setup times. Later, Pan et al. [217] presented a DDE based on the one in [S58] to solve the permutation flowshop scheduling problem. Furthermore, Qian et al. [S218] proposed another DE-based approach to solve the no-wait flowshop scheduling problem. Tasgetiren et al. [S59] developed a DDE for the single machine total weighted tardiness problem with sequence dependent setup times where novel speed-up methods were presented. In [S219], Pan et al. developed a novel differential evolution algorithm for bi-criteria no-wait flow shop scheduling problems. Wang et al. [220] proposed a hybrid discrete differential evolution algorithm for blocking flow shop scheduling problems. Another bi-criteria DE was presented by Qian et al. in [S221] to solve the multiobjective flow shop scheduling with limited buffers. In [S222], Tasgetiren et al. proposed an ensemble of discrete differential evolution algorithms for solving the generalized traveling salesman problem. The novelty in [S222] stems from the fact that the ensemble of destruction and construction procedures of iterated greedy algorithm and crossover operators are achieved in parallel populations. In addition, Damak et al. presented [S223] a DE variant for solving multimode resource-constrained project scheduling problems. In [S224] and [S225], DDE was applied to solve the no-idle permutation flow shop scheduling problems. Additional discrete and combinatorial applications of DE algorithms were presented in detail in [S226] and [S227]. Recently, Pampara et al. [67] proposed a new DE variant that can operate in binary problem spaces without deviating from the basic search mechanism of the classical DE. The algorithm was named by its authors as the angle modulated DE as it employs a trigonometric function as a bit string generator. The trigonometric generating function used in the angle modulation function is a composite sinusoidal function, which may be given as g(x) = sin(2¥ð(x . a) ¡¿ b ¡¿ cosA)) + d (33) where A = 2¥ð ¡¿ c ¡¿ (x . a) and x is a single element from a set of evenly separated intervals determined by the required number of bits that need to be generated. The DE is used to evolve the coefficients to the trigonometric function (a, b, c, d), thereby allowing a mapping from continuous-space to binary-space. Instead of evolving the higher-dimensional binary solution directly, angle modulation is used together with DE to reduce the complexity of the problem into a 4-D continuous-valued problem. Yuan et al. [S60] used a discrete binary differential evolution approach to solve the unit commitment problem. I. Parallel DE Exploiting the huge development of computational resources (both software and hardware), parallel computing has emerged as a form of high-performance computation, where many calculations are carried out simultaneously, based on the principle that large problems can often be divided into smaller ones, which are then solved concurrently (in parallel). Like other EAs, DE can also be parallelized (mainly for improving its speed and accuracy on expensive optimization problems) owing to the fact that each member of the population is evaluated independently. The only phase in the algorithm that necessitates communication with other individuals is reproduction. This phase can also be made parallel for pair of vectors. The first attempt to distribute DE across a cluster of computers (connected through local area networks) was made by Lampinen [38]. In his method, the whole population is kept in a master processor that selects individuals for mating and sends them to slave processors for performing other operations. Lampinen¡¯s parallelization scheme could also overcome DAS AND SUGANTHAN: DIFFERENTIAL EVOLUTION: A SURVEY OF THE STATE-OF-THE-ART 19 the drawbacks due to the heterogeneous speed of the slave processors. Tasoulis et al. [101] proposed a parallel DE scheme that maps an entire subpopulation to a processor, allowing different subpopulations to evolve independently toward a solution. To promote information sharing, the best individual of each subpopulation is allowed to move to other subpopulations according to a predefined topology. This operation is known as migration in parallel EA literature [S194] and island model GAs. During migration, instead of replacing a randomly chosen individual from a subpopulation, Kozlov and Samsonov [S195] suggested to replace the oldest member by the best member of another subpopulation in the topological neighborhood of the former subpopulation. Following the work of [101], Weber et al. [S196] proposed a scale factor (F) inheritance mechanism in conjunction with distributed DE with ring topology based migration scheme. In this framework, each sub-population is characterized by its own scale factor value. With a probabilistic criterion, that individual displaying the best performance is migrated to the neighbor population and replaces a randomly selected individual of the target subpopulation. The target sub-population inherits not only this individual but also the scale factor if it seems promising at the current stage of evolution. V. DE in Complex Environments This section reviews the extensions of DE for handling multiobjective, constrained, and large scale optimization problems. It also surveys the modifications of DE for optimization in dynamic and uncertain environments. A. DE for Multiobjective Optimization Due to the multiple criteria nature of most real-world problems, multiobjective optimization (MO) problems are ubiquitous, particularly throughout engineering applications. As the name indicates, multiobjective optimization problems involve multiple objectives, which should be optimized simultaneously and that often are in conflict with each other. This results in a group of alternative solutions, which must be considered equivalent in the absence of information concerning the relevance of the others. The concepts of dominance and Paretooptimality may be presented more formally in the following way. Definition 2: Consider without loss of generality the following multiobjective optimization problem with D decision variables x (parameters) and n objectives y: Minimize \u0002Y = f (\u0002X) = (f1(x1, ...., xD), ...., fn(x1, ...., xD)) (34) where \u0002X = [x1, ....., xD]T ¡ô P and \u0002Y = [y1, ...., yn]T ¡ô O and where \u0002X is called decision (parameter) vector, P is the parameter space, \u0002Y is the objective vector, and O is the objective space. A decision vector \u0002A ¡ô P is said to dominate another decision vector \u0002B ¡ô P (also written as \u0002A . \u0002B for minimization) if and only if ¢£i ¡ô {1, ...., n} : fi(\u0002A) ¡Â fi(\u0002B) ¡ü ¢¤j ¡ô {1, ....., n} : fj(\u0002A) < fj(\u0002B). (35) Based on this convention, we can define non-dominated, Pareto-optimal solutions as follows. Definition 3: Let \u0002A ¡ô P be an arbitrary decision vector. 1) The decision vector \u0002A is said to be non-dominated regarding the set P \u000e ¡ö P if and only if there is no vector in P \u000e which can dominate \u0002A. 2) The decision (parameter) vector \u0002A is called Paretooptimal if and only if \u0002A is non-dominated regarding the whole parameter space P. Many evolutionary algorithms were formulated by the researchers to tackle multiobjective problems in recent past [S61], [S62]. Apparently, the first paper that extends DE for handling MO problems is by Chang et al. [S63] and it bases itself on the idea of Pareto dominance. DE/rand/1/bin with an external archive (called ¡°Pareto optimal set¡± by the authors and also known as the current non-dominated set) is used to store the non-dominated solutions obtained during the search. The approach also incorporates fitness sharing to maintain diversity. Abbas and Sarkar presented the Pareto differential evolution (PDE) algorithm [2] for MO problems with continuous variables and achieved very competitive results compared to other evolution algorithms in MO literature. However, there is no obvious way to select best crossover and mutation rates apart from running the algorithm with different rates. It handles only one (main) population. Reproduction is undertaken only among non-dominated solutions, and offspring are placed into the population if they dominate the main parent. A distance metric relationship is used to maintain diversity. In [S64], Abbass presented an approach called Memetic Pareto artificial neural networks. This approach consists of PDE enhanced with the back-propagation local search algorithm, in order to speed up convergence. Kukkonen and Lampinen extended DE/rand/1/bin to solve multiobjective optimization problems in their approach called generalized differential evolution (GDE). In the first version of their approach [S65], the authors modified the original DE selection operation by introducing Pareto dominance as a selection criterion while in a second version, called GDE2 [S66] a crowding distance measure was used to select the best solution. To deal with the shortcomings of GDE2 regarding slow convergence, Kukkonen and Lampinen proposed an improved version called GDE3 [35] (a combination of the earlier GDE versions and the Pareto-based differential evolution algorithm [S67]). This version added a growing population size and nondominated sorting (as in the NSGAII [S68]) to improve the distribution of solutions in the final Pareto front and to decrease the sensitivity of the approach to its initial parameters. Santana-Quintero and Coello Coello proposed the ¡ô-MyDE in [86]. This approach keeps two populations: the main population (which is used to select the parents) and a secondary (external) population, in which the concept of ¡ô-dominance [S69] is adopted to retain the nondominated solutions found and to distribute them in a uniform way. In [105], Xue et al. came up with the multiobjective DE (MODE) in which the best individual is adopted to create the offspring. A Pareto-based approach is introduced to implement 20 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 15, NO. 1, FEBRUARY 2011 the selection of the best individual. If a solution is dominated, a set of non-dominated individuals can be identified and the ¡°best¡± turns out to be any individual (randomly picked) from this set. Also, the authors adopt (¥ì+¥ë) selection, Pareto ranking and crowding distance in order to produce and maintain well-distributed solutions. Robic and Filipic presented a DE for multiobjective optimization (called DEMO) in [83]. This algorithm combines the advantages of DE with the mechanisms of Pareto-based ranking and crowding distance sorting. DEMO only maintains one population and it is extended when newly created candidates take part immediately in the creation of the subsequent candidates. This enables a fast convergence toward the true Pareto front, while the use of non-dominated sorting and crowding distance (derived from the NSGA-II [S68]) of the extended population promotes the uniform spread of solutions. Iorio and Li [32] proposed the non-dominated sorting DE (NSDE), which is a simple modification of the NSGA-II [S68]. The only difference between this approach and the NSGA-II is in the method for generating new individuals. The NSGA-II uses a real-coded crossover and mutation operator, but in the NSDE, these operators were replaced with the operators of differential evolution. NSDE was shown to outperform NSGA-II on set of rotated MO problems with strong interdependence of variables. Some researchers have proposed approaches that use non- Pareto based multiobjective concepts like combination of functions, problem transformation, and so on. For example, Babu and Jehan [6] proposed a DE algorithm for MO problems, which uses the DE/rand/1/bin variant with two different mechanisms to solve bi-objective problems: first, incorporating one objective function as a constraint, and secondly using an aggregating function. Li and Zhang [S70], [46] proposed a multiobjective differential evolution algorithm based on decomposition (MOEA/D-DE) for continuous multiobjective optimization problems with variable linkages. The DE/rand/1/bin scheme is used for generating new trial solutions, and a neighborhood relationship among all the sub-problems generated is defined, such that they all have similar optimal solutions. In [46], they introduce a general class of continuous MO problems with complicated Pareto set (PS) shapes and reported the superiority of MOEA/D-DE over NSGA-II with DE type reproduction operators. Summation of normalized objective values with diversified selection approach was used in [79] without the need for performing non-dominated sorting. Some authors also consider approaches where a set of schemes have been mixed in the DE-based multiobjective algorithm. Examples of such combined techniques are the vector evaluated DE [70] by Parsopoulos et al and the work of Landa- Becerra and Coello Coello [42] where they hybridized the ¥å-constraint technique [S71] with a single-objective evolutionary optimizer: the cultured DE [43]. Recently the concept of self-adaptive DE has been extended to handle MO problems in [29], [30], and [116]. B. DE for Constrained Optimization Most of the real world optimization problems involve finding a solution that not only is optimal, but also satisfies one or more constraints. A general formulation for constrained optimization may be given in the following way. Definition 4: Find \u0002X = [x1, x2, ..., xD]T \u0002X ¡ô \u0004D to minimize: f (\u0002X) (36a) subjected to inequality constraints: gi(\u0002X) ¡Â 0 i = 1, 2, ...,K (36b) equality constraints: hj(\u0002X) = 0 j = 1, 2, ...,N (36c) and boundary constraints: xj,min ¡Â xj ¡Â xj,max. (36d) Boundary constraints are very common in real-world applications, often because parameters are related to physical components or measures that have natural bounds, e.g., the resistance of a wire or the mass of an object can never be negative. In order to tackle boundary constraints, penalty methods drive solutions from restricted areas through the action of an objective function-based criterion. DE uses the following four kinds of penalty method to handle boundary constraint violation. 1) Brick wall penalty [74]: if any parameter of a vector falls beyond the pre-defined lower or upper bounds, objective function value of the vector is made high enough (by a fixed big number) to guarantee that it never gets selected. 2) Adaptive penalty [90], [91]: similar to brick wall penalty, but here the increase in the objective function value of the offender vector may depend on the number of parameters violating bound constraints and their magnitudes of violation. 3) Random reinitialization [40], [74]: replaces a parameter that exceeds its bounds by a randomly chosen value from within the allowed range following (1). 4) Bounce-back [74]: relocates the parameter in between the bound it exceeded and the corresponding parameter from the base vector. The first known extension of DE toward the handling of inequality constrained optimization problems (mainly design centering) was by R. Storn [93]. He proposed a multimember DE (called CADE: constraint adaptation with DE, in his paper) that generates M (M > 1) children for each individual with three randomly selected distinct individuals in the current generation, and then only one of the M + 1 individuals will survive in the next generation. Mezura-Montes et al. [S72] used the concept also to solve constrained optimization problems. Zhang et al. [117] mixed the dynamic stochastic ranking with the multimember DE framework and obtained promising performance on the 22 benchmarks taken from the CEC 2006 competition on constrained optimization [47]. Lampinen applied DE to tackle constrained problems [39] by using Pareto dominance in the constraints space. Mezura- Montes et al. [S73] proposed to add Deb¡¯s feasibility rules [S74] into DE to deal with constraints. Kukkonen and Lampinen [36] presented a generalised DE-based approach to solve constrained multiobjective optimization problems. DAS AND SUGANTHAN: DIFFERENTIAL EVOLUTION: A SURVEY OF THE STATE-OF-THE-ART 21 Zielinsky and Laur [121] also used Deb¡¯s rules [S74] with DE to solve some constrained optimization problems. Some researchers have also tried hybrid approaches such as the DE with gradient-based mutation (where gradients were derived from the constraint equations) by Takahama and Sakai [97] and PSO-DE (PESO+) by Mu.noz-Zavala et al. [S75]. The ¥å- DE algorithm of Takahama and Sakai [97] uses a dynamic control of the allowable constraint violation specified by the ¥å-level and it achieved the first rank in the CEC 2006 competition on the constrained real-parameter optimization [47]. Tasgetiren and Suganthan presented a multi-populated DE algorithm [99] to solve real-parameter constrained optimization problems. They employed the notion of a near feasibility threshold in the proposed algorithm to penalize infeasible solutions. Mezura-Montes et al. [57] proposed a DE approach that attempts to increase the probability of each parent to generate a better offspring. This is done by allowing each solution to generate more than one offspring but using a different mutation operator, which combines information of the best solution in the population and also information of the current parent to find new search directions. On the other hand, some studies have also been reported regarding parameter control in DE for constrained optimization. Brest et al. [S76] have proposed an adaptive parameter control for two DE parameters related to the crossover and mutation operators. Huang et al. [28] used an adaptive mechanism to select among a set of DE variants to be used for the generation of new vectors based on a success measure. Moreover, they also adapted some DE parameters to control the variation operators. Very recently Mezura-Montes and Palomeque-Ortiz [S77] presented the adaptive parameter control in the diversity differential evolution (DDE) [S72] algorithm for constrained optimization. Three parameters namely the scale factor F, the crossover rate Cr, and the number of offspring generated by each target vector NO, are self-adapted by encoding them within each individual and a fourth parameter called selection ratio Sr is controlled by a deterministic approach. Huang et al. [31] presented a cooperative-coevolutionary approach in conjunction with DE for constrained optimization problems. In their algorithm first, a special penalty function is designed to handle the constraints. Second, a co-evolution model is presented and DE is employed to perform evolutionary search in spaces of both solutions and penalty factors. Thus, the solutions and penalty factors evolve interactively and self-adaptively, and both the satisfactory solutions and suitable penalty factors can be obtained simultaneously. Recently Ali and Kajee-Bagdadi proposed a local exploration-based DE [4] for constrained global optimization. They used a restricted version of the pattern search (PS) method [S78] as their local technique. Constraint handling methods such as the superiority of feasible points and the parameter free penalty are also employed. Recently Santana-Quintero et al. [87] extended the PDE [1], [2] to handle constrained MO problems by using a two-stage hybrid DE approach where in the first one, an MO version of DE is used to generate an initial approximation of the Pareto front. Then, in the second stage, rough set theory is used to improve the spread and quality of this initial approximation. C. DE for Large-Scale Optimization In the past two decades, several kinds of nature-inspired optimization algorithms have been designed and applied to solve optimization problems. Although these approaches have shown excellent search abilities when applied to some 30.100 dimensional problems, usually their performance deteriorates quickly as the dimensionality of search space increases beyond 500. The reasons appear to be two-fold. First, complexity of the problem usually increases with the size of problem, and a previously successful search strategy may no longer be capable of finding the optimal solution. Second, the solution space of the problem increases exponentially with the problem size, and a more efficient search strategy is required to explore all the promising regions in a given time budget. Since the performance of basic DE schemes also degrade with massive increase in problem dimensions, some important attempts have been made by the researchers to make DE suitable for handling such large-scale optimization problems. In [62], Noman and Iba proposed fittest individual refinement (FIR), a crossover based local search method for DE, such that the FIR scheme accelerates DE by enhancing its search capability through exploration of the neighborhood of the best solution in successive generations. The proposed memetic version of DE (augmented by FIR) was shown to obtain an acceptable solution with a lower number of evaluations particularly for higher dimensional functions. Another memetic DE for high-dimensional optimization was presented by Gao and Wang [27], where the stochastic properties of chaotic system is used to spread the individuals in search spaces as much as possible and the simplex search method is employed to speed up the local exploiting and the DE operators help the algorithm to jump to a better point. In terms of optimizing high-dimensional problems, cooperative co-evolution (first proposed by Potter and De Jong for GAs [S79]) with the following divide-and-conquer strategy has proven an effective choice. 1) Problem decomposition: splitting the object vectors into some smaller subcomponents. 2) Optimize subcomponents: evolve each subcomponent with a certain optimizer separately. 3) Cooperative combination: combine all subcomponents to form the whole system. In [109], the authors proposed two DE-variants (DECCI and DECC-II) that use self-adaptive NSDE (SaNSDE) (a synergy of the works reported in [108] and [76]) in a cooperative co-evolutionary framework with novel strategies for problem decomposition and subcomponents¡¯ cooperation. The algorithms were tested on a set of widely used benchmarks scaled up to 500 and 1000 dimensions. An important extension of the same work for better performance on rotated and nonseparable high-dimensional functions has been reported in [110] where the authors use random grouping scheme with adaptive weighting for problem decomposition and coevolution. Some theoretical analysis is also presented in this paper to show why and how the new framework can be effective for optimizing large non-separable problems. The theoretical analysis illustrates how such strategies can help to 22 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 15, NO. 1, FEBRUARY 2011 capture variable interdependencies in non-separable problems. Recently, Parsopoulos [S80] devised a cooperative micro- DE, which employs small cooperative subpopulations (with only few individuals) to detect subcomponents of the original problem¡¯s solution concurrently. The subcomponents are combined through cooperation of subpopulations to build complete solutions of the problem. Zamuda et al. [S81] proposed a DE-variant for large scale global optimization, where original DE is extended by log-normal self-adaptation of its control parameters and combined with cooperative co-evolution as a dimension decomposition mechanism. Among the other approaches, Su presented a surrogateassisted DE framework based on Gaussian process for solving large-scale computationally expensive problems in [S82]. Brest et al. [12] investigated a self-adaptive DE (abbreviated as jDEdynNP-F) where control parameters F and Cr are self-adapted and a population-size reduction method is used. The proposed jDEdynNP-F algorithm also employs a mechanism for sign changing of F with some probability based on the fitness values of randomly chosen vectors, which are multiplied by F in the mutation step of DE. The algorithm achieved third rank in CEC 2008 special session and competition on high-dimensional real-parameter optimization [98] that included non-separable functions like Schwefel¡¯s problem 2.21, Griewank¡¯s function, and fastfractal ¡°doubledip¡± function. D. DE for Optimization in Dynamic and Uncertain Environments In many real world applications, EAs often have to deal with optimization problems in the presence of a wide range of uncertainties. In general, there are four ways in which uncertainty may creep into the computing environment [S22]. First, the fitness function may be noisy. Second, the design variables and/or the environmental parameters may change after optimization, and the quality of the obtained optimal solution should be robust against environmental changes or deviations from the optimal point. Third, the fitness function may be approximated, which means that the fitness function suffers from approximation errors. Finally, the optimum of the problem to be solved changes its location over time and, thus, the optimizer should be able to track the optimum continuously. In all these cases, the EAs should be equipped with additional measures so that they are still able to work satisfactorily. For a noisy problem, a deterministic choice of the scale factor and the greedy selection methods can be inadequate and a standard DE can easily fail at handling a noisy fitness function, as experimentally shown in [34]. Looking at the problem from a different perspective, the DE employs too much deterministic search logic for a noisy environment and therefore tends to stagnate. Das et al. [18] made an attempt to improve the performance of DE on noisy functions by first varying the scale factor randomly between 0.5 and 1 and secondly by incorporating two not-so-greedy selection mechanisms (threshold based selection and stochastic selection) in DE. Liu et al. [49] combined the advantages of the DE algorithm, the optimal computing budget allocation technique and simulated annealing (SA) algorithm to devise a robust hybrid DE method abbreviated as DEOSA) that can work well in noisy environments. Mendes and Mohais presented DynDE [54].a multipopulation DE algorithm, developed specifically to optimize slowly time-varying objective functions. DynDE does not need any parameter control strategy for the F or Cr. The main components in DynDE are as follows. 1) Usage of several populations in parallel. 2) Usage of uniform dither for F? [0, 1] as well as Cr ¡ô [0, 1]. 3) To maintain diversity of the population based on two approaches. a) Reinitialization of a population if the best individual of a population gets too close to the best individual of another population. The population with the absolute best individual is kept while the other one is reinitialized. This way the various populations are prevented from merging. b) Randomization of one or more population vectors by adding a random deviation to the components. Experimentally, the authors show that this new algorithm is capable of efficiently solving the moving peaks benchmark described by Branke [S83]. Very recently Brest et al. [13] investigated a self-adaptive DE algorithm (jDE) where F and Cr control parameters are self-adapted and a multi-population method with aging mechanism is used to handle dynamic fitness landscapes. This algorithm achieved the first rank in the Competition on ¡°Evolutionary Computation in Dynamic and Uncertain Environments¡± in CEC2009 [45]. Angira and Santosh [5] presented a trigonometric differential evolution algorithm based on Fan and Lampinen¡¯s trigonometric mutation scheme [25] for solving dynamic optimization problems encountered in chemical engineering. E. DE for Multimodal Optimization and Niching Many practical objective functions are highly multimodal and likely to have several high quality global and/or local solutions. Often, it is desirable to identify as many of these solutions as possible so that the most appropriate solution can be chosen. In order to identify many solutions of a multimodal optimization problem, several ¡°niching¡± techniques have been developed. A niching method empowers an EA to maintain multiple groups within a single population in order to locate different optima. The niching techniques include crowding [103], fitness sharing [S203], [S209], clearing [S207], restricted tournament selection [81], [S204], and speciation [S205], [S208]. The crowding method [S206] allows competition for limited resources among similar individuals, i.e., within each niche. Generally, the similarity is measured using distance between individuals. The method compares an offspring with some randomly sampled individuals from the current population. The most similar individual will be replaced if the offspring is superior. Thomsen extended DE with a crowding scheme named as crowding DE (CDE) [103] to solve multimodal optimization problems. In CDE, when an offspring is generated, it will only compete with the most similar (measured by DAS AND SUGANTHAN: DIFFERENTIAL EVOLUTION: A SURVEY OF THE STATE-OF-THE-ART 23 Euclidean distance) individual in the population. The offspring will replace this individual if it has a better fitness value. The fitness sharing method [S203], [S209] divides the population into different subgroups according to parameter space similarity of the individuals. An individual must share its information with other individuals within the same niche. The shared fitness for ith individual can be represented by using the following equation: fshared (i) = foriginal(i) N \u0005 j=1 sh(dij) (37) where the sharing function is calculated as sh(dij) ...  dij ¥òshare \u000e¥á if dij < ¥òshare 0 otherwise and dij is the distance between individuals i and j, ¥òshare is the sharing radius, N is the population size and ¥á is a constant called sharing level. Thomsen integrated the fitness sharing concept with DE to form the sharing DE [103]. The restricted tournament selection (RTS) method uses tournament selection for multimodal optimization [S204]. The algorithm chooses a random sample of w (window size) individuals from the population and determines the nearest one to the offspring, by using either Euclidean (for real variables) or Hamming (for binary variables) distance measure. The nearest member within the w individuals will compete with the offspring and the one with higher fitness will survive in the next generation. The RTS was implemented with an ensemble of two different window sizes in [81] using the DE as the search method. VI. Analytical Studies on DE Theoretical and empirical analyses of the properties of evolutionary algorithms are very important to understand their search behaviors and to develop more efficient algorithms [S84]. Compared to the plethora of works concerning the empirical study of parameter selection and tuning process in DE, not much research has so far been devoted to theoretically analyze the search mechanism and convergence properties of DE and this area remains largely open to prospective future research. Below, we discuss some of the analytical results so far obtained on DE. A. Population Variance and Explorative Power of DE Some significant theoretical results on DE were first reported in [111] and then extended in [112] and [113] by Zaharie, where she theoretically analyzed the influence of the variation operators (mutation and recombination) and their parameters on the expected population variance. In [111], Zaharie showed that the expected population variance (after applying mutation and recombination, but without selection) of DE is greater than that of the ES algorithm analyzed in [S85]. This finding could explain to some extent the excellent performance of DE on certain test functions. In [114], Zaharie analyzed the impact on the expected population mean and variance of several variants of mutation and crossover operators used in DE algorithms. As a consequence of this analysis, she proposed a simple variance-based mutation operator without using differences but has the same impact on the population variance as classical DE operators proposed. She also presented a preliminary analysis of the distribution probability of the population in the case of a DE algorithm with binary encoding. B. Role of Crossover in DE Very recently in [115], the influence of the crossover rate on the distribution of the number of mutated components and on the probability for a component to be taken from the mutant vector (mutation probability) is theoretically analyzed for several variants of crossover, including classical binomial and exponential strategies in DE. For each crossover variant the relationship between the crossover rate and the mutation probability is identified and its impact on the choice and adaptation of control parameters is analyzed both theoretically and numerically. With numerical experiments, the author illustrates the fact that the difference between binomial and exponential crossover variants is mainly due to different distributions of the number of mutated components. On the other hand, the behavior of exponential crossover variants was found to be more sensitive to the problem size than the behavior of variants based on binomial crossover. C. Evolutionary Search-Dynamics in DE The first theoretical studies on the evolutionary searchdynamics of DE were carried out by Dasgupta et al. in [22] and [23]. The authors proposed a simple mathematical model of the underlying evolutionary dynamics of a 1-D DE-population (evolving with the DE/rand/1/bin algorithm) [22]. The model was based on the fact that DE perturbs each dimension separately and if a D-dimensional objective function is separable, this function can be optimized in a sequence of D 1-D optimization processes. The model reveals that the fundamental dynamics of each search-agent (1-D parameter vector) in DE employs the gradient-descent type search strategy (although it uses no analytical expression for the gradient itself), with a learning rate parameter that depends on control parameters like scale factor F and crossover rate Cr of DE. It is due to the gradient descent type search strategy, that DE converges much faster than some variants of GA or PSO over uni-modal benchmarks [104]. The stability and convergence-behavior of the proposed dynamics was analyzed in the light of Lyapunov¡¯s stability theorems very near to the isolated equilibrium points during the final stages of the search in [23] and the rate of convergence on smooth uni-modal functions were found to largely depend on Cr. However, the analysis undertaken in [22] and [23] appeared to be of very limited scope from a practical point of view, as the authors considered a 1-D fitness landscape. Generalizing it for multi-dimensional search space can be a challenging future research work. Also proving the probabilistic convergence of DE on even very simple objective functions is still an open problem for the theorists working with EAs. 24 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 15, NO. 1, FEBRUARY 2011 D. Timing Complexity of DE Runtime-complexity analysis of the population-based stochastic search techniques like DE is a critical issue by its own right. In [120], Zielinski et al. first investigated the runtime complexity of DE for various stopping criteria, both theoretically and empirically. The authors [120] pointed out that in each generation of DE a loop over NP is conducted, containing a loop over D. Since the mutation and crossover operations are performed at the component level for each DE vector, the number of fundamental operations in DE/rand/1/bin is proportional to the total number of loops conducted until the termination of the algorithm. Thus, if the algorithm is stopped after a fixed number of generations Gmax, then the runtime complexity is O(NP ¡¤D¡¤Gmax). Moreover the authors also inferred that maximum distance criterion MaxDist yields best overall performance for the DE algorithm. Note that this criteria stops the execution of the algorithms if the maximum distance from every vector to the best population member is below a given threshold m (say). E. Convergence of Multiobjective DE Recently, Xue et al. [106] performed the mathematical modeling and convergence analysis of continuous multiobjective differential evolution under certain simplifying assumptions. The authors investigated the population evolution of the MODE with only reproduction operators, i.e., the differential mutation and crossover and assuming that that the DE-population is initialized by sampling from a Gaussian distribution with given mean and standard deviation. Using simple mathematics, they prove that the initial population P0 is Gaussian distributed and contains the Pareto optimal set \u000e ., the subsequent populations generated by the MODE without selection are also Gaussian distributed and the population mean converges to the center of the Pareto optimal set \u000e ., i.e., if \u0002XG be a solution vector belonging to the population PG at generation G, then lim G¡æ¡Ä E(\u0002XG) = \u0002X . (38) where \u0002X . is a random solution uniformly distributed on the probability support defined by \u000e .. The works were extended in [107] by modeling a discrete version of MODE, D-MODE, in the framework of Markov processes and the corresponding convergence properties were developed. However, in most practical situations with finite population size, the optimal solutions will not be present in the initial population. The exploration of MODE would identify the global optimal solution during the evolutionary process and the selection operator would keep those optimal solutions found in the evolution. Mathematical analysis of convergence under such situations is yet to be developed. VII. Engineering Applications of DE Due to the rapidly growing popularity of DE as a simple and robust optimizer, researchers from several domains of science and engineering have been applying DE to solve optimization problems arising in their own fields. The literature on engineering applications of DE is huge and multifaceted. An internet search reveals that the number of DE research articles indexed in SCI database over the span of 2007. July 2009 is 3964 and out of these, there are more than thousands of application papers in diverse areas. For the sake of space economy, in Tables I and II we summarize only the major applications, where DE has been employed to solve the optimization problem, along with the type of the DE used, and the major publications associated with the application. A keen observation of Tables I and II reveals that practitioners mostly prefer to use the classical DE schemes like DE/rand/1/bin, DE/target-to-best/1/bin, and so on for solving domain-specific problems. More research is necessary in order to investigate the applicability of the most state-of-the-art DE-variants (like SaDE [76], DEGL [21], and ODE [82]) outlined in Section IV for obtaining improved performances on practical problems. Specific applications may bear some properties that make it worthwhile revisiting or extending DE so that the optimization matches the problem in the best possible way. Generally any knowledge about the problem should be incorporated into the optimization method and/or the objective function in order to make it more efficient. The interested readers are redirected to appropriate references for details of the applications wherever necessary. Elaborations on some of the applications cited above are available in [71] and [78]. VIII. Drawbacks of DE Like all other metaheuristics, DE also has some drawbacks which we must take a look at before proceeding to the discussion on future research directions with DE. Some of the recent publications [85], [S179] indicate that DE faces significant difficulty on functions that are not linearly separable and can be outperformed by CMA-ES. As pointed out by Sutton et al. [96], on such functions, DE must rely primarily on its differential mutation procedure, which, unlike its recombination strategy (with Cr < 1), is rotationally invariant. In [96], the authors also conjecture that this mutation strategy lacks sufficient selection pressure when appointing target and donor vectors to have satisfactory exploitative power on nonseparable functions. The authors also propose a rank-based parent selection scheme to impose bias on the selection step, so that DE may also learn distribution information from elite individuals in the population and can thus sample the local topology of the fitness landscape better. However, they ended up with the opinion that much more research is necessary in this area to make DE sufficiently robust against the strong interdependency of the search variables. Experimenting with different selection procedures that may increase the generational selective pressure between parents and offspring may also serve as another avenue of future work. In [44], Langdon and Poli made an attempt to evolve certain fitness landscapes with GP in order to demonstrate the benefits and weaknesses of a few population-based metaheuristics like PSO, DE, and CMA-ES. They pointed out that some problem landscapes may deceive DE such that it will get stuck in local optima most of the time; however, over similar landscapes DAS AND SUGANTHAN: DIFFERENTIAL EVOLUTION: A SURVEY OF THE STATE-OF-THE-ART 25 PSO will always find the global optima correctly within a maximum time-bound. The authors also indicated that DE sometimes has a limited ability to move its population large distances across the search space if the population is clustered in a limited portion of it. Indeed, in [S180, p. 20], the authors noted that the performance of DE deteriorates on the spiral ¡°long path problem.¡± The work in [44] also identified some landscape in which DE is outperformed by CMA-ES and a non-random gradient search based on Newton-Raphson¡¯s method (abbreviated as N-R in [S180]). A good amount of future research is needed to remove these drawbacks of DE. Also the most effective DE-variants developed so far should be investigated with the problem evolution methodology of Langdon and Poli, to identify their specific weak points over different function surfaces. IX. Potential Future Research Directions with DE Although during the last ten years, research on and with DE has reached an impressive state, there are still many open problems and new application areas are continually emerging for the algorithm. Below, we unfold some important future directions of research in the area of DE. 1) Like many other EAs, the mutation schemes employed by DE are also additive and the donor (mutant), the scaled difference, and the target vectors lie in the same hyper-plane (see Fig. 2). Is it possible to think of a new rotation-based mutation operation where the base vector is first rotated in D-dimensional hyperspace and then perturbed with the scaled difference vector? In this case the rotation will be accomplished by pre-multiplying the base vector with a D¡¿ D linear transformation matrix Q and the donor vector will be formed as \u0002V i,G = Q.\u0002Xri1 ,G + F ¡¤ (\u0002Xri2 ,G . \u0002Xri3 ,G). Consider a parameter vector \u0002Xi,G with each xj,i,G ¡ô [.a, a] (say) for j = 1, 2...D. Then as per Section IV-D, each component of the opposite vector \u0002XO i,G is formed as xO j,i,G = (.a + a) . xj,i,G = .xj,i,G and thus \u0002X O i,G = .\u0002Xi,G. Evidently, for symmetric search intervals, generation of an opposite vector amounts to rotating the actual vector by 180¡Æ. This technique has been used in ODE to improve the performance of DE. Hence, we propose to generalize this concept, i.e., rotating the mutant vectors at different angles, along with suitable self-adaptation schemes for the rotation matrix Q to improve the explorative power and thus efficiency of DE to a large extent. 2) The effectiveness of conventional DE in solving a numerical optimization problem depends on the selected mutation strategy and its associated parameter values. However, different optimization problems require different mutation strategies with different parameter values depending on the nature of problem (unimodal and multimodal) and available computation resources. In addition, to solve a specific problem, different mutation strategies with different parameter settings may be better during different stages of the evolution than a single mutation strategy with unique parameter settings as in the conventional DE. In the area of machine learning the concept of combining classifiers in an ensemble [S197] is employed to improve the overall classification performance. These classifiers could be based on a variety of classification methodologies. Similar concept was used [53] in conjunction with DE by forming an ensemble of mutation strategies and parameter values where a pool of mutation strategies, along with a pool of values corresponding to each associated parameter competes to produce successful offspring population. The candidate pool of mutation strategies and parameters should be restrictive to avoid the unfavorable influences of less effective mutation strategies and parameters. The mutation strategies or the parameters present in a pool should have diverse characteristics, so that they can exhibit distinct performance characteristics during different stages of the evolution, when dealing with a particular problem. This approach differs from SaDE as the latter adapts the parameter values slowly while the ensemble approach allows the parameters to jump to any appropriate value. This ensemble approach can be investigated further with enhanced mutation strategies and with different crossover approaches to solve different problem scenarios. 3) Future research may focus on integrating the oppositionnumber based initialization and generation jumping with self-adaptive DE variants like SaDE for improving the performance of the later. DEGL [21] puts forward the concept of topological neighborhood (ring shaped) of the population members for devising a local mutation scheme. The effect of various neighborhood topologies (star-shaped, wheel-shaped, fully connected, and so on) on the performance of DEGL should be investigated in future. Integrating DEGL in ensemble of DE schemes [S210], [52], [80], [100] should also be studied in the future. 4) Unlike the significant advancement made in the theoretical understanding of GAs, ES, and EP (see [S84], [S174].[S176]) analysis of DE family of algorithms has still not made a considerable progress. Especially an investigation of the probabilistic convergence properties of DE even on some very simple objective functions remains largely an open problem so far. Concepts like drift analysis, martingale theory [S177] and stochastic Lyapunov energy functions [S178] that have already been applied to the convergence analysis of other real coded EAs may also be investigated in context to DE. 5) Sutton et al. conjectured in [96] that over DE¡¯s weak selective pressure (due to unbiased selection of parents or target vectors) may result in inefficient exploitation when relying on differential mutation, especially on the rotated optimization problems. The authors proposed a rank-based parent selection scheme. It is obtained by sorting the population by fitness and then drawing the indices for base vector and other vectors that constitute 26 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 15, NO. 1, FEBRUARY 2011 TABLE I Summary of Applications of DE to Engineering Optimization Problems Sub Areas and Details Types of DE Applied and References Electrical Power Systems Economic dispatch Optimal power flow Power system planning, generation expansion planning Capacitor placement Distribution systems¡¯ network reconfiguration Power filter, power system stabilizer Chaotic DE [S31], hybrid DE with acceleration and migration [S87], DE/rand/1/bin [S88], hybrid DE with improved constraint handling [S89], variable scaling hybrid DE [S90] DE/target-to-best/1/bin [S91], cooperative co-evolutionary DE [S92], DE/rand/1/bin with non-dominated sorting [S93], conventional DE/rand/1/bin [S94], [S96], DE with random localization [S95]. Modified DE with fitness sharing [S97], conventional DE/rand/1/bin [S98], comparison of 10 DE strategies of Storn and Price [S99], robust searching hybird DE [S100] Hybrid of ant system and DE [S49] Hybrid DE with variable scale factor [S101], mixed integer hybrid DE [S185]. Hybrid DE with acceleration and migration operators [S102], DE/targetto- best/1/bin [S103], hybrid of DE with ant systems [S104] Electromagnetism, Propagation, and Microwave Engineering Capacitive voltage divider Electromagnetic inverse scattering Design of circular waveguide mode converters Parameter estimation and property analysis for electromagnetic devices, materials, and machines Electromagnetic imaging Antenna array design MODE and NSDE [S105] DE/rand/1/bin [S106], conventional DE with individuals in groups [S107], dynamic DE [77] DE/rand/1/bin [S108] DE/rand/1/bin [S109].[S111], [S113], DE/target-to-best/1/bin [S112] Conventional DE/rand/1/bin [S114], [S115], DE/best/1/bin [S116] Multimember DE (see [93] for details) [S117], hybrid real/integer-coded DE [S118], DE/rand/1/bin [S119], [S120], modified DE with refreshing distribution operator and fittest individual refinement operator [S121], DE/best/1/bin [S122], MOEA/D-DE [68], [69] Control Systems and Robotics System identification Optimal control problems Controller design and tuning Aircraft control Nonlinear system control Simultaneous localization and modeling problem Robot motion planning and navigation Cartesian robot control Multi-sensor data fusion Conventional DE/rand/1/bin [S123].[S126] DE/rand/1/bin and DE/best/2/bin [S127], modified DE with adjustable control weight gradient methods [S128]. Self adaptive DE [S129], DE/rand/1 with arithmetic crossover [S130], DE/rand/1/bin with random scale factor and time-varying Cr [S131]. Hybrid DE with downhill simplex local optimization [55]. Hybrid DE with convex mutation [15]. DE/rand/1/bin [S132], [S133] DE/rand/1/bin [S134], [S135] Memetic compact DE [61] DE/best/2/bin [S136] DAS AND SUGANTHAN: DIFFERENTIAL EVOLUTION: A SURVEY OF THE STATE-OF-THE-ART 27 TABLE II Summary of Applications of DE to Engineering Optimization Problems (Continued from Table I) Sub Areas and Details Types of DE Applied and References Bioinformatics Gene regulatory networks Micro-array data analysis Protein folding Bioprocess optimization DE with adaptive local search (see [22] for details) [63], hybrid of DE and PSO [S137] Multiobjective DE-variants (MODE, DEMO) [S138] DE/rand/1/bin [S139] DE/rand/1/bin [S140] Chemical Engineering Chemical process synthesis and design Phase equilibrium and phase study Parameter estimation of chemical process Modified DE with single array updating [S141], [7], 10 DE-variants of Storn and Price (see [74], [75]) compared in [S142], [S144], multiobjective DE [S143], hybrid DE with migration and acceleration operators [S145]. DE/rand/1/bin [S146]. Hybrid DE with geometric mean mutation [S147], DE/target-tobest/ 1/exp [S148]. Pattern Recognition and Image Processing Data clustering Pixel clustering and regionbased image segmentation Feature extraction Image registration and enhancement Image Watermarking DE/rand/1/bin [S149], DE with random scale factor and time-varying crossover rate [20], DE with neighborhood-based mutation [S150] Modified DE with local and global best mutation [S151], DE with random scale factor and time-varying crossover rate [S152]. DE/rand/1/bin [S153] DE/rand/1/bin [S154], DE with chaotic local search [S155] DE/rand/1/bin and DE/target-to-best/1/bin [S156] Artificial Neural Networks Training of feed-forward ANNs Training of wavelet neural networks Training of B-Spline neural networks DE/rand/1/bin [S157], [S160], generalization-based DE (GDE) [S158], DE/target-to-best/1/bin [S159] DE/rand/1/bin [S161] DE with chaotic sequence-based adjustment of scale factor F [S162] Signal Processing Non-linear ¥ó estimation Digital filter design Fractional order signal processing Dynamic DE [S163] DE/rand/1/bin [S164], [S165], DE with random scale factor [S166] DE/rand/1/bin [S167] Others Layout synthesis for MEMS Engineering design Manufacturing optimization Molecular configuration Urban energy management Optoelectronics Chess evaluation function tuning Improved DE with stochastic ranking (SR) [S168] DE/rand/1/bin [S169], multimember constraint adaptive DE [93] DE/rand/1/bin [S170], hybrid DE with forward/backward transformation [S171] Local search-based DE [S172] Hybrid of DE and CMA-ES [S173] DE/target-to-best/1/bin [S186] DE/rand/1/bin [S228] 28 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 15, NO. 1, FEBRUARY 2011 the difference vectors from a linear distribution function [S198], such that the top ranked individual is more likely to be selected than the one with poorer rank. Since high selective pressure can often cause rapid loss of diversity of the population, in order to obtain a better balance between exploration and exploitation (even when the parent selection is biased), a time-varying selection pressure is necessary. One interesting way to achieve this is to introduce an annealing schedule to the bias controlling term of the linear distribution function, such that at the beginning stage of the search the probability of selecting a top ranked individual is small (thus favoring exploration) and it gradually grows with the intensification of the search so that good exploitation can be achieved during the later stages. Also it is worthy to investigate the effect of tournament selection instead of rank-based selection that necessitates the sorting of the population at each generation. 6) In an EA, exploration is directly connected with enabling the algorithm to advance from one optimum to another. In the EA-literature, a less common exploration mechanism is saddle crossing [S199], which consists in gradual change of the position of the whole population along the saddle that connects two adjacent attraction basins of local maxima. Saddle crossing usually takes a few generations and is accompanied with a temporary decrease of the mean population fitness. Fitness proportionate selection without elitism is reported to be the most appropriate to promote saddle crossing, since is implies relatively soft selection [S199]. DE usually employs a very hard selection criterion with one-to-one competition between a parent and its offspring. For this reason, DE is very sensitive to the choice of the initial population and may suffer from the premature convergence [94]. A very interesting future research topic may be to integrate the mutation operator taken from DE with the non-elitist, fitness proportionate selection, without crossover that can result in good saddle crossing abilities of an EA [S200]. This can significantly reduce the risk of premature convergence, even in the case of unfortunate population initialization nearby a local optimum with a large attraction basin. 7) Combinatorial optimization problems deal with discrete parameters and can be found profusely in diverse domains of engineering. As pointed out in Section IV-H, DE already achieved some reputation of solving discrete, binary and mixed-integer problems. However, there is no good evidence so far that DE is applicable to strict-sense combinatorial problems, especially when they are heavily constrained. In [74], the authors discussed the topic of combinatorial problems and they attributed the success of DE-based solutions to combinatorial problems to well-chosen repair mechanisms in the algorithm rather than DE-mutation. Therefore, proving the applicability of DE to strict-sense combinatorial problems remains an open problem so far. Finding a discrete operator that corresponds to the difference vector in the continuous domain is also a challenging issue for future research. Moreover, in discrete DE-variants, it is required that the combination of a base vector and a difference vector (or recombination vector) yields a new valid vector. The validity of the newly generated vector is a big problem for most of the classical combinatorial problems like the traveling salesperson problem. 8) Many objective optimization problems [S181], [S182] typically deal with more than three objective functions. Many conventional MOEAs applying Pareto optimality as a ranking metric may perform poorly over a large number of objective functions. Extending the multiobjective variants of DE to solve many-objective problems remains open as a challenging field of future research so far. X. Conclusion With the increasing complexity of real world optimization problems, demand for robust, fast, and accurate optimizers is on the rise among researchers from various fields. DE emerged as a simple and efficient scheme for global optimization over continuous spaces more than a decade ago. Over the past few years, many researchers have contributed to make it a general and fast optimization method for any kind of objective function by twisting and tuning the various constituents of DE, i.e., initialization, mutation, diversity enhancement, and selection of DE as well as by the choice of the control variables, even though the NFL [S25] suggested already that such a cure-alloptimization algorithm could not exist. Nonetheless the existing literature clearly indicates that DE exhibits remarkable performance in optimizing a wide variety of multi-dimensional, multiobjective and multimodal optimization problems. This paper attempted to provide an overall picture of the state-of-the-art research on and with DE. Starting with a comprehensive introduction to the basic steps of the DE algorithm, it discussed the different schemes of parameter control and adaptation for DE and then overviewed several promising variants of the conventional DE. Next it provided an extensive review of the modifications of DE for tackling constrained, multiobjective, uncertain, and large-scale optimization problems. It gave a brief overview of various most significant engineering applications of DE and unfolded a number of future research directions as well. The content of the paper indicates the fact that DE will continue to remain a vibrant and active field of multi-disciplinary research in the years to come. References [1] H. Abbass, ¡°The self-adaptive Pareto differential evolution algorithm,¡± in Proc. Congr. Evol. Comput., vol. 1. May 2002, pp. 831.836. [2] H. A. Abbass and R. Sarker, ¡°The Pareto differential evolution algorithm,¡± Int. J. Artif. Intell. Tools, vol. 11, no. 4, pp. 531.552, 2002. [3] M. M. Ali and A. Torn, ¡°Population set based global optimization algorithms: Some modifications and numerical studies,¡± Comput. Oper. Res., vol. 31, no. 10, pp. 1703.1725, 2004. [4] M. M. Ali and Z. Kajee-Bagdadi, ¡°A local exploration-based differential evolution algorithm for constrained global optimization,¡± Appl. Math. Comput., vol. 208, no. 1, pp. 31.48, Feb. 2009. [5] R. Angira and A. Santosh, ¡°Optimization of dynamic systems: A trigonometric differential evolution approach,¡± Comput. Chem. Eng., vol. 31, no. 9, pp. 1055.1063, Sep. 2007. DAS AND SUGANTHAN: DIFFERENTIAL EVOLUTION: A SURVEY OF THE STATE-OF-THE-ART 29 [6] B. V. Babu and M. M. L. Jehan, ¡°Differential evolution for multiobjective optimization,¡± in Proc. Congr. Evol. Comput., vol. 4. Dec. 2003, pp. 2696.2703. [7] B. V. Babu and R. Angira, ¡°Modified differential evolution (MDE) for optimization of non-linear chemical processes,¡± Comput. Chem. Eng., vol. 30, nos. 6.7, pp. 989.1002, 2006. [8] H.-G. Beyer and K. Deb, ¡°On self-adapting features in real-parameter evolutionary algorithms,¡± IEEE Trans. Evol. Comput., vol. 5, no. 3, pp. 250.270, Jun. 2001. [9] A. Biswas, S. Dasgupta, S. Das, and A. Abraham, ¡°A synergy of differential evolution and bacterial foraging algorithm for global optimization,¡± Neural Netw. World, vol. 17, no. 6, pp. 607.626, 2007. [10] J. Brest, S. Greiner, B. Bo¢§skovi¢¥c, M. Mernik, and V. ¢§Zumer, ¡°Selfadapting control parameters in differential evolution: A comparative study on numerical benchmark problems,¡± IEEE Trans. Evol. Comput., vol. 10, no. 6, pp. 646.657, Dec. 2006. [11] J. Brest and M. S. Maueec, ¡°Population size reduction for the differential evolution algorithm,¡± Appl. Intell., vol. 29, no. 3, pp. 228.247, Dec. 2008. [12] J. Brest, A. Zamuda, B. Boskovic, M. S. Maucec, and V. Zumer, ¡°Highdimensional real-parameter optimization using self-adaptive differential evolution algorithm with population size reduction,¡± in Proc. IEEE Congr. Evol. Comput., Jun. 2008, pp. 2032.2039. [13] J. Brest, A. Zamuda, B. Boskovic, M. S. Maucec, and V. Zumer, ¡°Dynamic optimization using self-adaptive differential evolution,¡± in Proc. IEEE Congr. Evol. Comput., May 2009, pp. 415.422. [14] A. Caponio, F. Neri, and V. Tirronen, ¡°Super-fit control adaptation in memetic differential evolution frameworks,¡± Soft Comput. A Fusion Found. Methodol. Applicat., vol. 13, no. 8, pp. 811.831, 2009. [15] C.-H. Chen, C.-J. Lin, and C.-T. Lin, ¡°Nonlinear system control using adaptive neural fuzzy networks based on a modified differential evolution,¡± IEEE Trans. Syst. Man Cybern. Part C, vol. 39, no. 4, pp. 459.473, Jul. 2009. [16] S. Das, A. Konar, and U. K. Chakraborty, ¡°Improving particle swarm optimization with differentially perturbed velocity,¡± in Proc. Genet. Evol. Comput. Conf., Jun. 2005, pp. 177.184. [17] S. Das, A. Konar, and U. K. Chakraborty, ¡°Two improved differential evolution schemes for faster global search,¡± in Proc. ACM-SIGEVO GECCO, Jun. 2005, pp. 991.998. [18] S. Das, A. Konar, and U. Chakraborty, ¡°Improved differential evolution algorithms for handling noisy optimization problems,¡± in Proc. IEEE Congr. Evol. Comput., vol. 2. 2005, pp. 1691.1698. [19] S. Das, A. Konar, and U. K. Chakraborty, ¡°Annealed differential evolution,¡± in Proc. IEEE Congr. Evol. Comput., 2007, pp. 1926.1933. [20] S. Das, A. Abraham, and A. Konar, ¡°Automatic clustering using an improved differential evolution algorithm,¡± IEEE Trans. Syst. Man Cybern. Part A, vol. 38, no. 1, pp. 218.236, Jan. 2008. [21] S. Das, A. Abraham, U. K. Chakraborty, and A. Konar, ¡°Differential evolution using a neighborhood based mutation operator,¡± IEEE Trans. Evol. Comput., vol. 13, no. 3, pp. 526.553, Jun. 2009. [22] S. Dasgupta, S. Das, A. Biswas, and A. Abraham, ¡°The population dynamics of differential evolution: A mathematical model,¡± in Proc. IEEE Congr. Evol. Comput., Jun. 2008, pp. 1439.1446. [23] S. Dasgupta, S. Das, A. Biswas, and A. Abraham, ¡°On stability and convergence of the population-dynamics in differential evolution,¡± AI Commun., vol. 22, no. 1, pp. 1.20, 2009. [24] K. Deb and H.-G. Beyer, ¡°Self-adaptive genetic algorithms with simulated binary crossover,¡± Evol. Comput., vol. 9, no. 2, pp. 197. 221, Jun. 2001. [25] H.-Y. Fan and J. Lampinen, ¡°A trigonometric mutation operation to differential evolution,¡± J. Global Optimization, vol. 27, no. 1, pp. 105. 129, 2003. [26] V. Feoktistov and S. Janaqi, ¡°Generalization of the strategies in differential evolution,¡± in Proc. 18th IPDPS, Apr. 2004, p. 165a. [27] Y. Gao and Y. Wang, ¡°A memetic differential evolutionary algorithm for high dimensional function spaces optimization,¡± in Proc. 3rd ICNC 20, vol. 4. Aug. 2007, pp. 188.192. [28] V. L. Huang, A. K. Qin, and P. N. Suganthan, ¡°Self-adaptive differential evolution algorithm for constrained real-parameter optimization,¡± in Proc. IEEE Congr. Evol. Comput., Jul. 2006, pp. 324.331. [29] V. L. Huang, A. K. Qin, P. N. Suganthan, and M. F. Tasgetiren, ¡°Multiobjective optimization based on self-adaptive differential evolution algorithm,¡± in Proc. Congr. Evol. Comput., Sep. 2007, pp. 3601.3608. [30] V. L. Huang, S. Z. Zhao, R. Mallipeddi, and P. N. Suganthan, ¡°Multiobjective optimization using self-adaptive differential evolution algorithm (special session and competition on ¡®performance assessment of constrained/ bound constrained multiobjective optimization algorithms¡¯),¡± in Proc. Conf. Congr. Evol. Comput., May 2009, pp. 190.194. [31] F. Huang, L. Wang, and Q. He, ¡°An effective co-evolutionary differential evolution for constrained optimization,¡± Appl. Math. Comput., vol. 186, no. 1, pp. 340.356, Mar. 2007. [32] A. W. Iorio and X. Li, ¡°Solving rotated multiobjective optimization problems using differential evolution,¡± in Proc. AI: Adv. Artif. Intell., LNCS 3339. 2004, pp. 861.872. [33] P. Kaelo and M. M. Ali, ¡°Differential evolution algorithms using hybrid mutation,¡± Comput. Optimization Applicat., vol. 37, pp. 231.246, Jun. 2007. [34] T. Krink, B. Filipi¢§c, and G. B. Fogel, ¡°Noisy optimization problems: A particular challenge for differential evolution,¡± in Proc. IEEE Congr. Evol. Comput., 2004, pp. 332.339. [35] S. Kukkonen and J. Lampinen, ¡°GDE3: The third evolution step of generalized differential evolution,¡± in Proc. IEEE Congr. Evol. Comput., vol. 1. Sep. 2005, pp. 443.450. [36] S. Kukkonen and J. Lampinen, ¡°Constrained real-parameter optimization with generalized differential evolution,¡± in Proc. IEEE Congr. Evol. Comput., Jul. 2006, pp. 911.918. [37] J. Lampinen, ¡°A bibliography of differential evolution algorithm,¡± Lab. Inform. Process., Dept. Inform. Technol., Lappeenranta Univ. Technol., Lappeenranta, Finland, Tech. Rep., 1999 [Online]. Available: http://www.lut.fi/¡­jlampine/debiblio.htm [38] J. Lampinen, ¡°Differential evolution: New naturally parallel approach for engineering design optimization,¡± in Developments in Computational Mechanics with High Performance Computing, B. H. V. Topping, Ed. Edinburgh, U.K.: Civil-Comp Press, 1999, pp. 217.228. [39] J. Lampinen, ¡°A constraint handling approach for the differential evolution algorithm,¡± in Proc. Congr. Evol. Comput., vol. 2. May 2002, pp. 1468.1473. [40] J. Lampinen and I. Zelinka, ¡°Mixed integer-discrete-continuous optimization with differential evolution,¡± in Proc. 5th Int. Mendel Conf. Soft Comput., Jun. 1999, pp. 71.76. [41] J. Lampinen and I. Zelinka, ¡°On stagnation of the differential evolution algorithm,¡± in Proc. 6th Int. Mendel Conf. Soft Comput., Jun. 2000, pp. 76.83. [42] R. Landa Becerra and C. A. Coello Coello, ¡°Solving hard multiobjective optimization problems using ¥å-constraint with cultured differential evolution,¡± in Proc. 9th Int. Conf. Parallel Problem Solving Nature, LNCS 4193. Sep. 2006, pp. 543.552. [43] R. Landa Becerra and C. A. Coello Coello, ¡°Cultured differential evolution for constrained optimization,¡± Comput. Methods Appl. Mech. Eng., vol. 195, nos. 33.36, pp. 4303.4322, Jul. 2006. [44] W. B. Langdon and R. Poli, ¡°Evolving problems to learn about particle swarm optimizers and other search algorithms,¡± IEEE Trans. Evol. Comput., vol. 11, no. 5, pp. 561.578, Oct. 2007. [45] C. Li, S. Yang, T. T. Nguyen, E. L. Yu, X. Yao, Y. Jin, H.-G. Beyer, and P. N. Suganthan, ¡°Benchmark generator for CEC¡¯2009 competition on dynamic optimization,¡± Univ. Leicester, Leicester, U.K., Univ. Birmingham, U.K., Nanyang Technological Univ., Singapore, Tech. Rep., Sep. 2008. [46] H. Li and Q. Zhang, ¡°Multiobjective optimization problems with complicated Pareto sets, MOEA/D and NSGA-II,¡± IEEE Trans. Evol. Comput., vol. 13, no. 2, pp. 284.302, Apr. 2009. [47] J. J. Liang, T. P. Runarsson, E. Mezura-Montes, M. Clerc, P. N. Suganthan, C. A. Coello Coello, and K. Deb, ¡°Problem definitions and evaluation criteria for the CEC 2006 (special session on constrained real-parameter optimization),¡± Nanyang Technol. Univ., Singapore, Tech. Rep., 2006. [48] J. Liu and J. Lampinen, ¡°A fuzzy adaptive differential evolution algorithm,¡± Soft Comput. A Fusion Founda. Methodol. Applicat., vol. 9, no. 6, pp. 448.462, 2005. [49] B. Liu, X. Zhang, and H. Ma, ¡°Hybrid differential evolution for noisy optimization,¡± in Proc. IEEE Congr. Evol. Comput., Jun. 2008, pp. 587.592. [50] R. Mallipeddi and P. N. Suganthan, ¡°Empirical study on the effect of population size on differential evolution algorithm,¡± in Proc. IEEE Congr. Evol. Comput., Jun. 2008, pp. 3663.3670. [51] R. Mallipeddi and P. N. Suganthan, ¡°Differential evolution algorithm with ensemble of populations for global numerical optimization,¡± OPSEARCH, vol. 46, no. 2, pp. 184.213, Jun. 2009. [52] R. Mallipeddi and P. N. Suganthan, ¡°Ensemble of constraint handling techniques,¡± IEEE Trans. Evol. Comput., vol. 14, no. 4, pp. 561.579, Aug. 2010. [53] R. Mallipeddi, P. N. Suganthan, Q. K. Pan, and M. F. Tasgetiren, ¡°Differential evolution algorithm with ensemble of param- 30 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 15, NO. 1, FEBRUARY 2011 eters and mutation strategies,¡± Appl. Soft Comput., to be published. [54] R. Mendes and A. S. Mohais, ¡°DynDE: A differential evolution for dynamic optimization problems,¡± in Proc. IEEE Congr. Evol. Comput., vol. 2. 2005, pp. 2808.2815. [55] P. P. Menon, J. Kim, D. G. Bates, and I. Postlethwaite, ¡°Clearance of nonlinear flight control laws using hybrid evolutionary optimization,¡± IEEE Trans. Evol. Comput., vol. 10, no. 6, pp. 689.699, Dec. 2006. [56] E. Mezura-Montes, J. Vel¢¥azquez-Reyes, and C. A. Coello Coello, ¡°A comparative study of differential evolution variants for global optimization,¡± in Proc. Genet. Evol. Comput. Conf., 2006, pp. 485. 492. [57] E. Mezura-Montes, J. Vel¢¥azquez-Reyes, and C. A. Coello Coello, ¡°Modified differential evolution for constrained optimization,¡± in Proc. IEEE Congr. Evol. Comput., Jul. 2006, pp. 332.339. [58] P. W. Moore and G. K. Venayagamoorthy, ¡°Evolving digital circuit using hybrid particle swarm optimization and differential evolution,¡± Int. J. Neural Syst., vol. 16, no. 3, pp. 163.177, 2006. [59] F. Neri and V. Tirronen, ¡°Scale factor local search in differential evolution,¡± Memetic Comput., vol. 1, no. 2, pp. 153.171, Jun. 2009. [60] F. Neri and V. Tirronen, ¡°Recent advances in differential evolution: A review and experimental analysis,¡± Artif. Intell. Rev., vol. 33, no. 1, pp. 61.106, Feb. 2010. [61] F. Neri and E. Mininno, ¡°Memetic compact differential evolution for Cartesian robot control,¡± IEEE Comput. Intell. Mag., vol. 5, no. 2, pp. 54.65, May 2010. [62] N. Noman and H. Iba, ¡°Enhancing differential evolution performance with local search for high dimensional function optimization,¡± in Proc. Conf. Genet. Evol. Comput., Jun. 2005, pp. 967.974. [63] N. Noman and H. Iba, ¡°Inferring gene regulatory networks using differential evolution with local search heuristics,¡± IEEE/ACM Trans. Comput. Biol. Bioinform., vol. 4, no. 4, pp. 634.647, Oct. 2007. [64] N. Noman and H. Iba, ¡°Accelerating differential evolution using an adaptive local search,¡± IEEE Trans. Evol. Comput., vol. 12, no. 1, pp. 107.125, Feb. 2008. [65] M. G. H. Omran, A. Salman, and A. P. Engelbrecht, ¡°Self-adaptive differential evolution,¡± in Proc. Comput. Intell. Security, Lecture Notes in Artificial Intelligence 3801. 2005, pp. 192.199. [66] M. G. H. Omran, A. P. Engelbrecht, and A. Salman, ¡°Bare bones differential evolution,¡± Eur. J. Oper. Res., vol. 196, no. 1, pp. 128. 139, Jul. 2009. [67] G. Pampara, A. P. Engelbrecht, and N. Franken, ¡°Binary differential evolution,¡± in Proc. IEEE Congr. Evol. Comput., Jul. 2006, pp. 1873. 1879. [68] S. Pal, Q. Boyang, S. Das, and P. N. Suganthan, ¡°Optimal synthesis of linear antenna arrays with multiobjective differential evolution,¡± Prog. Electromag. Res. PIERB, vol. 21, pp. 87.111, 2010. [69] S. Pal, S. Das, A. Basak, and P. N. Suganthan, ¡°Synthesis of difference patterns for monopulse antennas with optimal combination of array-size and number of subarrays: A multiobjective optimization approach,¡± Prog. Electromag. Res. PIERB, vol. 21, pp. 257.280, 2010. [70] K. E. Parsopoulos, D. K. Taoulis, N. G. Pavlidis, V. P. Plagianakos, and M. N. Vrahatis, ¡°Vector evaluated differential evolution for multiobjective optimization,¡± in Proc. Congr. Evol. Comput., vol. 1. Jun. 2004, pp. 204.211. [71] V. P. Plagianakos, D. K. Tasoulis, and M. N. Vrahatis, ¡°A review of major application areas of differential evolution,¡± in Advances in Differential Evolution, SCI 143, U. K. Chakraborty, Ed. Berlin, Germany: Springer, 2008, pp. 197.238. [72] K. V. Price, ¡°Differential evolution vs. the functions of the 2nd ICEO,¡± in Proc. IEEE Int. Conf. Evol. Comput., Apr. 1997, pp. 153.157. [73] K. V. Price and R. Storn, ¡°Differential evolution: A simple evolution strategy for fast optimization,¡± Dr. Dobb¡¯s J., vol. 22, no. 4, pp. 18.24, 1997. [74] K. Price, R. Storn, and J. Lampinen, Differential Evolution.A Practical Approach to Global Optimization. Berlin, Germany: Springer, 2005. [75] K. V. Price, ¡°An introduction to differential evolution,¡± in New Ideas in Optimization, D. Corne, M. Dorigo, and V. Glover, Eds. London, U.K.: McGraw-Hill, 1999, pp. 79.108. [76] A. K. Qin, V. L. Huang, and P. N. Suganthan, ¡°Differential evolution algorithm with strategy adaptation for global numerical optimization,¡± IEEE Trans. Evol. Comput., vol. 13, no. 2, pp. 398.417, Apr. 2009. [77] A. Qing, ¡°Dynamic differential evolution strategy and applications in electromagnetic inverse scattering problems,¡± IEEE Trans. Geosci. Remote Sensing, vol. 44, no. 1, pp. 116.125, Jan. 2006. [78] A. Qing, Differential Evolution.Fundamentals and Applications in Electrical Engineering. New York: Wiley, Sep. 2009. [79] B. Y. Qu and P. N. Suganthan, ¡°Multiobjective evolutionary algorithms based on the summation of normalized objectives and diversified selection,¡± Inform. Sci., vol. 180, no. 17, pp. 3170.3181, Sep. 2010. [80] B. Y. Qu and P. N. Suganthan, ¡°Constrained multiobjective optimization algorithm with ensemble of constraint handling methods,¡± Eng. Optimization, to be published. [81] B. Y. Qu and P. N. Suganthan, ¡°Novel multimodal problems and differential evolution with ensemble of restricted tournament selection,¡± in Proc. IEEE Congr. Evol. Comput., Jul. 2010, pp. 3480.3486. [82] S. Rahnamayan, H. R. Tizhoosh, and M. M. A. Salama, ¡°Oppositionbased differential evolution,¡± IEEE Trans. Evol. Comput., vol. 12, no. 1, pp. 64.79, Feb. 2008. [83] T. Robic and B. Filipic, ¡°DEMO: Differential evolution for multiobjective optimization,¡± in Proc. 3rd Int. Conf. Evol. Multi-Criterion Optimization, LNCS 3410. 2005, pp. 520.533. [84] J. Ronkkonen and J. Lampinen, ¡°On using normally distributed mutation step length for the differential evolution algorithm,¡± in Proc. 9th Int. Conf. Soft Comput. MENDEL, 2003, pp. 11.18. [85] J. Ronkkonen, S. Kukkonen, and K. V. Price, ¡°Real parameter optimization with differential evolution,¡± in Proc. IEEE CEC, vol. 1. 2005, pp. 506.513. [86] L. V. Santana-Quintero and C. A. Coello Coello, ¡°An algorithm based on differential evolution for multiobjective problems,¡± Int. J. Comput. Intell. Res., vol. 1, no. 2, pp. 151.169, 2005. [87] L. V. Santana-Quintero, A. G. Hernandez-Diaz, J. Molina, C. A. Coello Coello, and R. Caballero, ¡°DEMORS: A hybrid multiobjective optimization algorithm using differential evolution and rough sets for constrained problems,¡± Comput. Oper. Res., vol. 37, no. 3, pp. 470.480, Mar. 2010. [88] R. Storn and K. V. Price, ¡°Differential evolution: A simple and efficient adaptive scheme for global optimization over continuous spaces,¡± ICSI, USA, Tech. Rep. TR-95-012, 1995 [Online]. Available: http://icsi.berkeley.edu/¡­storn/litera.html [89] R. Storn and K. V. Price, ¡°Minimizing the real functions of the ICEC 1996 contest by differential evolution,¡± in Proc. IEEE Int. Conf. Evol. Comput., 1996, pp. 842.844. [90] R. Storn, ¡°On the usage of differential evolution for function optimization,¡± in Proc. North Am. Fuzzy Inform. Process. Soc., 1996, pp. 519.523. [91] R. Storn, ¡°Differential evolution design of an IIR-filter with requirements for magnitude and group delay,¡± in Proc. IEEE Int. Conf. Evol. Comput., 1996, pp. 268.273. [92] R. Storn and K. Price, ¡°Differential evolution: A simple and efficient heuristic for global optimization over continuous spaces,¡± J. Global Optimization, vol. 11, no. 4, pp. 341.359, 1997. [93] R. Storn, ¡°System design by constraint adaptation and differential evolution,¡± IEEE Trans. Evol. Comput., vol. 3, no. 1, pp. 22.34, Apr. 1999. [94] R. Storn, ¡°Differential evolution research: Trends and open questions,¡± in Advances in Differential Evolution, U. K. Chakraborty, Ed. Berlin, Germany: Springer, 2008, pp. 1.32. [95] P. N. Suganthan, N. Hansen, J. J. Liang, K. Deb, Y.-P. Chen, A. Auger, and S. Tiwari, ¡°Problem definitions and evaluation criteria for the CEC 2005 special session on real-parameter optimization,¡± Nanyang Technol. Univ., Singapore, Tech. Rep., IIT Kanpur, Kanpur, India, KanGAL Rep. #2005005, May 2005. [96] A. M. Sutton, M. Lunacek, and L. D. Whitley, ¡°Differential evolution and non-separability: Using selective pressure to focus search,¡± in Proc. 9th Annu. Conf. GECCO, Jul. 2007, pp. 1428.1435. [97] T. Takahama and S. Sakai, ¡°Constrained optimization by the ¥å constrained differential evolution with gradient-based mutation and feasible elites,¡± in Proc. IEEE Congr. Evol. Comput., Jul. 2006, pp. 308.315. [98] K. Tang, X. Yao, P. N. Suganthan, C. MacNish, Y. P. Chen, C. M. Chen, and Z. Yang, ¡°Benchmark functions for the CEC¡¯2008 special session and competition on large scale global optimization,¡± Nature Inspired Comput. Applicat. Lab., USTC, China, Nanyang Technol. Univ., Singapore, Tech. Rep., 2007. [99] M. F. Tasgetiren and P. N. Suganthan, ¡°A multi-populated differential evolution algorithm for solving constrained optimization problem,¡± in Proc. IEEE Congr. Evol. Comput., Jul. 2006, pp. 340.354. [100] M. F. Tasgetiren, P. N. Suganthan, and Q. K. Pan, ¡°An ensemble of discrete differential evolution algorithms for solving the generalized traveling salesman problem,¡± Appl. Math. Comput., vol. 215, no. 9, pp. 3356.3368, Jan. 2010. DAS AND SUGANTHAN: DIFFERENTIAL EVOLUTION: A SURVEY OF THE STATE-OF-THE-ART 31 [101] D. K. Tasoulis, N. G. Pavlidis, V. P. Plagianakos, and M. N. Vrahatis, ¡°Parallel differential evolution,¡± in Proc. Congr. Evol. Comput., 2004, pp. 2023.2029. [102] J. Teo, ¡°Exploring dynamic self-adaptive populations in differential evolution,¡± Soft Comput. A Fusion Found. Methodol. Applicat., vol. 10, no. 8, pp. 673.686, 2006. [103] R. Thomsen, ¡°Multimodal optimization using crowding-based differential evolution,¡± in Proc. IEEE Congr. Evol. Comput., 2004, pp. 1382. 1389. [104] J. Vesterstr©ªm and R. A. Thomson, ¡°Comparative study of differential evolution, particle swarm optimization, and evolutionary algorithms on numerical benchmark problems,¡± in Proc. IEEE Congr. Evol. Comput., 2004, pp. 1980.1987. [105] F. Xue, A. C. Sanderson, and R. J. Graves, ¡°Pareto-based multiobjective differential evolution,¡± in Proc. Congr. Evol. Comput., vol. 2. 2003, pp. 862.869. [106] F. Xue, A. C. Sanderson, and R. J. Graves, ¡°Modeling and convergence analysis of a continuous multiobjective differential evolution algorithm,¡± in Proc. IEEE Congr. Evol. Comput., vol. 1. Sep. 2005, pp. 228.235. [107] F. Xue, A. C. Sanderson, and R. J. Graves, ¡°Multiobjective differential evolution: Algorithm, convergence analysis, and applications,¡± in Proc. IEEE Congr. Evol. Comput., vol. 1. Sep. 2005, pp. 743.750. [108] Z. Yang, J. He, and X. Yao, ¡°Making a difference to differential evolution,¡± in Advances in Metaheuristics for Hard Optimization, Z. Michalewicz and P. Siarry, Eds. Berlin, Germany: Springer, 2007, pp. 415.432. [109] Z. Yang, K. Tang, and X. Yao, ¡°Differential evolution for highdimensional function optimization,¡± in Proc. IEEE Congr. Evol. Comput., Sep. 2007, pp. 3523.3530. [110] Z. Yang, K. Tang, and X. Yao, ¡°Large scale evolutionary optimization using cooperative coevolution,¡± Inform. Sci., vol. 178, no. 15, pp. 2985. 2999, 2008. [111] D. Zaharie, ¡°On the explorative power of differential evolution,¡± in Proc. 3rd Int. Workshop Symbolic Numerical Algorithms Scientific Comput., Oct. 2001 [Online]. Available: http://web.info.uvt.ro/¡­dzaharie/online.papers.html [112] D. Zaharie, ¡°Critical values for the control parameters of differential evolution algorithms,¡± in Proc. 8th Int. Mendel Conf. Soft Comput., 2002, pp. 62.67. [113] D. Zaharie, ¡°Parameter adaptation in differential evolution by controlling the population diversity,¡± in Proc. 4th Int. Workshop Symbolic Numeric Algorithms Sci. Comput., 2002, pp. 385.397. [114] D. Zaharie, ¡°Statistical properties of differential evolution and related random search algorithms,¡± in Proc. Int. Conf. Comput. Statist., Aug. 2008, pp. 473.485. [115] D. Zaharie, ¡°Influence of crossover on the behavior of differential evolution algorithms,¡± Appl. Soft Comput., vol. 9, no. 3, pp. 1126. 1138, Jun. 2009. [116] A. Zamuda, J. Brest, B. Boskovic, and V. Zumer, ¡°Differential evolution for multiobjective optimization with self adaptation,¡± in Proc. Congr. Evol. Comput., Sep. 2007, pp. 3617.3624. [117] M. Zhang, W. Luo, and X. Wang, ¡°Differential evolution with dynamic stochastic selection for constrained optimization,¡± Inform. Sci., vol. 178, no. 15, pp. 3043.3074, Aug. 2008. [118] J. Zhang and A. C. Sanderson, ¡°JADE: Adaptive differential evolution with optional external archive,¡± IEEE Trans. Evol. Comput., vol. 13, no. 5, pp. 945.958, Oct. 2009. [119] W.-J. Zhang and X.-F. Xie, ¡°DEPSO: Hybrid particle swarm with differential evolution operator,¡± in Proc. IEEE Int. Conf. Syst. Man Cybern., 2003, pp. 3816.3821. [120] K. Zielinski, D. Peters, and R. Laur, ¡°Run time analysis regarding stopping criteria for differential evolution and particle swarm optimization,¡± in Proc. 1st Int. Conf. Exp./Process/System Modelling/ Simulation/Optimization, 2005 [Online]. Available: http://www. item.uni-bremen.de/research/papers/paper.pdf/Zielinski.Karin/zielinski05run. pdf [121] K. Zielinski and R. Laur, ¡°Constrained single-objective optimization using differential evolution,¡± in Proc. IEEE Congr. Evol. Comput., Jul. 2006, pp. 927.934. Swagatam Das (M¡¯10) received the B.E. Tel. E., M.E. Tel. E. (control engineering specialization), and Ph.D. degrees, all from Jadavpur University, Kolkata, India, in 2003, 2005, and 2009, respectively. He is currently an Assistant Professor with the Department of Electronics and Telecommunication Engineering, Jadavpur University. He has published more than 100 research articles in peer-reviewed journals and international conferences. He has coauthored a research monograph on metaheuristic clustering techniques published by Springer, Berlin, Germany, in 2009. His current research interests include evolutionary computing, swarm intelligence, pattern recognition, bioinformatics, control systems engineering, and digital signal processing. Dr. Das serves as an Associate Editor for the Information Sciences Journal (Elsevier), and as an Editorial Board Member of the International Journal of Artificial Intelligence and Soft Computing and the International Journal of Adaptive and Autonomous Communication Systems. He is a Founding Co- Editor-in-Chief of Swarm and Evolutionary Computation, an international journal from Elsevier. He has been acting as a Regular Reviewer for journals like Pattern Recognition, IEEE Transactions on Evolutionary Computation, IEEE/ACM Transactions on Computational Biology and Bioinformatics, and IEEE Transactions on SMC Part A and Part B. Ponnuthurai Nagaratnam Suganthan (M¡¯91. SM¡¯01) received the B.A. degree, the Postgraduate Certificate, and the M.A. degree in electrical and information engineering from the University of Cambridge, Cambridge, U.K., in 1990, 1992, and 1994, respectively, and the Ph.D. degree from the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore. He was a Pre-Doctoral Research Assistant with the Department of Electrical Engineering, University of Sydney, Sydney, Australia, from 1995 to 1996, and was a Lecturer with the Department of Computer Science and Electrical Engineering, University of Queensland, Brisbane, Australia, from 1996 to 1999. Since 1999, he has been with the School of Electrical and Electronic Engineering, Nanyang Technological University, where he was previously an Assistant Professor and is currently an Associate Professor. His current research interests include evolutionary computation, pattern recognition, multiobjective evolutionary algorithms, bioinformatics, applications of evolutionary computation, and neural networks. Dr. Suganthan is an Associate Editor of the journals IEEE Transactions on Evolutionary Computation, Information Sciences, Pattern Recognition, and the International Journal of Swarm Intelligence Research. He is a Founding Co-Editor-in-Chief of Swarm and Evolutionary Computation, an international journal from Elsevier.\"],\n",
    "[8,3,1,\"398 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 13, NO. 2, APRIL 2009 Differential Evolution Algorithm With Strategy Adaptation for Global Numerical Optimization A. K. Qin, V. L. Huang, and P. N. Suganthan Abstract—Differential evolution (DE) is an efficient and powerful population-based stochastic search technique for solving optimization problems over continuous space, which has been widely applied in many scientific and engineering fields. However, the success of DEin solving a specific problem crucially depends on appropriately choosing trial vector generation strategies and their associated control parameter values. Employing a trial-and-error scheme to search for the most suitable strategy and its associated parameter settings requires high computational costs. Moreover, at different stages of evolution, different strategies coupled with different parameter settings may be required in order to achieve the best performance. In this paper, we propose a self-adaptive DE (SaDE) algorithm, in which both trial vector generation strategies and their associated control parameter values are gradually self-adapted by learning from their previous experiences in generating promising solutions. Consequently, a more suitable generation strategy along with its parameter settings can be determined adaptively to match different phases of the search process/evolution. The performance of the SaDE algorithm is extensively evaluated (using codes available from P. N. Suganthan) on a suite of 26 bound-constrained numerical optimization problems and compares favorably with the conventional DE and several state-of-the-art parameter adaptive DE variants. Index Terms—Differential evolution (DE), global numerical optimization, parameter adaptation, self-adaptation, strategy adaptation. I. INTRODUCTION EVOLUTIONARY ALGORITHMs (EAs), inspired by the natural evolution of species, have been successfully applied to solve numerous optimization problems in diverse fields. However, when implementing the EAs, users not only need to determine the appropriate encoding schemes and evolutionary operators, but also need to choose the suitable parameter settings to ensure the success of the algorithm, which may lead to demanding computational costs due to the time-consuming trial-and-error parameter and operator tuning process. To overcome such inconvenience, researchers have actively investigated the adaptation of parameters and operators in EAs [1]–[3]. Different categorizations of parameter Manuscript received January 16, 2007; revised July 20, 2007, March 24, 2008, and May 08, 2008. First published September 26, 2008; current version published April 01, 2009. This work was supported by the A*Star (Agency for Science, Technology and Research, Singapore) under Grant 052 101 0020. The authors are with the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore 639798, Singapore (e-mail: qinkai@pmail.ntu.edu.sg; huangling@pmail.ntu.edu.sg; epnsugan@ntu.edu.sg). Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org. Digital Object Identifier 10.1109/TEVC.2008.927706 adaptation methods have been presented in [4]–[6]. Angeline [4] summarized two types of parameter updating rules in an adaptive EA, namely, absolute and empirical rules. Absolute updating rules usually prespecify how the parameter modifications would be made, while empirical updating rules adapt parameters according to the competition inherent in EAs. Literature [5] divided the parameter adaptation techniques into three categories: deterministic, adaptive, and self-adaptive control rules. Deterministic rules modify the parameters according to certain predetermined rationales without utilizing any feedback from the search process. Adaptive rules incorporate some form of the feedback from the search procedure to guide the parameter adaptation. Self-adaptive rules directly encode parameters into the individuals and evolve them together with the encoded solutions. Parameter values involved in individuals with better fitness values will survive, which fully utilize the feedback from the search process. Generally speaking, self-adaptive rules can also refer to those rules that mainly utilize the feedback from the search process such as fitness values to guide the updating of parameters. The differential evolution (DE) algorithm, proposed by Storn and Price [7], is a simple yet powerful population-based stochastic search technique, which is an efficient and effective global optimizer in the continuous search domain. DE has been successfully applied in diverse fields such as mechanical engineering [13], [14], communication [11], and pattern recognition [10]. In DE, there exist many trial vector generation strategies out of which a few may be suitable for solving a particular problem. Moreover, three crucial control parameters involved in DE, i.e., population size , scaling factor , and crossover rate , may significantly influence the optimization performance of the DE. Therefore, to successfully solve a specific optimization problem at hand, it is generally required to perform a time-consuming trial-and-error search for the most appropriate strategy and to tune its associated parameter values. However, such a trial-and-error searching process requires high computational costs. Moreover, as evolution proceeds, the population of DE may move through different regions in the search space, within which certain strategies associated with specific parameter settings may be more effective than others. Therefore, it is desirable to adaptively determine an appropriate strategy and its associated parameter values at different stages of evolution/search process. In this paper, we propose a self-adaptive DE (SaDE) algorithm to avoid the expensive computational costs spent on searching for the most appropriate trial vector generation strategy as well as its associated parameter values by a trial-and-error procedure. Instead, both strategies and their associated parameters are gradually 1089-778X/$25.00 © 2008 13:36:41 UTC from IEEE Xplore. Restrictions apply. QIN et al.: DIFFERENTIAL EVOLUTION ALGORITHM WITH STRATEGY ADAPTATION FOR GLOBAL NUMERICAL OPTIMIZATION 399 self-adapted by learning from their previous experiences in generating promising solutions. Consequently, a more suitable generation strategy along with its parameter settings can be determined adaptively to match different search/evolution phases. Specifically, at each generation, a set of trial vector generation strategies together with their associated parameter values will be separately assigned to different individuals in the current population according to the selection probabilities learned from the previous generations. The remainder of this paper is organized as follows. The conventional DE and related work are reviewed in Sections II and III, respectively. Section IV describes the proposed SaDE. Experimental results demonstrating the performance of SaDE in comparison with the conventional DE and several state-of-the-art adaptive DE variants over a suite of 26 bound constrained numerical optimization problems are presented in Section V. Section VI concludes this paper. II. DE ALGORITHM DE algorithm aims at evolving a population of -dimensional parameter vectors, so-called individuals, which encode the candidate solutions, i.e., towards the global optimum. The initial population should better cover the entire search space as much as possible by uniformly randomizing individuals within the search space constrained by the prescribed minimum and maximum parameter bounds and . For example, the initial value of the th parameter in the th individual at the generation is generated by (1) where represents a uniformly distributed random variable within the range . A. Mutation Operation After initialization, DE employs the mutation operation to produce a mutant vector with respect to each individual , so-called target vector, in the current population. For each target vector at the generation , its associated mutant vector can be generated via certain mutation strategy. For example, the five most frequently used mutation strategies implemented in the DE codes1 are listed as follows: 1) “DE/rand/1”: (2) 2) “DE/best/1”: (3) 3) “DE/rand-to-best/1”: (4) 1Publicly available online at http://www.icsi.berkeley.edu/~storn/code.html 4) “DE/best/2”: (5) 5) “DE/rand/2”: (6) The indices are mutually exclusive integers randomly generated within the range , which are also different from the index . These indices are randomly generated once for each mutant vector. The scaling factor is a positive control parameter for scaling the difference vector. is the best individual vector with the best fitness value in the population at generation . B. Crossover Operation After the mutation phase, crossover operation is applied to each pair of the target vector and its corresponding mutant vector to generate a trial vector: . In the basic version, DE employs the binomial (uniform) crossover defined as follows: if or otherwise (7) In (7), the crossover rate is a user-specified constant within the range , which controls the fraction of parameter values copied from the mutant vector. is a randomly chosen integer in the range . The binomial crossover operator copies the th parameter of the mutant vector to the corresponding element in the trial vector if or . Otherwise, it is copied from the corresponding target vector . There exists another exponential crossover operator, in which the parameters of trial vector are inherited from the corresponding mutant vector starting from a randomly chosen parameter index till the first time . The remaining parameters of the trial vector are copied from the corresponding target vector . The condition is introduced to ensure that the trial vector will differ from its corresponding target vector by at least one parameter. DE’s exponential crossover operator is functionally equivalent to the circular two-point crossover operator. C. Selection Operation If the values of some parameters of a newly generated trial vector exceed the corresponding upper and lower bounds, we randomly and uniformly reinitialize them within the prespecified range. Then, the objective function values of all trial vectors are evaluated. After that, a selection operation is performed. The objective function value of each trial vector is compared to that of its corresponding target vector in the current population. If the trial vector has less or equal objective function value than the corresponding target vector, the trial vector will replace the target vector and enter the population of the next generation. Otherwise, the target vector will remain in Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:36:41 UTC from IEEE Xplore. Restrictions apply. 400 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 13, NO. 2, APRIL 2009 TABLE I ALGORITHMIC DESCRIPTION OF DE the population for the next generation. The selection operation can be expressed as follows: if otherwise. (13) The above 3 steps are repeated generation after generation until some specific termination criteria are satisfied. The algorithmic description of DE is summarized in Table I. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:36:41 UTC from IEEE Xplore. Restrictions apply. QIN et al.: DIFFERENTIAL EVOLUTION ALGORITHM WITH STRATEGY ADAPTATION FOR GLOBAL NUMERICAL OPTIMIZATION 401 III. PREVIOUS WORK RELATED TO DE The performance of the conventional DE algorithm highly depends on the chosen trial vector generation strategy and associated parameter values used. Inappropriate choice of strategies and parameters may lead to premature convergence or stagnation, which have been extensively demonstrated in [8], [16], [17], [24], and [29]. In the past decade, DE researchers have suggested many empirical guidelines for choosing trial vector generation strategies and their associated control parameter settings. Storn and Price [15] suggested that a reasonable value for should be between and , and a good initial choice of was 0.5. The effective range of values was suggested between 0.4 and 1. The first reasonable attempt of choosing value can be 0.1. However, because the large value can speed up convergence, the value of 0.9 for may also be a good initial choice if the problem is near unimodal or fast convergence is desired. Moreover, if the population converges prematurely, either or can be increased. It was recommended in [20] to use the trial vector generation strategy DE/current-to-rand/1 and parameter setting . If the DE converges prematurely, one should increase the value of and or decrease the value of . If the population stagnates, one should increase the value of or , or randomly choose within the range . If none of the above configuration works, one may try the strategy DE/rand/1/bin along with a small {\\rm CR} value. Gämperle et al. [17] examined different parameter settings for DE on Sphere, Rosenbrock, and Rastrigin functions. Their experimental results showed that the searching capability and convergence speed are very sensitive to the choice of control parameters , , and . They recommended that the population size be between and , the scaling factor equal 0.6, and the crossover rate be between . Recently, Rönkkönen et al. [21] suggested using values between with being a good initial choice. The values should lie in when the function is separable while in when the function’s parameters are dependent. However, when solving a real engineering problem, the characteristics of the problem are usually unknown. Consequently, it is difficult to choose the most appropriate value in advance. In DE literatures, various conflicting conclusions have been drawn with regard to the rules for manually choosing the strategy and control parameters, which undesirably confuse scientist and engineers who are about to utilize DE to solve scientific and engineering problems. In fact, most of these conclusions lack sufficient justifications as their validity is possibly restricted to the problems, strategies, and parameter values considered in the investigations. Therefore, researchers have developed some techniques to avoid manual tuning of the control parameters. For example, Das et al. [29] linearly reduced the scaling factor with increasing generation count from a maximum to a minimum value, or randomly varied in the range . They also employed a uniform distribution between 0.5 and 1.5 (with a mean value of 1) to obtain a new hybrid DE variant [30]. In addition, several researchers [18], [24]–[26] focused on the adaptation of the control parameters and . Liu and Lampinen introduced fuzzy adaptive differential evolution (FADE) using fuzzy logic controllers whose inputs incorporate the relative function values and individuals of successive generations to adapt the parameters for the mutation and crossover operations [18]. Based on the experimental results on test functions, the FADE algorithm outperformed the conventional DE on higher dimensional problems. Zaharie proposed a parameter adaptation for DE (ADE) based on the idea of controlling the population diversity, and implemented a multipopulation approach [24]. Following the same ideas, Zaharie and Petcu designed an adaptive Pareto DE algorithm for multiobjective optimization and analyzed its parallel implementation [25]. Abbass [26] self-adapted the crossover rate of DE for multiobjective optimization problems, by encoding the crossover rate into each individual, to simultaneously evolve with other parameter. The scaling factor is generated for each variable from a Gaussian distribution . Since our preliminary self-adaptive DE work presented in [19], some new research papers focusing on the adaptation of control parameters in DE were published. Omran et al. [27] introduced a self-adapted scaling factor parameter analogous to the adaptation of crossover rate in [26]. The crossover rate in [27] is generated for each individual from a normal distribution . This approach (called SDE) was tested on four benchmark functions and performed better than other versions of DE. Besides adapting the control parameters or , Teo proposed differential evolution with self adapting populations (DESAP) [22], based on Abbass’s self-adaptive Pareto DE. Recently, Brest et al. [28] encoded control parameters and into the individuals and adjusted by introducing two new parameters and . In their algorithm (called jDE), a set of values were assigned to individuals in the population. Then, a random number rand was uniformly generated in the range of . If , the was reinitialized to a new random value in the range of , otherwise it was kept unchanged. The was adapted in the same manner but with a different reinitialization range of . Brest et al. further compared the performance of several self-adaptive and adaptive DE algorithms in [42]. Recently, the self-adaptive neighborhood search DE algorithm was adopted into a novel cooperative coevolution framework [39]. Moreover, researchers improved the performance of DE by implementing opposition-based learning [40] or local search [41]. IV. SADE ALGORITHM To achieve the most satisfactory optimization performance by applying the conventional DE to a given problem, it is common to perform a trial-and-error search for the most appropriate trial vector generation strategy and fine-tune its associated control parameter values, i.e., the values of , , and . Obviously, it may expend a huge amount of computational costs. Moreover, during different stages of evolution, different trial vector generation strategies coupled with specific control parameter values can be more effective than others. Motivated by these observations, we develop a SaDE algorithm, in which both trial vector generation strategies and their associated control parameter values can be gradually self-adapted according to their previous experiences of generating promising solutions. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:36:41 UTC from IEEE Xplore. Restrictions apply. 402 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 13, NO. 2, APRIL 2009 The core idea behind the proposed SaDE algorithm is elucidated as follows. A. Trial Vector Generation Strategy Adaptation DE realizations employing different trial vector generation strategies usually performs differently when solving different optimization problems. Instead of employing the computationally expensive trial-and-error search for the most suitable strategy and its associated parameter values, we maintain a strategy candidate pool including several effective trial vector generation strategies with effective yet diverse characteristics. During evolution, with respect to each target vector in the current population, one strategy will be chosen from the candidate pool according to a probability learned from its previous experience of generating promising solutions and applied to perform the mutation operation. The more successfully one strategy behaved in previous generations to generate promising solutions, the more probably it will be chosen in the current generation to generate solutions. In the following, we investigate several effective trial vector generation strategies commonly referred to in DE literatures and choose some of them to construct the strategy candidate pool. • Strategies relying on the best solution found so far such as “DE/rand-to-best/1/bin,” “DE/best/1/bin,” and “DE/best/2/ bin,” usually have the fast convergence speed and perform well when solving unimodal problems. However, they are more likely to get stuck at a local optimum and thereby lead to a premature convergence when solving multimodal problems. • The “DE/rand/1/bin” strategy usually demonstrates slow convergence speed and bears stronger exploration capability. Therefore, it is usually more suitable for solving multimodal problems than the strategies relying on the best solution found so far. • The “DE/best/1/bin” strategy is a degenerated case of the “DE/rand-to-best/1/bin” strategy with equal to 1. • Two-difference-vectors-based strategies may result in better perturbation than one-difference-vector-based strategies. Storn [11] claimed that according to the central limit theorem, the random variation of the summation of difference vectors of all target vector pairs in the current population was shifted slightly towards the Gaussian direction, which is the most commonly used mutation operator in EAs. The advantage of using two-difference- vectors-based strategies was also discussed in [35] in the particle swarm optimization (PSO) context, which empirically demonstrated that the statistical distribution of the summation of all one-difference vectors had a triangle shape, while the statistical distribution of the summation of all two-difference vectors had a bell shape that was generally regarded as a better perturbation mode. • DE/current-to-rand/1 is a rotation-invariant strategy. Its effectiveness has been verified when it was applied to solve multiobjective optimization problems [34]. Our preliminary study in [19] only included two trial vector generation strategies into the strategy candidate pool, i.e., “DE/ rand/1/bin” and “DE/rand-to-best/2/bin,” which were frequently employed in many DE literatures. We incorporate two additional strategies: “DE/rand/2/bin” and “DE/current-to-rand/1” into the pool. The former strategy can have a better exploration capability due to the Gaussian-like perturbation while the latter one enables the algorithm to solve rotated problems more effectively. The four trial vector generation strategies constituting the strategy candidate pool in the proposed SaDE algorithm are listed as follows. The binomial-type crossover operator is utilized in the first three strategies due to its popularity in many DE literatures [7], [8], as shown in the equation at the bottom of the page. Generally speaking, a good candidate pool should be restrictive so that the unfavorable influences of less effective strategies can be suppressed. Moreover, a set of effective strategies contained in a good candidate pool should have diverse characteristics, that is, the used strategies should demonstrate distinct capabilities when dealing with a specific problem at different stages of evolution. The theoretical study on the choice of the optimal pool size and the selection of strategies used in the pool are attractive research issues and deserve further investigations. In the SaDE algorithm, with respect to each target vector in the current population, one trial vector generation strategy is selected from the candidate pool according to the probability learned from its success rate in generating improved solutions within a certain number of previous generations. The selected strategy is subsequently applied to the corresponding target vector to generate a trial vector. More specifically, at each generation, the probabilities of choosing each strategy in the candidate pool are summed to 1. These probabilities are DE/rand/1/bin: if or otherwise DE/rand-to-best/2/bin: if or otherwise DE/rand/2/bin: if or otherwise DE/current-to-rand/1: Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:36:41 UTC from IEEE Xplore. Restrictions apply. QIN et al.: DIFFERENTIAL EVOLUTION ALGORITHM WITH STRATEGY ADAPTATION FOR GLOBAL NUMERICAL OPTIMIZATION 403 Fig. 1. Success memory and failure memory. gradually adapted during evolution in the following manner. Assume that the probability of applying the th strategy in the candidate pool to a target vector in the current population is , where is the total number of strategies contained in the pool. The probabilities with respect to each strategy are initialized as , i.e., all strategies have the equal probability to be chosen. We use the stochastic universal selection method [31] to select one trial vector generation strategy for each target vector in the current population. At the generation , after evaluating all the generated trial vectors, the number of trial vectors generated by the th strategy that can successfully enter the next generation is recorded as while the number of trial vectors generated by the th strategy that are discarded in the next generation is recorded as . We introduce success and failure memories to store these numbers within a fixed number of previous generations hereby named learning period (LP). As illustrated in Fig. 1, at the generation , the number of trial vectors generated by different strategies that can enter or fail to enter the next generation over the previous LP generations are stored in different columns of the success and failure memories. Once the memories overflow after LP generations, the earliest records stored in the memories, i.e., or will be removed so that those numbers calculated in the current generation can be stored in the memories, as shown in Fig. 2. After the initial LP generations, the probabilities of choosing different strategies will be updated at each subsequent generation based on the success and failure memories. For example, at the generation , the probability of choosing the th strategy is updated by where (14) where represents the success rate of the trial vectors generated by the th strategy and successfully entering the next generation within the previous LP generations with respect to generation . The small constant value is used to avoid the possible null success rates. To ensure that the probabilities of choosing strategies are always summed to 1, we further divide by to calculate . Obviously, the larger the success rate for the th strategy within the previous LP generations is, the larger the probability of applying it to generate the trial vectors at the current generation is. B. Parameter Adaptation In the conventional DE, the choice of numerical values for the three control parameters , , and highly depends on the problem under consideration. Some empirical guidelines for choosing reasonable parameter settings have been discussed in Section III. In the proposed SaDE algorithm, we leave as a user-specified parameter because it highly replies on the complexity of a given problem. In fact, the population size does not need to be fine-tuned and just a few typical values can be tried according to the preestimated complexity of the given problem. Between other two parameters, is usually more sensitive to problems with different characteristics, e.g., the unimodality and multimodality, while is closely related to the convergence speed. In our SaDE algorithm, the parameter is approximated by a normal distribution with mean value 0.5 and standard deviation 0.3, denoted by . A set of values are randomly sampled from such normal distribution and applied to each target vector in the current population. It is easy to verify that values of must fall into the range with the probability of 0.997. By doing so, we attempt to maintain both exploitation (with small values) and exploration (with large values) power throughout the entire evolution process. Following suggestions in [20], the control parameter in the strategy “DE/current-to-rand/1” is hereby randomly generated within so as to eliminate one additional parameter. As demonstrated by a suite of extensive experiments in [8], the proper choice of can lead to successful optimization performance while a wrong choice may deteriorate the performance. In fact, good values of generally fall into a small range for a given problem, with which the algorithm can perform consistently well. Therefore, we consider gradually adjusting the range of values for a given problem according to previous values that have generated trial vectors successfully entering the next generation. Specifically, we assume that obeys a normal distribution with mean value and standard deviation , denoted by where is initialized as 0.5. The should be set as a small value to guarantee that most values generated by are between , even when is near 0 or 1. Hence, the value of is set as 0.1. Our experiments showed that minor changes to the of the Gaussian distribution do not influence the performance of SaDE significantly. In our preliminary work in [19], the same value was used for all the trial vector generation strategies. However, it is possible that different strategies can perform well by using different ranges of values. Hence, it is reasonable to adapt the value of CRm with respect to each trial vector generation strategy. Without loss of generality, with respect to the th Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:36:41 UTC from IEEE Xplore. Restrictions apply. 404 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 13, NO. 2, APRIL 2009 Fig. 2. Progress of success memory. strategy, the value of is initialized to 0.5. A set of values are randomly generated according to and then applied to those target vectors to which the th strategy is assigned. To adapt the crossover rate , we establish memories named to store those values with respect to the th strategy that have generated trial vectors successfully entering the next generation within the previous LP generations. Specifically, during the first LP generations, values with respect to th strategy are generated by . At each generation after LP generations, the median value stored in will be calculated to overwrite . Then, values can be generated according to when applying the th strategy. After evaluating the newly generated trial vectors, values in that correspond to earlier generations will be replaced by promising values obtained at the current generation with respect to the th strategy. By incorporating the aforementioned trial vector generation strategy and control parameter adaptation schemes into the conventional DE framework, a SaDE algorithm is developed. In the SaDE algorithm, both trial vector generation strategies and their associated parameter values are gradually self-adapted by learning their previous experiences of generating promising solutions. Consequently, a more suitable strategy along with its parameter setting can be determined adaptively to suit different phases of the search process. Extensive experiments described in Section IV verify the promising performance of the SaDE to handle problems with distinct properties such as unimodality and multimodality. The algorithmic description of the SaDE is presented in Table II. V. NUMERICAL EXPERIMENTS AND RESULTS A. Test Functions As discussed in [32], many benchmark numerical functions commonly used to evaluate and compare optimization algorithms may suffer from two problems. First, global optimum lies at the center of the search range. Second, local optima lie along the coordinate axes or no linkage among the variables/dimensions exists. To solve these problems, we can shift or rotate the conventional benchmark functions. For benchmark functions suffering from the first problem, we may shift the global optimum to a random position so that the global optimum position has different numerical values for different dimensions, i.e., , where is the new function, is the old function, is the old global optimum, and is the new global optimum with different values for different dimensions and not lying at the center of the search range. For the second problem, we can rotate the function , where is an TABLE II ALGORITHMIC DESCRIPTION OF SADE orthogonal rotation matrix, to avoid local optima lying along the coordinate axes while retaining the properties of the test function. We hereby shift nine commonly used benchmark Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:36:41 UTC from IEEE Xplore. Restrictions apply. QIN et al.: DIFFERENTIAL EVOLUTION ALGORITHM WITH STRATEGY ADAPTATION FOR GLOBAL NUMERICAL OPTIMIZATION 405 functions , and further rotate three of them . Two composition functions and are chosen from [32], which are constructed by using some basic benchmark functions to obtain more challenging problems. Gaussian function is used to combine the simple benchmark functions and blur the functions’ structures. The composition functions are asymmetrical multimodal problems, with different properties in different areas. The detailed principle of constructing this class of functions is presented in [32], which is not repeated here. In the following, 14 test functions are listed, among which functions are unimodal and functions are multimodal. These 14 test functions are dimensionwise scalable. 1) Shifted sphere function the shifted global optimum 2) Shifted Schwefel’s Problem 1.2 the shifted global optimum 3) Rosenbrock’s function 4) Shifted Schwefel’s Problem 1.2 with noise in fitness the shifted global optimum 5) Shifted Ackley’s function the shifted global optimum 6) Shifted rotated Ackley’s function2 the shifted global optimum 7) Shifted Griewank’s function the shifted global optimum 8) Shifted rotated Griewank’s function the shifted global optimum 9) Shifted Rastrigin’s function the shifted global optimum 10) Shifted rotated Rastrigin’s function the shifted global optimum 11) Shifted noncontinuous Rastrigin’s function for the shifted global optimum 2..\u0002\u0003\u0004\u0005..\u0006 means the condition number of rotation matrix... Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:36:41 UTC from IEEE Xplore. Restrictions apply. 406 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 13, NO. 2, APRIL 2009 TABLE III GLOBAL OPTIMUM, SEARCH RANGES, AND INITIALIZATION RANGES OF THE TEST FUNCTIONS 12) Schwefel’s function 13) Composition function 1 (CF1) in [32]. The function (CF1) is composed by using ten sphere functions. The global optimum is easy to find once the global basin is found. The details of constructing such functions are presented in [32] and [37]. 14) Composition function 6 (CF6) in [32]. The function (CF6) is composed by using ten different benchmark functions, i.e., two rotated Rastrigin’s functions, two rotated Weierstrass functions, two rotated Griewank’s functions, two rotated Ackley’s functions, and two rotated Sphere functions. To make our test suite more comprehensive, we also chose an additional set of 12 test functions from [33] and [38]. 15) Schwefel’s Problem 2.22 16) Schwefel’s Problem 2.21 17) Generalized penalized function 1 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:36:41 UTC from IEEE Xplore. Restrictions apply. QIN et al.: DIFFERENTIAL EVOLUTION ALGORITHM WITH STRATEGY ADAPTATION FOR GLOBAL NUMERICAL OPTIMIZATION 407 18) Generalized penalized function 2 19) Kowalik’s function 20) Six-hump camel-back function 21) Branin function 22) Hartman’s function 1 23) Hartman’s function 2 24) Shekel’s family with and for and B. Algorithms for Comparison Experiments were conducted on a suite of 26 numerical functions to evaluate nine algorithms including the proposed SaDE algorithm. For functions , both 10-dimensional (10-D) and 30-dimensional (30-D) functions were tested. The maximum number of function evaluations (FEs) is set to 100 000 when solving 10-D problems, and 300 000 when solving 30-D counterpart. For the remaining functions , the function dimensions are listed in Table III, and the maximum number of FEs is set to 500 000 [38]. All experiments were run 30 times, independently. The nine algorithms in comparison are listed as follows: • DE/rand/1/bin: ; • DE/rand/1/bin: ; • DE/rand/1/bin: ; • DE/rand-to-best/1/bin: ; • DE/rand-to-best/2/ bin with ; • SaDE algorithm; • adaptive DE algorithm [24]; • SDE algorithm [27]; • jDE [28]. Here, “DE/rand/1/bin” is chosen because it employs a most commonly used trial vector generation strategy, with three commonly suggested sets of control parameters: 1) ; 2) ; and 3) [15], [21], [33]. “DE/rand-to-best/1/bin” and “DE/rand-to-best/2/bin” employ more reliable trial vector generation strategies, and the control parameters are both set as in our experiments [8], [9]. We also choose three most representative adaptive DE variants proposed recently to compare with our proposed SaDE algorithm. The population sizes are all set to 50 and the learning periods are 50 for both 10-D and 30-D functions. C. Experimental Results and Discussions Tables IV and V report the mean and standard deviation of function values as well as the success rates by applying the nine algorithms to optimize the 10-D and 30-D numerical functions , respectively. The best results are typed in bold. The success of an algorithm means that this algorithm can result in a function value no worse than the prespecified optimal value, i.e., for all problems with the number of FEs less than the prespecified maximum number. The success rate is calculated as the number of successful runs divided by the total number of runs. Figs. 3 and 4 illustrate the convergence characteristics in terms of the best fitness value of the median run of each algorithm for functions – with . Because the convergence graphs of the 30-D problems are similar to their 10-D counterparts, they are omitted here. For functions – , because most algorithms can locate the global optima with 100% success rate within the specified maximum FEs, it is unnecessary to present the mean and standard deviation of function values. Instead, to compare the convergence speed, we report the average number of function evaluations (NFE) required to find the global optima when an algorithm solves the problem with 100% success rate. 1) Comparing SaDE With Conventional DE: In this section, we intend to show how well the proposed SaDE algorithm performs when compared to the conventional DE. For the 10-D – , Tables IV and VI show that and are easily optimized by fiveDE variants and the SaDE algorithm with 100% success rate. Except these two functions, the Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:36:41 UTC from IEEE Xplore. Restrictions apply. 408 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 13, NO. 2, APRIL 2009 TABLE IV RESULTS FOR 10-D PROBLEMS performances of DE/rand/1 and DE/rand/1 are almost contrary. For example, DE/rand/1/bin obtains better results on unimodal functions and where DE/rand/1/bin performs poorly. On the contrary, DE/rand/1/bin performs efficiently and robustly on multimodal functions , and , while DE/rand/1/bin fails totally on , and , only TABLE V RESULTS FOR 30-D PROBLEMS 13% success rate on . Therefore, different values of for DE/rand/1/bin demonstrate diverse performances on different problems. DE/rand-to-best/1/bin and DE/rand-to-best/2/bin are more reliable than DE/rand/1/bin. However, they also totally or partially fail in optimizing some problems where SaDE can locate the global optima in every run. Overall, SaDE obtains a smaller mean value and a higher success rate than the five DE variants for all problems. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:36:41 UTC from IEEE Xplore. Restrictions apply. QIN et al.: DIFFERENTIAL EVOLUTION ALGORITHM WITH STRATEGY ADAPTATION FOR GLOBAL NUMERICAL OPTIMIZATION 409 Fig. 3. Median convergence characteristics of rand/1 ..\u0002 \u0003 \u0004..\u0005, rand/1 ..\u0002 \u0003 \u0004..\u0006, rand/1 \u0002 \u0003 \u0004..\u0007\u0003..\u0002 \u0003 \u0004..\b, rand-to-best/1, rand-to-best/2, and SaDE on 10-D test functions \u0004 –\u0004 . (a) \u0004 : sphere; (b) \u0004 : Schwefel’s 1.2; (c) \u0004 : Rosenbrock; (d) \u0004 : Schwefel’s Problem 1.2 with noise in fitness; (e) \u0004 : Ackley; (f) \u0004 : rotated Ackley; (g) \u0004 : Griewank; (h) \u0004 : rotated Griewank; (i) \u0004 : Rastrigin; (j) \u0004 : rotated Rastrigin; (k) \u0004 : noncontinuous Rastrigin; (l) \u0004 : Schwefel; (m) \u0004 : composition function 1; and (n) \u0004 : composition function 6. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:36:41 UTC from IEEE Xplore. Restrictions apply. 410 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 13, NO. 2, APRIL 2009 Fig. 4. Median convergence characteristics of SaDE, ADE (Zaharie), SDE, and jDE on 10-D test functions .. –.. . (a) .. : sphere; (b) .. : Schwefel’s 1.2; (c) .. : Rosenbrock; (d) .. : Schwefel’s Problem 1.2 with noise in fitness; (e) .. : Ackley; (f) .. : rotated Ackley; (g) .. : Griewank; (h) .. : rotated Griewank; (i) .. : Rastrigin; (j) .. : rotated Rastrigin; (k) .. : noncontinuous Rastrigin; (l) .. : Schwefel; (m) .. : composition function 1; and (n) .. : composition function 6. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:36:41 UTC from IEEE Xplore. Restrictions apply. QIN et al.: DIFFERENTIAL EVOLUTION ALGORITHM WITH STRATEGY ADAPTATION FOR GLOBAL NUMERICAL OPTIMIZATION 411 TABLE VI COMPARISON OF  AND SADE Furthermore, the convergence map of DE/rand/1/bin , DE/rand/1/bin , DE/rand/1/bin , DE/rand-to-best/1/bin , DE/rand-to-best/2/bin , and SaDE in Fig. 3 shows that the SaDE algorithm always converges faster than others on six problems , and , while slightly slower than DE/rand-to-best/1/bin or DE/rand/1/bin on the remaining problems. It can be observed that on DE/rand/1/bin , DE/rand-to-best/1/bin and DE/rand/1/bin converge fast, followed by SaDE. This is because a small value ( or ) is an effective value to optimize Rastrigin problem, while our SaDE with an initial needs some generations to self-adapt the parameters to suitable values. This issue will be discussed in Section V-D. For the 30-D problems – , DE/rand/1/bin has great difficulty in finding the global optima on all problems. DE/rand/1/bin yields 100% success rate for , and . The SaDE algorithm performs much better with success rates of 100% on most problems, and 90% on where other DE variants totally fail in finding the global optima. When the dimension of variable was increased to 30, some problems such as functions , and become so difficult that all the six approaches could not find the optimal solutions within the maximum FEs. Regarding the speed of Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:36:41 UTC from IEEE Xplore. Restrictions apply. 412 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 13, NO. 2, APRIL 2009 Fig. 5. Empirical distribution of normalized success performance on all 26 test problems. different algorithms in reaching the optimal solutions, we further compare the NFEs on problems with 100% success rate in Table VI, and find that SaDE converges the fastest among these algorithms on eight problems except . Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:36:41 UTC from IEEE Xplore. Restrictions apply. QIN et al.: DIFFERENTIAL EVOLUTION ALGORITHM WITH STRATEGY ADAPTATION FOR GLOBAL NUMERICAL OPTIMIZATION 413 TABLE VII RESULTS OF SADE AND FADE ALGORITHMS TABLE VIII SADE’S RESULTS WITH DIFFERENT LEARNING PERIOD (LP) ON TEST FUNCTIONS .. .. \u0002 , AND \u0002 (10-D) For the test problems – , both SaDE and DE/rand/1/bin successfully solve all problems in each run, while SaDE shows an overall better convergence speed than DE/rand/1/bin . 2) Comparing SaDE With Other Adaptive DE Variants: The performance of the SaDE is compared with three other adaptive DE variants: ADE [24], SDE [27], and jDE [28] on 10-D and 30-D problems – . For 10-D problems, the best results of unimodal functions – are obtained by SaDE and jDE, where SaDE demonstrates a slight superiority on efficiency, which can be observed from the convergence graphs [Fig. 4(a)–(d)]. Among the multimodal functions, Ackley and rotated Ackley are easily solved by all the adaptive DE variants with 100% success rate. For Griewank and Rastrigin, before rotation, SaDE, ADE, and jDE have high success rates as shown in Table IV. After rotation ( and ), only SaDE and jDE successfully find the global optimum on in some run, and SaDE offers a higher success rate than jDE. is so difficult that no algorithm could find the global optimum. For the last two composition problems and , SaDE is able to locate the optimal solutions with smaller standard deviations and higher success rates, demonstrating better efficiency and stability than the other three algorithms. From the results of 30-D problems shown in Table V, we can observe that the algorithms achieved similar ranking as in the 10-D problems, and were not very successful in optimizing , and .However, with respect to the mean and standard deviations, SaDE obtains smallest values on and , and the second smallest values on where jDE gets the smallest values. 3) Overall Performance Comparison: In this part, we intend to further compare the overall performances of nine algorithms on all functions by plotting empirical distribution of normalized success performance [36]. The success performance is defined as success performance (SP) mean (FEs for successful runs)*(# of total runs)/(# of successful runs). We first calculated the success performance of nine algorithms on each test function, and then normalized the SP by dividing all SPs by the SP of the best algorithm on the respective function. Results of all functions are used where at least one algorithm was successful in at least one run. Therefore, for 10-D – problems, we plot the results of all functions except , and exclude functions , and for 30-D – problems. The test problems – are all plotted. Small values of SP and large values of the empirical distribution in graphs are preferable. The first one that reaches the top of the graph will be regarded as the best algorithm. From Fig. 5, we can observe that SaDE outperforms other approaches on overall performance on test problems. 4) ComparisonWith FADE: The FADE algorithm was tested on a set of standard test functions in [18], including 2, 3, 30, and 50 dimensions. Because the low-dimensional (2 or 3) test functions are easy to solve for both conventional DE and FADE, we hereby only compare our SaDE with the FADE algorithm on 50-D test functions chosen from [18]. The parameter settings are the same as in [18]: population size , and maximum generations are listed in Table VII. The averaged results of 100 independent runs are summarized in the table (results for FADE are taken from [18]), which show that the proposed SaDE algorithm obviously performs better than the FADE algorithm. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:36:41 UTC from IEEE Xplore. Restrictions apply. 414 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 13, NO. 2, APRIL 2009 Fig. 6. Self-adaptation characteristics of ..\u0002\u0003 on Rosenbrock and Rastrigin functions (10-D). (a) Rosenbrock function. (b) Rastrigin function. D. Analysis of Self-Adaptation Property of SaDE 1) Self-Adaptation of Crossover Probability: Usually, when DE solves problems, there is no specific value of and , which is effective in all search phases. Instead, possibly several combinations of different and values can be effective in different search phases. However, Rosenbrock and Rastrigin function are exceptional. From the experiments of DE/rand/1/bin on these two functions, we know that usually DE/rand/1/bin with a large could obtain a good result on Rosenbrock function and a small is beneficial to Rastrigin function, which could also be concluded from comparing the results of DE/rand/1/bin and . As we know the suitable values for Rosenbrock and Rastrigin functions, we can observe the variation of values in SaDE algorithm to check whether the self-adaptation of is effective. Because the value in SaDE is mostly depended on the mean value of normal distribution, we plot the value Fig. 7. Self-adaptation characteristics of strategies (10-D). (a) Griewank. (b) Rotated Griewank. when the SaDE algorithm optimized Rosenbrock and Rastrigin functions as the generation increases in Fig. 6.We found that for Rosenbrock function, the CRm values of strategy “rand/1/bin,” “rand-to-best/2/bin,” and “rand/2/bin” keep increasing during evolution, as we expected. On the contrary, for Rastrigin function, the values of three strategies all keep decreasing during evolution. Therefore, we can say that our proposed SaDE algorithm self-adapts the crossover probability effectively. 2) Self-Adaptation of Trial Vector Generation Strategy: To investigate the self-adaptive selection of trial vector generation strategy in SaDE algorithm, we plot the variations of the four different strategies’ probabilities as the evolution progresses. As we know that the strategy “current-to-rand” is rotationally invariant [20] and has superior performance on rotated problem as stated in Section II, this strategy should occupy more proportion if it yields good results when dealing with rotated problems. As shown in Fig. 7, for Griewank’s function, the strategy “current- to-rand” occupied a very small proportion during the whole evolution progress. At the beginning, we assign each strategy equal probability. Because the strategy “current-to-rand” could Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:36:41 UTC from IEEE Xplore. Restrictions apply. QIN et al.: DIFFERENTIAL EVOLUTION ALGORITHM WITH STRATEGY ADAPTATION FOR GLOBAL NUMERICAL OPTIMIZATION 415 Fig. 8. SaDE’s results with different learning period LP on 11 test functions (10-D). not yield better results, our self-adaptive mechanism of learning is unlikely to choose the strategy “current-to-rand” in next few generations. As this strategy always performed badly, its proportion almost lowered to 0. The individuals were mostly mutated by the other three strategies that always occupy proportions above 0.25 as shown in Fig. 7. After 800 generations, the algorithm converged, and these four strategies performed competitively so as to all occupy around 0.25 proportion. On the contrary, after we rotated the Griewank’s function, the strategy “current-to-rand” demonstrated a good performance at the be- Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:36:41 UTC from IEEE Xplore. Restrictions apply. 416 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 13, NO. 2, APRIL 2009 ginning so as to occupy a higher proportion in 300 generations. During about 300–550 generations, the proportion of strategy “current-to-rand” slightly decreased under 0.25 because relatively inferior solutions are found by this strategy during this stage of evolution. After 550 generations, strategy “current-torand” occupied a higher proportion again until the algorithm converged and four strategies were almost stable. E. Parameter Study 1) Learning Period (LP): The learning period parameter LP needs to be optimized. In this section, we use 10-D test functions – to investigate the impact of this parameter on SaDE algorithm. The SaDE algorithm runs 30 times on each function with five different learning periods LP of 20, 30, 40, 50, and 60. Because functions , and are solved with 100% success rate with five different LP values, we here omit the results of mean and standard deviation for these functions; only the results of , and are shown in Table VIII. However, for functions , and , we investigate the influence of LPs on convergence speed of SaDE algorithm, by comparing the FEs that SaDE algorithm cost to obtain the global optimum with different LPs. Fig. 8 shows the box plots of FEs that SaDE algorithm requires to reach the global optimum with five different LPs. The box has lines at the lower quartile, median, and upper quartile values. The whiskers are lines extending from each end of the box to show the extent of the remaining data. Outliers are data with values beyond the ends of the whiskers. If there is no data outside the whisker, a dot is placed at the bottom whisker. From Fig. 8, we found that SaDE algorithm succeeds on all functions with similar FEs by using five different LPs. Therefore, the convergence speed of SaDE algorithm is less sensitive to the parameter LP parameter values between 20 and 60. VI. CONCLUSION This paper presented a SaDE algorithm, which eliminates the time-consuming exhaustive search for the most suitable trial vector generation strategy and the associated control parameters and . In SaDE, trial vector generation strategies together with their two control parameters will be probabilistically assigned to each target vector in the current population according to the probabilities gradually learned from the experience to generate improved solutions. We have investigated the self-adaptive characteristics of the value and trial vector generation strategies. Experiments showed that the SaDE algorithm could evolve suitable strategies and parameter values as evolution progresses. The sensitivity analysis of LP parameter indicated that it had insignificant impact on the performance of the SaDE. We have compared the performance of SaDE with the conventional DE and three adaptive DE variants over a suite of 26 bound constrained numerical optimization problems, and concluded that SaDE was more effective in obtaining better quality solutions, which are more stable with the relatively smaller standard deviation, and had higher success rates. REFERENCES [1] A. Tuson and P. Ross, “Adapting operator settings in genetic algorithms,” Evolut. Comput., vol. 6, no. 2, pp. 161–184, 1998. [2] J. Gomez, D. Dasgupta, and F. Gonzalez, “Using adaptive operators in genetic search,” in Proc. Genetic Evolut. Comput. Conf., Chicago, IL, Jul. 2003, pp. 1580–1581. [3] B. R. Julstrom, “What have you done for me lately? Adapting operator probabilities in a steady-state genetic algorithm,” in Proc. 6th Int. Conf. Genetic Algorithms, Pittsburgh, PA, Jul. 15–19, 1995, pp. 81–87. [4] P. J. Angeline, “Adaptive and self-adaptive evolutionary computation,” in Computational Intelligence: A Dynamic System Perspective, M. Palaniswami, Y. Attikiouzel, R. J. Marks, D. Fogel, and T. Fukuda, Eds. New York: IEEE Press, 1995, pp. 152–161. [5] J. E. Smith and T. C. Fogarty, “Operator and parameter adaptation in genetic algorithms,” Soft Comput., vol. 1, no. 2, pp. 81–87, Jun. 1997. [6] A. E. Eiben, R. Hinterding, and Z. Michalewicz, “Parameter control in evolutionary algorithms,” IEEE Trans. Evolut. Comput., vol. 3, no. 2, pp. 124–141, Jul. 1999. [7] R. Storn and K. V. Price, “Differential evolution-A simple and efficient heuristic for global optimization over continuous Spaces,” J. Global Optim., vol. 11, pp. 341–359, 1997. [8] K. Price, R. Storn, and J. Lampinen, Differential Evolution—A Practical Approach to Global Optimization. Berlin, Germany: Springer- Verlag, 2005. [9] V. Feoktistov, Differential Evolution: In Search of Solutions. Berlin, Germany: Springer-Verlag, 2006. [10] J. Ilonen, J.-K. Kamarainen, and J. Lampinen, “Differential evolution training algorithm for feed-forward neural networks,” Neural Process. Lett., vol. 7, no. 1, pp. 93–105, 2003. [11] R. Storn, “On the usage of differential evolution for function optimization,” in Proc. Biennial Conf. North Amer. Fuzzy Inf. Process. Soc., Berkeley, CA, 1996, pp. 519–523. [12] R. Storn, “Differential evolution design of an IIR-filter,” in Proc. IEEE Int. Conf. Evolut. Comput., Nagoya, Japan, May 1996. [13] T. Rogalsky, R. W. Derksen, and S. Kocabiyik, “Differential evolution in aerodynamic optimization,” in Proc. 46th Annu. Conf. ofCan. Aeronaut. Space Inst., Montreal, QC, Canada, May 1999, pp. 29–36. [14] R. Joshi and A. C. Sanderson, “Minimal representation multisensor fusion using differential evolution,” IEEE Trans. Syst. Man Cybern. A, Syst. Humans, vol. 29, no. 1, pp. 63–76, Jan. 1999. [15] R. Storn and K. Price, “Differential evolution-a simple and efficient adaptive scheme for global optimization over continuous spaces,” TR-95-012, 1995 [Online]. Available: http://http.icsi.berkeley.edu/ ~storn/litera.html [16] J. Lampinen and I. Zelinka, “On stagnation of the differential evolution algorithm,” in Proc. 6th Int. Mendel Conf. Soft Comput., P. O.smera, Ed., 2002, pp. 76–83. [17] R. Gämperle, S. D. Müller, and P. Koumoutsakos, “A parameter study for differential evolution,” in Advances in Intelligent Systems, Fuzzy Systems, Evolutionary Computation, A. Grmela and N. E. Mastorakis, Eds. Interlaken, Switzerland: WSEAS Press, 2002, pp. 293–298. [18] J. Liu and J. Lampinen, “A fuzzy adaptive differential evolution algorithm,” Soft Comput., vol. 9, no. 6, pp. 448–462, Apr. 2005. [19] A. K. Qin and P. N. Suganthan, “Self-adaptive differential evolution algorithm for numerical optimization,” in Proc. IEEE Congr. Evolut. Comput., Edinburgh, Scotland, Sep. 2005, pp. 1785–1791. [20] K. V. Price, “An introduction to differential evolution,” in New Ideas in Optimization, D. Corne, M. Dorigo, and F. Glover, Eds. London, U.K.: McGraw-Hill, 1999, pp. 79–108. [21] J. Rönkkönen, S. Kukkonen, and K. V. Price, “Real-parameter optimization with differential evolution,” in Proc. IEEE Congr. Evolut. Comput., Edinburgh, Scotland, Sep. 2005, pp. 506–513. [22] J. Teo, “Exploring dynamic self-adaptive populations in differential evolution,” Soft Comput., vol. 10, no. 8, pp. 637–686, 2006. [23] M. M. Ali and A. Torn, “Population set-based global optimization algorithms: Some modifications and numerical studies,” Comput. Operat. Res., vol. 31, no. 10, pp. 1703–1725, 2004. [24] D. Zaharie, “Control of population diversity and adaptation in differential evolution algorithms,” in Proc. Mendel 9th Int. Conf. Soft Comput., R. Matousek and P. Osmera, Eds., Brno, Czech Republic, Jun. 2003, pp. 41–46. [25] D. Zaharie and D. Petcu, “Adaptive pareto differential evolution and its parallelization,” in Proc. 5th Int. Conf. Parallel Process. Appl. Math., Czestochowa, Poland, Sep. 2003, pp. 261–268. [26] H. A. Abbass, “The self-adaptive Pareto differential evolution algorithm,” in Proc. Congr. Evolut. Comput., Honolulu, HI, May 2002, pp. 831–836. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:36:41 UTC from IEEE Xplore. Restrictions apply. QIN et al.: DIFFERENTIAL EVOLUTION ALGORITHM WITH STRATEGY ADAPTATION FOR GLOBAL NUMERICAL OPTIMIZATION 417 [27] M. G. H. Omran, A. Salman, and A. P. Engelbrecht, “Self-adaptive differential evolution,” in Lecture Notes in Artificial Intelligence. Berlin, Germany: Springer-Verlag, 2005, pp. 192–199. [28] J. Brest, S. Greiner, B. Boskovic, M. Mernik, and V. Zumer, “Self-adapting control parameters in differential evolution: A comparative study on numerical benchmark problems,” IEEE Trans. Evolut. Comput., vol. 10, no. 6, pp. 646–657, Dec. 2006. [29] S. Das, A. Konar, and U. K. Chakraborty, “Two improved differential evolution schemes for faster global search,” in ACM-SIGEVO Proc. Genetic Evolut. Comput. Conf., Washington, DC, pp. 991–998. [30] U. K. Chakraborty, S. Das, and A. Konar, “Differential evolution with local neighborhood,” in Proc. Congr. Evolut. Comput., Vancouver, BC, Canada, 2006, pp. 2042–2049. [31] J. E. Baker, “Reducing bias and inefficiency in the selection algorithm,” in Proc. 2nd Int. Conf. Genetic Algorithms, Cambridge, MA, 1987, pp. 14–21. [32] J. J. Liang, P. N. Suganthan, and K. Deb, “Novel composition test functions for numerical global optimization,” in Proc. IEEE Swarm Intell. Symp., Pasadena, CA, Jun. 2005, pp. 68–75. [33] J. Vesterstrøm and R. Thomson, “A comparative study of differential evolution, particle swarm optimization, and evolutionary algorithms on numerical benchmark problems,” in Proc. IEEE Congr. Evolut. Comput., Portland, OR, June 2004, pp. 1980–1987. [34] A. Iorio and X. Li, “Solving rotated multi-objective optimization problems using differential evolution,” in Proc. Australian Conf. Artif. Intell., Cairns, Dec. 2004, pp. 861–872. [35] W. J. Zhang and X. F. Xie, “DEPSO: Hybrid particle swarm with differential evolution operator,” in Proc. IEEE Int. Conf. Syst. Man Cybern., Washington, DC, 2003, pp. 3816–3821. [36] N. Hansen, “Compilation of results on the 2005 CEC benchmark function set,” May 4, 2006 [Online]. Available: http://www.ntu.edu.sg/ home/epnsugan/index_files/CEC-05/compareresults.pdf [37] P. N. Suganthan, N. Hansen, J. J. Liang, K. Deb, Y.-P. Chen, A. Auger, and S. Tiwari, “Problem definitions and evaluation criteria for the CEC 2005 special session on real-parameter optimization,” Nanyang Technol. Univ., Singapore, Tech. Rep. KanGAL #2005005, May 2005, IIT Kanpur, India. [38] X. Yao, Y. Liu, and G. Lin, “Evolutionary programming made faster,” IEEE Trans. Evolut. Comput., vol. 3, no. 2, pp. 82–102, Jul. 1999. [39] Z. Y. Yang, E. K. Tang, and X. Yao:, “Large scale evolutionary optimization using cooperative coevolution,” Inf. Sci., accepted for publication. [40] S. Rahnamayan, H. R. Tizhoosh, and M. M. A. Salama, “Oppositionbased differential evolution,” IEEE Trans. Evolut. Comput., vol. 12, no. 1, pp. 64–79, Feb. 2008. [41] N. Noman and H. Iba, “Accelerating differential evolution using an adaptive local search,” IEEE Trans. Evolut. Comput., vol. 12, no. 1, pp. 107–125, Feb. 2008. [42] J. Brest, B. Boskovic, S. Greiner,V. Zumer, and M. S. Maucec, “Performance comparison of self-adaptive and adaptive differential evolution algorithms,” Soft Comput., vol. 11, no. 7, pp. 617–629, May 2007. A. K. Qin received his B.E. Degree from Department of Automatic Control Engineering of Southeast University, Nanjing, P.R. China in 2001. He completed his Ph.D. degree in the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore in 2007. His research interests include pattern recognition, machine learning, neural network, genetic and evolutionary algorithms, computer vision and bioinformatics. V. L. Huang received the B. E. degree from Huazhong University of Sci. & Tech. Wuhan, P. R. China in 2002. She has been worked toward the Ph.D. degree in the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore since 2004. Her research interests include evolutionary algorithms, differential evolution, and applications of evolutionary algorithms. P. N. Suganthan received the B.A degree, Postgraduate Certificate and M.A. degree in electrical and information engineering from the University of Cambridge, UK in 1990, 1992 and 1994, respectively. He obtained his Ph.D. degree from the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore in 1996. He was a predoctoral Research Assistant in the Department of Electrical Engineering, University of Sydney in 1995–96 and a lecturer in the Department of Computer Science and Electrical Engineering, University of Queensland in 1996–99. Between 1999 and 2003, he was an Assistant Professor in the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore where he is now an Associate Professor. He is an associate editor of the IEEE Transactions on Evolutionary Computation and Pattern Recognition Journal. His research interests include evolutionary computation, applications of evolutionary computation, neural networks, pattern recognition and bioinformatics. He is a senior member of the IEEE.\"],\n",
    "[9,3,1,\"646 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 6, DECEMBER 2006 Self-Adapting Control Parameters in Differential Evolution: A Comparative Study on Numerical Benchmark Problems Janez Brest, Member, IEEE, Sa¡so Greiner, Borko Bo¡skovic´, Marjan Mernik, Member, IEEE, and Viljem ¡ Zumer, Member, IEEE Abstract—We describe an efficient technique for adapting control parameter settings associated with differential evolution (DE). The DE algorithm has been used in many practical cases and has demonstrated good convergence properties. It has only a few control parameters, which are kept fixed throughout the entire evolutionary process. However, it is not an easy task to properly set control parameters in DE. We present an algorithm—a new version of the DE algorithm—for obtaining self-adaptive control parameter settings that showgood performance on numerical benchmark problems. The results show that our algorithm with self-adaptive control parameter settings is better than, or at least comparable to, the standard DE algorithm and evolutionary algorithms from literature when considering the quality of the solutions obtained. Index Terms—Adaptive parameter control, differential evolution (DE), evolutionary optimization. I. INTRODUCTION DIFFERENTIAL evolution (DE) is a simple yet powerful evolutionary algorithm (EA) for global optimization introduced by Price and Storn [1]. The DE algorithm has gradually become more popular and has been used in many practical cases, mainly because it has demonstrated good convergence properties and is principally easy to understand [2]. EAs [3] are a broad class of stochastic optimization algorithms inspired by biology and, in particular, by those biological processes that allow populations of organizms to adapt to their surrounding environments: genetic inheritance and survival of the fittest. EAs have a prominent advantage over other types of numerical methods. They only require information about the objective function itself, which can be either explicit or implicit. Other accessory properties such as differentiability or continuity are not necessary. As such, they are more flexible in dealing with a wide spectrum of problems. When using an EA, it is also necessary to specify how candidate solutions will be changed to generate new solutions [4]. EA may have parameters, for instance, the probability of mutation, the tournament size of selection, or the population size. Manuscript received June 14, 2005; revised September 19, 2005 and November 9, 2005. This work was supported in part by the Slovenian Research Agency under Programme P2-0041, Computer Systems, Methodologies, and Intelligent Services. The authors are with the Computer Architecture and Languages Laboratory, Institute of Computer Science, Faculty of Electrical Engineering and Computer Science, University of Maribor, SI-2000 Maribor, Slovenia (e-mail: janez.brest@uni-mb.si; saso.greiner@uni-mb.si; borko.boskovic@uni-mb.si; marjan.mernik@uni-mb.si; zumer@uni-mb.si). Digital Object Identifier 10.1109/TEVC.2006.872133 The values of these parameters greatly determine the quality of the solution obtained and the efficiency of the search [5]–[7]. Starting with a number of guessed solutions, the multipoint algorithm updates one or more solutions in a synergistic manner in the hope of steering the population toward the optimum [8], [9]. Choosing suitable parameter values is, frequently, a problemdependent task and requires previous experience of the user. Despite its crucial importance, there is no consistent methodology for determining the control parameters of an EA, which are, most of the time, arbitrarily set within some predefined ranges [4]. In their early stage, EAs did not usually include control parameters as a part of the evolving object but considered them as external fixed parameters. Later, it was realized that in order to achieve optimal convergence, these parameters should be altered in the evolution process itself [5], [7]. The control parameters were adjusted over time by using heuristic rules, which take into account information about the progress achieved. However, heuristic rules, which might be optimal for one optimization problem, might be inefficient or even fail to guarantee convergence for another problem. A logical step in the development of EAs was to include control parameters into the evolving objects and allow them to evolve along with the main parameters [3], [10], [11]. Globally, we distinguish two major forms of setting parameter values: parameter tuning and parameter control. The former means the commonly practiced approach that tries to find good values for the parameters before running the algorithm, then tuning the algorithm using these values, which remain fixed during the run. The latter means that values for the parameters are changed during the run. According to Eiben et al. [5], [7], the change can be categorized into three classes. 1) Deterministic parameter control takes place when the value of a parameter is altered by some deterministic rule. 2) Adaptive parameter control is used to place when there is some form of feedback from the search that is used to determine the direction and/or the magnitude of the change to the parameter. 3) Self-adaptive parameter control is the idea that “evolution of the evolution” can be used to implement the self-adaptation of parameters. Here, the parameters to be adapted are encoded into the chromosome (individuals) and undergo the actions of genetic operators. The better values of these encoded parameters lead to better individuals which, in turn, are more likely to survive and produce offspring and, hence, propagate these better parameter values. 1089-778X/$20.00 © 2006 13:37:02 UTC from IEEE Xplore. Restrictions apply. BREST et al.: SELF-ADAPTING CONTROL PARAMETERS IN DIFFERENTIAL EVOLUTION 647 Hence, it is seemingly natural to use an EA, not only for finding solutions to a problem but also for tuning the (same) algorithm to the particular problem. Technically speaking, we are trying to modify the values of parameters during the run of the algorithm by taking the actual search progress into account. As discussed in [5] and [7], there are two ways to do this. The first way is to use some heuristic rule which takes feedback from the current state of the search and modifies the parameter values accordingly (adaptive parameter control), such as the credit assignment process presented by [12]. A second way is to incorporate parameters into the chromosomes, thereby making them subject to evolution (self-adaptive parameter control) [13]. The proof of convergence of EAs with self-adaptation is difficult because control parameters are changed randomly and the selection does not affect their evolution directly [14], [15]. Since DE is a particular instance of EA, it is interesting to investigate how self-adaptivity can be applied to it. Until now, no research work on self-adaptivity in DE has been reported. First, we define a type of optimization problem. In this paper, we will only concern ourselves with those optimization methods that use an objective function. In most cases, the objective function defines the optimization problem as a minimization task. To this end, the following investigation is further restricted to the minimization of problems. When the objective function is nonlinear and nondifferentiable, direct search approaches are the methods of choice [1]. In optimizing a function, an optimization algorithm aims to find such that , , where does not need to be continuous but must be bounded. This paper only considers unconstrained function optimization. DE is a floating point encoding an EA for global optimization over continuous spaces [2], [16], [17]. DE creates new candidate solutions by combining the parent individual and several other individuals of the same population. A candidate replaces the parent only if it has better fitness. DE has three parameters: amplification factor of the difference vector , crossover control parameter , and population size . DE is also particularly easy to work with, having only a few control parameters, which are kept fixed throughout the entire optimization process [2], [16], [18]. Since the interaction of control parameters with the DE’s performance is complex in practice, a DE user should select the initial parameter settings for the problem at hand from previous experiences or from literature. Then, the trial-and-error method has to be used for fine tuning the control parameters further. In practice, the optimization run has to be performed multiple times with different settings. In some cases, the time for finding these parameters is unacceptably long. In our paper, the parameter control technique is based on the self-adaptation of two parameters ( and ), associated with the evolutionary process. The main goal here is to produce a flexible DE, in terms of control parameters and . This paper introduces a novel approach to the self-adapting control parameter of DE. It gives some comparisons against several adaptive and nonadaptive methods for a set of test functions. The paper is organized as follows. Related work is described in Section II. The DE is briefly presented in Section III. Some suggested choices for the fixed settings of the control parameters from literature are collected in Section IV. In Section V, the proposed new version of the DE algorithm with self-adapted control parameters is described in detail. Benchmark functions are presented in Section VI. Experiments are then presented in Section VII. A comparison of the self-adaptive DE and DE algorithms with other EP algorithms is made, followed by an experiment on the parameter settings for the DE algorithm. Then experiments with and values by the adaptive DE are presented, and finally a comparison of self-adaptive DE algorithm with fuzzy adaptive differential evolution algorithm is shown. In conclusion, some remarks are given in Section VIII. II. RELATED WORK This section reviews papers that already compare DE with other instances of EAs, such as particle swarm optimization and genetic algorithms, as well as papers that compare a different extension of DE with the original DE. After that, we concentrate on papers that deal with parameter control in DE. In the end, we mention papers on EA that use similar benchmark functions as presented in this paper. DE was proposed by Price and Storn [1], [18]. It is a very simple and straightforward strategy. Vesterstroem et al. [19] compared the DE algorithm with particle swarm optimization (PSO) and EAs on numerical benchmark problems. DE outperformed PSO and EAs in terms of the solution’s quality on most benchmark problems. The benchmark functions in [19] are similar to benchmark functions used in our paper. Ali and Törn in [9] proposed new versions of the DE algorithm and also suggested some modifications to classical DE to improve its efficiency and robustness. They introduced an auxiliary population of individuals alongside the original population (noted in [9], a notation using sets is used—population set-based methods). Next, they proposed a rule for calculating the control parameter automatically (see Section IV). Sun et al. [20] proposed a combination of DE algorithms and the estimation of distribution algorithm (EDA), which tries to guide its search toward a promising area by sampling new solutions from a probability model. Based on experimental results, it has been demonstrated that the DE/EDA algorithm outperforms the DE algorithm and the EDA. There are quite different conclusions about the rules for choosing the control parameters of DE. In [21], it is stated that the control parameters of DE are not difficult to choose. On the other hand, Gämperle et al. [22] reported that choosing the proper control parameters for DE is more difficult than expected. Liu and Lampinen [2] reported that effectiveness, efficiency, and robustness of the DE algorithm are sensitive to the settings of the control parameters. The best settings for the control parameters can be different for different functions and the same function with different requirements for consumption time and accuracy. However, there still exists a lack of knowledge on how to find reasonably good values for the control parameters of DE for a given function [16]. Liu and Lampinen [16] proposed a new version of DE, where the mutation control parameter and the crossover control parameter are adaptive. It is called the Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:37:02 UTC from IEEE Xplore. Restrictions apply. 648 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 6, DECEMBER 2006 fuzzy adaptive differential evolution (FADE) algorithm. It dynamically controls DE parameters and/or . The FADE algorithm, especially when adapting and , converges much faster than the traditional DE, particularly when the dimensionality of the problem is high or the problem concerned is complicated [16]. In this paper, we compare our version of a self-adaptive DE with the classical DE algorithm and with the FADE algorithm. A performance comparison is also made with EP algorithms, described in the following. In [23], a “fast EP” (FEP) is proposed which uses a Cauchy, instead of Gaussian, mutation as the primary search operator. In [24], a further generalization of FEP is described by using mutation based on the Lévy probability distribution.With Lévy probability distribution, one can extend and generalize FEP because the Cauchy probability distribution is a special case of the Lévy probability distribution. The large variation at a single mutation enables Lévy mutation to discover a wider region of the search space globally [24]. The Lévy-mutated variables cover a wider range than those mutated by Gaussian distributions. Large variations of the mutated offspring can help to escape from local optima. Finally, we give two more references which have dealt with function optimizations evaluated on some similar benchmark test functions. Tu et al. [25] suggest the use of the stochastic genetic algorithm (StGA), where the stochastic coding strategy is employed. The search space is explored region by region. Regions are dynamically created using a stochastic method. In each region, a number of children are produced through random sampling, and the best child is chosen to represent the region. The variance values are decreased if at least one of five generated children results in improved fitness; otherwise, the variance values are increased. The StGA codes each chromosome as a representative of a stochastic region described by a multivariate Gaussian distribution rather than a single candidate solution, as in the conventional GA. The paper [26] presents a technique for adapting control parameter settings associated with genetic operators using fuzzy logic controllers and coevolution. III. DE ALGORITHM There are several variants of DE[1], [18]. In this paper,we use the DE scheme which can be classified using notation [1], [18] as DE/rand/1/bin strategy. This strategy is the most often used in practice [1], [2], [20], [22] and can be described as follows. A set of optimization parameters is called an individual. It is represented by a -dimensional parameter vector. A population consists of parameter vectors , . denotes one generation. We have one population for each generation. is the number of members in a population. It is not changed during the minimization process. The initial population is chosen randomly with uniform distribution. According to Storn and Price [1], [18], we have three operations: mutation, crossover, and selection. The crucial idea behind DE is a scheme for generating trial parameter vectors. Mutation and crossover are used to generate new vectors (trial vectors), and selection then determines which of the vectors will survive into the next generation. A. Mutation For each target vector , a mutant vector is generated according to with randomly chosen indexes . Note that indexes have to be different from each other and from the running index so that must be at least four. is a real number that controls the amplification of the difference vector . If a component of a mutant vector goes off the box , then this component is set to bound value. The same “solution” is used by classic DE too. B. Crossover The target vector is mixed with the mutated vector, using the following scheme, to yield the trial vector where if or if and for . is the th evaluation of a uniform random generator number. is the crossover constant , which has to be determined by the user. is a randomly chosen index which ensures that gets at least one element from . Otherwise, no new parent vector would be produced and the population would not alter. C. Selection A greedy selection scheme is used if for minimization problems otherwise for . If, and only if, the trial vector yields a better cost function value than , then is set to ; otherwise, the old value is retained. IV. CONTROL PARAMETER SETTINGS FOR DE ALGORITHM According to Storn et al. [1], [18], DE is much more sensitive to the choice of than it is to the choice of . The suggested choices by Storn in [1] and [16] are: 1) ; 2) ; 3) . Recall that is the dimensionality of the problem. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:37:02 UTC from IEEE Xplore. Restrictions apply. BREST et al.: SELF-ADAPTING CONTROL PARAMETERS IN DIFFERENTIAL EVOLUTION 649 Fig. 1. Self-adapting: encoding aspect. Liu and Lampinen in [16] used control parameters set to , . The values were chosen based on discussions in [21]. Ali and Törn in [9] empirically obtained an optimal value for . They used . was calculated according to the following scheme: if otherwise ensuring that . and are the maximum and minimum values of vectors , respectively. is the lower bound for . In [9], is used. In our paper, we use a self-adaptive control mechanism to change the control parameters and during the run. The third control parameter is not changed during the run. V. SELF-ADAPTING PARAMETERS—NEW VERSION OF DE ALGORITHM Choosing suitable control parameter values is, frequently, a problem-dependent task. The trial-and-error method used for tuning the control parameters requires multiple optimization runs. In this section, we propose a self-adaptive approach for control parameters. Each individual in the population is extended with parameter values. In Fig. 1, the control parameters that will be adjusted by means of evolution are and . Both of them are applied at the individual level. The better values of these (encoded) control parameters lead to better individuals which, in turn, are more likely to survive and produce offspring and, hence, propagate these better parameter values. The solution (Fig. 1) is represented by a -dimensional vector , . New control parameters or factors and are calculated as if otherwise if otherwise and they produce factors and in a new parent vector. , are uniform random values . and represent probabilities to adjust factors and , respectively. In our experiments, we set . Because and , the new takes a value form [0.1,1.0] in a random manner. The new takes a value from [0,1]. and are obtained before the mutation is performed. So, they influence the mutation, crossover, and selection operations of the new vector . We have made a decision about the range for , which is determined by values and , based on the suggested values by other authors and based on the experimental results. In the literature, is rarely greater than one. If control parameter , the new trial vector is generated using crossover but no mutation; therefore, we propose . The classic DE has three control parameters that need to be adjusted by the user. It seems that our self-adaptive DE has even more parameters, but please note that we have used fixed values for , , , and for all benchmark functions in our selfadaptive DE algorithm. The user does not need to adjust those (additional) parameters. Suitable control parameters are different for different function problems. Which are the best values of control parameters and how could we get them? Are there any universal directions on how to get good initial values for control parameters? In our method, the algorithm can change control parameters with some probabilities ( and ) and after that, better control parameters are used in the next generations. We have made additional experiments with some combinations with and using values: 0.05, 0.1, 0.2, and 0.3, and we did not notice any significant difference in results. Therefore, we peaked at , and those values were used in this paper. The main contribution of our approach is that user does not need to guess the good values for and , which are problem dependent. The rules for self-adapting control parameters and are quite simple; therefore, the new version of the DE algorithm does not increase the time complexity, in comparison to the original DE algorithm. VI. BENCHMARK FUNCTIONS Twenty-one benchmark functions from [23] were used to test the performance of ourDEalgorithm to assure a fair comparison. If the number of test problems were smaller, itwould be very difficult to make a general conclusion. Using a test set which is too small also has the potential risk that the algorithm is biased (optimized) toward the chosen set of problems. Such bias might not be useful for other problems of interest. The benchmark functions are given inTable I. denotes the dimensionality of the test problem, denotes the ranges of the variables, and is a function value of the global optimum. A more detailed description of each function is given in [23] and [24], where the functions were divided into three classes: functions with no local minima, many local minima, and a few local minima. Functions are high-dimensional problems. Functions are unimodal. Function is the step function which has one minimum and is discontinuous. Function is a noisy quadratic function. Functions are multimodal functions where the number of local minima increases exponentially with the problem dimension [23], [27]. Functions are low-dimensional functions which have only a few local minima [23], [27]. Yao et al. [23] described the benchmark functions and convergence rates of algorithms, as follows. For unimodal functions, the convergence rates of FEP and classical EP (CEP) algorithms are more interesting than the final results of optimization, as there are other methods which are specifically designed to optimize unimodal functions. For multimodal functions, the final Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:37:02 UTC from IEEE Xplore. Restrictions apply. 650 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 6, DECEMBER 2006 TABLE I BENCHMARK FUNCTIONS results are much more important since they reflect the algorithm’s ability to escape from poor local optima and locate a good near-global optimum. VII. EXPERIMENTAL RESULTS We applied self-adaptive DE and (original) DE to a set of benchmark optimization problems. The initial population was generated uniformly at random in the range, as specified in Table I. Throughout this paper, we have used and for the (original) DE algorithm. Our decision for using those values is based on proposed values from literature [1], [9], [16], [19]. A. Comparison of Self-Adaptive DE and DE Algorithms With FEP and CEP Algorithms In the experiment, we set the parameters as in [23] for fair performance comparison. The following parameters were used in our experiment: 1) population size 100; 2) maximum number of generations: 1500 for , , , , and , 2000 for and , 3000 for , 4000 for , 5000 for , , and , 9000 for , 20 000 for , and 100 for . Therefore, in our experiment, self-adaptive DE and DE used the same population size as in [23] and the same stopping criteria (i.e., equal number of function evaluations). The average results of 50 independent runs are summarized in Table II. Results for the FEP and CEP algorithms are taken from [23, Tables II–IV]. The comparison shows that self-adaptive DE gives better results on benchmark functions than FEP and CEP. Self-adaptive DE algorithm performs better than DE, while DE does not always perform better than FEP and CEP. When Compared With IFEP: Yao et al. in [23] proposed an improved FEP (IFEP) based on mixing (rather than switching) different mutation operators. IFEP generates two candidate offspring from each parent, one by Cauchy mutation and one by Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:37:02 UTC from IEEE Xplore. Restrictions apply. BREST et al.: SELF-ADAPTING CONTROL PARAMETERS IN DIFFERENTIAL EVOLUTION 651 TABLE II EXPERIMENTAL RESULTS, AVERAGED OVER 50 INDEPENDENT RUNS, OF SELF-ADAPTIVE DE, DE, FEP, AND CEP ALGORITHMS. “MEAN BEST” INDICATES AVERAGE OF MINIMUM VALUES OBTAINED AND “STD DEV” STANDS FOR STANDARD DEVIATION. t-TEST TESTS SELF-ADAPTIVE DE AGAINST OTHER ALGORITHMS, RESPECTIVELY Gaussian mutation. The better one is then chosen as the offspring. IFEP has improved FEP’s performance significantly. If we compared self-adaptive DE with IFEP taken from [23, Table X], it is clear that self-adaptive DE is certainly better than IFEP, too. Many test functions take their minimum in the middlepoint of . Three additional experiments for high-dimensional problems were performed to make sure that our algorithm performs well, too, if was not symmetrical about the point where the objective function takes its minimum: 1) middle point is shifted; 2) lower bound was set to zero; and 3) upper bound was set to zero. Albeit no systematical experiments have been carried out, it can be observed, according to preliminary results, that our approach is not significantly influenced when function does not take its minimum in the middlepoint of . B. Comparison of Self-Adaptive DE and DE Algorithms With Adaptive LEP and Best Lévy Algorithms In the experiment, we used the same function set and the parameters as in [24]. The following parameters were used in our experiments: 1) population size 100; 2) maximum number of generations: 1500 for , 30 for and , and 100 for , , and . Table III summarizes the average results of 50 independent runs. A comparison with results from [24] is made. It is clear that no algorithm performs superiorly better than others, but on average self-adaptive DE performs better than the other algorithms. For the unimodal functions and , both self-adaptive DE andDE are better than adaptive LEP and Best Lévy. For function , adaptive LEP performs better than self-adaptive DE. The -test shows a statistically significant difference (please note, in Table II, self-adaptive DE gives good results when number of generations is 5000). Adaptive LEP and self-adaptive DE outperform DE and Best Lévy. For the multimodal functions with many local minima, i.e., , it is clear that the best results are obtained by selfadaptive DE. Interestingly, DE is worse than adaptive LEP and Best Lévy for functions and and better for functions . For the functions and with only a few local minima, the dimension of the functions is also small. In this case, it is hard to judge the performances of individual algorithms. All algorithms were able to find optimal solutions for these two functions. For functions , there is no superior algorithm either. For , self-adaptive DE and DE are better than adaptive LEP and Best Lévy. There are similar algorithm performances for functions and , except adaptive LEP, which performed slightly worse for function . Fig. 2 shows average best fitness curves for the self-adaptive DE algorithm with over 50 independent runs for selected benchmark functions , , , . Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:37:02 UTC from IEEE Xplore. Restrictions apply. 652 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 6, DECEMBER 2006 TABLE III EXPERIMENTAL RESULTS, AVERAGED OVER 50 INDEPENDENT RUNS, OF SELF-ADAPTIVE DE, DE, ADAPTIVE LEP, AND BEST OF FOUR NONADAPTIVE LEP ALGORITHMS (BEST LÉVY). “MEAN BEST” INDICATES AVERAGE OF MINIMUM VALUES OBTAINED AND “STD DEV” STANDS FOR STANDARD DEVIATION. t-TEST TESTS SELF-ADAPTIVE DE AGAINST OTHER ALGORITHMS, RESPECTIVELY Fig. 2. Average best fitness curves of self-adaptive DE algorithm for selected benchmark functions. All results are means of 50 runs. (a) Test function f . (b) Test function f . (c) Test function f . (d) Test function f . C. Discussion on Control Parameter Settings for DE Algorithm In order to compare our self-adaptive version of DE algorithm with the DE algorithm, the best control parameter settings for DE may be needed. DE algorithm does not change control parameter values during optimization process. For all benchmark function problems, the DE algorithm was performed with and taken from [0.0, 0.95] by step 0.05. First, we set control parameters and and kept them fixed during 30 independent runs. Then, we set and for the next 30 runs, etc. The other Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:37:02 UTC from IEEE Xplore. Restrictions apply. BREST et al.: SELF-ADAPTING CONTROL PARAMETERS IN DIFFERENTIAL EVOLUTION 653 Fig. 3. Evolutionary processes of DE for functions f and f . Results were averaged over 30 independent runs. (a) Test function f . (b) Test function f . (c) Test function f . (d) Test function f . Fig. 4. Evolutionary processes of DE for functions f and f . Results were averaged over 30 independent runs. (a) Test function f . (b) Test function f . (c) Test function f . (d) Test function f . (parameter) settings were the same as proposed in Section VII-B. The results were averaged over 30 independent runs. The selected function problems are depicted in Figs. 3 and 4. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:37:02 UTC from IEEE Xplore. Restrictions apply. 654 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 6, DECEMBER 2006 Fig. 5. CR and F values by self-adaptive DE for functions f , f , and f , respectively. Dot is plotted when best fitness value in generation is improved. (a) Test function f . (b) Test function f . (c) Test function f . (d) Test function f . (e) Test function f . (f) Test function f . For function , the good control values and are from [0.15, 0.5] [Fig. 3(a)] and [0.4, 0.75] [Fig. 3(b)], respectively. The best averaged fitness value for function was obtained by and (number of generation was 1500, and ). The best averaged fitness value for function was obtained by and (for high values for give better results). It is very interesting that apparently there are CR values that make a sensitive parameter (where the mean best depends on the value of ), and there are values that make a robust parameter (where the mean best does not depend on the value of ). There are two disadvantages in DE. Parameter tuning requires multiple runs and it is usually not a feasible solution for problems which are very time consuming. The best control parameter settings of DE are problem dependent. The proposed selfadaptive DE overcomes those disadvantages, so there is no need for multiple runs to adjust control parameters, and self-adaptive DE is much more problem independent than DE. D. and Values for Self-Adaptive DE In self-adaptive DE, and values are being changed during evolutionary process. If we want to look into an evolutionary process, we should look at fitness curves. The most important is the best fitness curve. For the selected functions , , , , and , and values are depicted in Figs. 5 and 6 only when the best fitness value in generation is improved. For example, most of the values for functions and are lower than 0.2, while for function they are greater than 0.8. If we know that is good for function , we can use this “knowledge” in initialization by DE and also by our self-adaptive DE. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:37:02 UTC from IEEE Xplore. Restrictions apply. BREST et al.: SELF-ADAPTING CONTROL PARAMETERS IN DIFFERENTIAL EVOLUTION 655 Fig. 6. CR and F values by self-adaptive DE for functions f and f , respectively. Dot is plotted when best fitness value in generation is improved. (a) Test function f . (b) Test function f . (c) Test function f . (d) Test function f . TABLE IV EXPERIMENTAL RESULTS, AVERAGED OVER 50 INDEPENDENT RUNS, OF SELF-ADAPTIVE DE WITH DIFFERENT INITIAL F AND CR VALUES FOR SELECTED BENCHMARK FUNCTIONS It is interesting to make a comparison of values for control parameters and of Figs. 3 and 4 with Figs. 5 and 6, for each function, respectively.We can see that the values of control parameters obtained by self-adaptive DE algorithm are quite similar to (good) and values obtained from the experiment in Section VII-B. But this time, good and parameter values are not obtained by tuning, hence saving many runs. Based on the experiment in this section, the necessity of changing control parameter during the optimization process is confirmed once again. Initialization: The initial vector population is chosen randomly and there arises the question as to how to choose the initial and control parameters for self-adaptive DE, since and are encoded in the individuals (Fig. 1). We performed an additional experiment to determine the initial and values for our self-adaptive DE. Table IV shows the results obtained in our additional experiment only for the selected benchmark functions. The results do not differ ( -test does not show any significant differences); therefore, our selfadaptive DE is not sensitive to the initial and values. This is an advantage of our algorithm. E. Comparison of Self-Adaptive DE With Fuzzy Adaptive Differential Evolution Algorithm Liu and Lampinen [16] introduce a new version of the differential evolution algorithm with adaptive control parameters, the fuzzy adaptive differential evolution (FADE) algorithm, which uses fuzzy logic controllers to adapt the search parameters for the mutation operation and crossover operation. The control inputs incorporate the relative objective function values and individuals of the successive generations. The FADE algorithm was tested with a set of standard test functions, where it outperforms the original DE when the dimensionality of the problem is high [16]. In [16], ten benchmark functions are used, and nine of them are the same as the benchmark functions in [23] and in this Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:37:02 UTC from IEEE Xplore. Restrictions apply. 656 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 6, DECEMBER 2006 TABLE V EXPERIMENTAL RESULTS, AVERAGED OVER 100 INDEPENDENT RUNS, OF SELF-ADAPTIVE DE AND FUZZY ADAPTIVE DE ALGORITHMS. “MEAN BEST” INDICATES AVERAGE OF MINIMUM VALUES OBTAINED AND “STD DEV” STANDS FOR STANDARD DEVIATION. t-TEST TESTS SELF-ADAPTIVE DE AGAINST OTHER ALGORITHMS, RESPECTIVELY paper. The following parameters were used in our experiment (the same parameter settings are used in [16]): 1) dimensionality of the problem ; 2) population size ; 3) maximum number of generations: 5000 for , , , , and , 7000 for , 10 000 for , 100 for , and 50 for . Both algorithms use an approach to adapt mutation control parameter and the crossover control parameter . The average results of 100 independent runs are summarized in Table V. The experimental results suggest that the proposed algorithm certainly performs better than the FADE algorithm. This is clearly reflected also by the -test. Based on the obtained results in this section, we can conclude that our self-adaptive method is very good in solving benchmark functions (yielding excellent results) and for determination of good values for control parameters of a DE. VIII. CONCLUSION Choosing the proper control parameters for DE is quite a difficult task because the best settings for the control parameters can be different for different functions. In this paper, the proposed self-adaptive method is an attempt to determine the values of control parameters and . Our self-adaptive DE algorithm has been implemented and tested on benchmark optimization problems taken from literature. The results show that our algorithm, with self-adaptive control parameter settings, is better or at least comparable to the standard DE algorithm and evolutionary algorithms from literature considering the quality of the solutions found. The proposed algorithm gives better results in comparison with the FADE algorithm. Our self-adaptive method could be simply incorporated into existing DE algorithms, which are used to solve problems from different optimization areas. We did not experiment with different population sizes, nor did we make population size adaptive. This remains a challenge for future work. ACKNOWLEDGMENT The authors would like to thank J. S. Versterstroem for letting us use his source code for most of the benchmark functions. The authors would also like to thank Prof. X. Yao, the anonymous associate editor, and the referees for their valuable comments that helped greatly to improve this paper. Simulation studies for the differential evolution strategy were performed with the C code downloaded from http://www.icsi.berkeley.edu/~storn/ code.html. REFERENCES [1] R. Storn and K. Price, “Differential evolution—A simple and efficient heuristic for global optimization over continuous spaces,” J. Global Optimiz., vol. 11, pp. 341–359, 1997. [2] J. Liu and J. Lampinen, “On setting the control parameter of the differential evolution method,” in Proc. 8th Int. Conf. Soft Computing (MENDEL 2002), 2002, pp. 11–18. [3] T. Bäck, D. B. Fogel, and Z. Michalewicz, Eds., Handbook of Evolutionary Computation. New York: Inst. Phys. and Oxford Univ. Press, 1997. [4] M. H. Maruo, H. S. Lopes, and M. R. Delgado, “Self-adapting evolutionary parameters: Encoding aspects for combinatorial optimization problems,” in Lecture Notes in Computer Science, G. R. Raidl and J. Gottlieb, Eds. Lausanne, Switzerland: Springer-Verlag, 2005, vol. 3448, Proc. Evol. Comput. Combinatorial Optimization, pp. 155–166. [5] A. E. Eiben, R. Hinterding, and Z. Michalewicz, “Parameter control in evolutionary algorithms,” IEEE Trans. Evol. Comput., vol. 3, no. 2, pp. 124–141, Jul. 1999. [6] T. Krink and R. K. Ursem, “Parameter control using the agent based patchwork model,” in Proc. Congr. Evolutionary Computation, La Jolla, CA, Jul. 6–9, 2000, pp. 77–83. [7] A. E. Eiben and J. E. Smith, Introduction to Evolutionary Computing, ser. Natural Computing. Berlin, Germany: Springer-Verlag, 2003. [8] K. Deb, “A population-based algorithm-generator for real-parameter optimization,” Soft Computing—A Fusion of Foundations, Methodologies and Applications, vol. 9, no. 4, pp. 236–253, 2005 [Online]. Available: http://springerlink.metapress.com/index/10.1007/ s00500-004-0377-4 [9] M. M. Ali and A. Törn, “Population set-based global optimization algorithms: Some modifications and numerical studies,” Comput. Oper. Res., vol. 31, no. 10, pp. 1703–1725, 2004. [10] T. Bäck, “Evolution strategies: An alternative evolutionary algorithm,” in Lecture Notes in Computer Science, J. M. Alliott, E. Lutton, E. Ronald, M. Schoenauer, and D. Snyders, Eds. Heidelberg, Germany: Springer-Verlag, 1996, vol. 1063, Proc. Artificial Evolution: Eur. Conf., pp. 3–20. [11] ——, “Adaptive business intelligence based on evolution strategies: some application examples of self-adaptive software,” Inf. Sci., vol. 148, pp. 113–121, 2002. [12] L. Davis, Ed., Handbook of Genetic Algorithms. New York: Van Nostrand Reinhold, 1991. [13] W. M. Spears, “Adapting crossover in evolutionary algorithms,” in Proc. 4th Annual Conf. Evolutionary Programming, J. R. McDonnell, R. G. Reynolds, and D. B. Fogel, Eds., 1995, pp. 367–384. [14] M. A. Semenov and D. A. Terkel, “Analysis of convergence of an evolutionary algorithm with self-adaptation using a stochastic Lyapunov function,” Evol. Comput., vol. 11, no. 4, pp. 363–379, 2003. [15] J. He and X. Yao, “Toward an analytic framework for analysing the computation time of evolutionary algorithms,” Artificial Intell., vol. 145, no. 1–2, pp. 59–97, 2003. [16] J. Liu and J. Lampinen, “A fuzzy adaptive differential evolution algorithm,” Soft Computing—A Fusion of Foundations, Methodologies and Applications, vol. 9, no. 6, pp. 448–462, 2005 [Online]. Available: http://springerlink.metapress.com/index/10.1007/s00500-004-0363-x Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:37:02 UTC from IEEE Xplore. Restrictions apply. BREST et al.: SELF-ADAPTING CONTROL PARAMETERS IN DIFFERENTIAL EVOLUTION 657 [17] ——, “Adaptive parameter control of differential evolution,” in Proc. 8th Int. Conf. Soft Computing (MENDEL 2002), 2002, pp. 19–26. [18] R. Storn and K. Price, Differential Evolution—A Simple and efficient adaptive scheme for global optimization over continuous spaces, Berkeley, CA, Tech. Rep. TR-95-012, 1995 [Online]. Available: citeseer.ist.psu.edu/article/storn95differential.html [19] J. Vesterstroem and R. Thomsen, “A comparative study of differential evolution, particle swarm optimization, and evolutionary algorithms on numerical benchmark problems,” in Proc. IEEE Congr. Evolutionary Computation, Portland, OR, Jun. 20–23, 2004, pp. 1980–1987. [20] J. Sun, Q. Zhang, and E. Tsang, “DE/EDA: A new evolutionary algorithm for global optimization,” Info. Sci., vol. 169, pp. 249–262, 2004. [21] K. Price and R. Storn, “Differential evolution: A simple evolution strategy for fast optimization,” Dr. Dobb’s J. Software Tools, vol. 22, no. 4, pp. 18–24, Apr. 1997. [22] R. Gämperle, S. D. Müller, and P. Koumoutsakos, “A parameter study for differential evolution,” WSEAS NNA-FSFS-EC 2002. Interlaken, Switzerland, WSEAS, Feb. 11–15, 2002 [Online]. Available: http://www.worldses.org/online/ [23] X. Yao, Y. Liu, and G. Lin, “Evolutionary programming made faster,” IEEE Trans. Evol. Comput., vol. 3, no. 2, p. 82, Jul. 1999. [24] C. Y. Lee and X. Yao, “Evolutionary programming using mutations based on the Lévy probability distribution,” IEEE Trans. Evol. Comput., vol. 8, no. 1, pp. 1–13, Feb. 2004. [25] Z. Tu and Y. Lu, “A robust stochastic genetic algorithm (StGA) for global numerical optimization,” IEEE Trans. Evol. Comput., vol. 8, no. 5, pp. 456–470, Oct. 2004. [26] F. Herrera and M. Lozano, “Adaptive genetic operators based on coevolution with fuzzy behaviors,” IEEE Trans. Evol. Comput., vol. 5, no. 2, pp. 149–165, Apr. 2001. [27] A. Törn and A. ¡ Zilinskas, “Global optimization,” in Lecture Notes Computer Science. Heidelberg, Germany: Spring-Verlag, 1989, vol. 350, pp. 1–24. Janez Brest (M’02) received the B.S., M.Sc., and Ph.D. degrees in computer science from the University of Maribor, Maribor, Slovenia, in 1995, 1998, and 2001, respectively. He has been with the Laboratory for Computer Architecture and Programming Languages, University of Maribor, since 1993. He is currently an Assistant Professor. His research interests include evolutionary computing, artificial intelligence, and optimization. His fields of expertise embrace programming languages, web-oriented programming, and parallel and distributed computing research. Dr. Brest is a member of ACM. Sa¡so Greiner received the B.S. and M.Sc. degrees in computer science from the University of Maribor, Maribor, Slovenia, in 2002 and 2004, respectively. He is currently a Teaching Assistant at the Faculty of Electrical Engineering and Computer Science, University of Maribor. His research interests include object-oriented programming languages, compilers, computer architecture, and web-based information systems. Borko Bo¡skovic´ received the B.S. degree, in 2003. He is currently a Teaching Assistant at the Faculty of Electrical Engineering and Computer Science, University of Maribor, Maribor, Slovenia. He has worked in the Laboratory for Computer Architecture and Programming Languages, University of Maribor, since 2000. His research interests include web-oriented programming, evolutionary algorithms, and search algorithms for two players with perfect-information zero-sum games. Marjan Mernik (M’95) received the M.Sc. and Ph.D. degrees in computer science from the University of Maribor, Maribor, Slovenia, in 1994 and 1998, respectively. He is currently an Associate Professor in the Faculty of Electrical Engineering and Computer Science, University of Maribor. He is also an Adjunct Associate Professor in the Department of Computer and Information Sciences, University of Alabama, Birmingham. His research interests include programming languages, compilers, grammar-based systems, grammatical inference, and evolutionary computations. He is a member of ACM and EAPLS. Viljem ¡ Zumer (M’77) is a full Professor in the Faculty of Electrical Engineering and Computer Science, University of Maribor, Maribor, Slovenia. He is the Head of Laboratory for Computer Architecture and Programming Languages and the Head of the Institute of Computer Science as well. His research interests include programming languages and computer architecture.\"],\n",
    "[10,4,1,\"Particle Swarm Optimization James Kennedy' and Russell Eberhart2 Washington, DC 20212 kennedyjim @bls .gov 2Purdue School of Engineering and Technology Indianapolis, IN 46202-5160 eberhart @ engr.iupui .edu 1 ABSTRACT A concept for the optimization of nonlinear functions using particle swarm methodology is introduced. The evolution of several paradigms is outlined, and an implementation of one of the paradigms is discussed. Benchmark testing of the paradigm is described, and applications, including nonlinear function optimization and neural network training, are proposed. The relationships between particle swarm optimization and both artificial life and genetic algorithms are described, 1 INTRODUCTION This paper introduces a method for optimization of continuous nonlinear functions. The method was discovered through simulation of a simplified social model; thus the social metaphor is discussed, though the algorithm stands without metaphorical support. This paper describes the particle swarm optimization concept in terms of its precursors, briefly reviewing the stages of its development from social simulation to optimizer. Discussed next are a few paradigms that implement the concept. Finally, the implementation of one paradigm is discussed in more detail, followed by results obtained from applications and tests upon which the paradigm has been shown to perform successfully. Particle swarm optimization has roots in two main component methodologies. Perhaps more obvious are its ties to artificial life (A-life) in general, and to bird flocking, fish schooling, and swarming theory in particular. It is also related, however, to evolutionary computation, and has ties to both genetic algorithms and evolutionary programming. These relationships are briefly reviewed in the paper. Particle swarm optimization as developed by the authors comprises a very simple concept, and paradigms can be implemented in a few lines of computer code. It requires only primitive mathematical operators, and is computationally inexpensive in terms of both memory requirements and speed. Early testing has found the implementation to be effective with several kinds of problems. This paper discusses application of the algorithm to the training of artificial neural network weights, Particle swarm optimization has also been demonstrated to perform well on genetic algorithm test functions. This paper discusses the performance on Schaffer's f6 function, as described in Davis [l]. 2 SIMULATING SOCIAL BEHAVIOR A number of scientists have created computer simulations of various interpretations of the movement of organisms in a bird flock or fish school. Notably, Reynolds [8] and Heppner and Grenander [4] presented simulations of bird flocking. Reynolds was intrigued by the aesthetics of bird flocking choreography, and Heppner, a zoologist, was interested in discovering the underlying rules that enabled large numbers of birds to flock synchronously, often changing direction suddenly, scattering and regrouping, etc. Both of these scientists had the insight that local processes, such as those modeled by 0-7803-2768-3/95/$4.00 0 1995 IEEE 1942 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:41:22 UTC from IEEE Xplore. Restrictions apply. cellular automata, might underlie the unpredic:table group dynamics of bird social behavior. Both models relied heavily on manipulation of inter-individual distances; that is, the synchrony of flocking behavior was thought to be a function of birds’ efforts to maintain an optimum distance between themselves and their neighbors. It does not seem a too-large leap of logic to suppose that some same rules underlie animal social behavior, including herds, schools, and flocks, and that of humans. As sociobiologist E. 0. Wilson [9] has written, in reference to fish schooling, “In theory at least, individual members of the school can profit from the discoveries and previous experience of all other members of the school during the search for food. This advantage can become decisive, outweighing the disadvantages of competition for food items, whenever the resource is unpedictably distributed in patches” (p.209). This statement suggests that social sharing of information among conspeciates offers an evolutionary advantage: this hypothesis was fundamental to the developnmt of particle swarm optimization. One motive for developing the simdation was to model human social behavior, which is of course not identical to fish schooling or bird flocking. Che important difference is its abstractness. Birds and fish adjust their physical movement to avoid prechtors, seek food and mates, optimize environmental parameters such as temperature, etc. Humans; adjust not only physical movement but cognitive or experiential variables as well. We do not usually walk in step and tum in unison (though some fascinating research in human conformity shows that we are capable of it); rather, we tend to adjust our beliefs and attitudes to conform with those cd our social peers. This is a major distinction in terms of contriving a computer simulation, for at least one obvious reason: collision. Two individuals can hold identical attitudes and beliefs without banging together, but two birds cannot occupy the same position in space without colliding. It seems reasonable, in discussing human social behavior, to map thie concept of change into the birdfish analog of movement. This is consistent with the classic Aristotelim view of qualitative and quantitative change as types of movement. Thus, besides moving through tlhree-dimensional physical space, and avoiding collisions, humans change in abstract multidimensional space, colision-free. Physical space of course affects informational inputs, but it is arguably a trivial component of psychological experience. Humans learn to avoid physical collision by an early age, hit navigation of n-dimensional psychosocial space requires decades of practice - and many of us never seem to acquire quite all the skills we need! 3 PRECURSORS: THE ETIOLOGY OF PARTICLE SWARM OPTIMIZATION The particle swarm optimizer is probably kcst presented by explaining its conceptual development. As mentioned above, the algorithm began as a simulation of a simplified social milieu. Agents were thought of as collision-proof birds, and the original intent was to graphically simulate the graceful but unpredictable choreography of a bird flock. 3.1 Nearest Neighbor Velocity Matching and Craziness A satisfying simulation was rather quickly written, which relied on two props: nearest-neighbor velocity matching and “craziness.” A populartion of birds was randomly initialized with a position for each on a torus pixel grid and with X and Y velocities. At each iteration a loop in the program determined, for each agent (a more appropriate term than bird), which other agent was its nearest neighbor, then assigned that agent’s X and Y velocities to the agent in focus. Essentially this simple d e created a synchrony of movement. Unfortunately, the flock quickly settled on ii lunanimous, unchanging direction. Therefore, a stochastic variable called craziness was introduced. At each iteration some change was added to randomly 1943 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:41:22 UTC from IEEE Xplore. Restrictions apply. chosen X and Y velocities. This i n t r d u d enough variation into the system to give the simulation an interesting and “lifelike” appearance, though of course the variation was wholly artiticial. 3.2 The Cornfield Vector Heppner’s bird simulations had a feature which introduced a dynamic force into the simulation. His birds flocked around a “roost,” a position on the pixel screen that attracted them until they finally landed there. This eliminated the need for a variable like craziness, as the simulation took on a lie of its own. While the idea of a roost was intriguing, it led to another question which seemed even more stimulating. Heppner’s birds knew where their roost was, but in real life birds land on any tree or telephone wire that meets their immediate needs. Even more importantly, bird flocks land where there is food. How do they find food? Anyone who has ever put out a bird feeder knows that within hours a great number of birds will likely find it, even though they had no previous knowledge of its location, appearance, etc. It seems possible that something about the flock dynamic enables members of the flock to capitalize on one another’s knowledge, as in Wilson’s quote above. The second variation of the simulation defined a “comfield vector,” a two-dimensional vector of XY coordinates on the pixel plane. Each agent was programmed to evaluate its present position in terms of the equation: Eval= J - 4w+ so that at the (100,100) position the value was zero. Each agent “remembered” the best value and the XY position which had resulted in that value. The value was called pbest[] and the positions pbestx[] and pbestyl] (brackets indicate that these are arrays, with number of elements = number of agents). As each agent moved through the pixel space evaluating positions, its X and Y velocities were adjusted in a simple manner. If it was to the right of its pbestx, then its X velocity (call it vx) was adjusted negatively by a random amount weighted by a parameter of the system: vx[]=vx[] - rand()*p-increment. If it was to the left of pbestx, rand()*p-increment was added to vx[]. Similarly, Y velocities vy[] were adjusted up and down, depending on whether the agent was above or below pbesty. Secondly, each agent “knew” the globally best position that one member of the flock had found, and its value. This was accomplished by simply assigning the array index of the agent with the best value to a variable called gbest, so that pbestx[gbest] was the group’s best X position, and pbesty[gbest] its best Y position, and this information was available to all flock members. Again, each member’s vx[] and vy[] were adjusted as follows, where g-increment is a system parameter. ifpresentx[l> pbestx[gbest] then vx[] = vx[] - rand() *g-increment ifpresentx[] < pbestx[gbest] then vx[] = vx[] + rand() *g-increment ifpresenty[] > pbesty[gbestl then vy[] = vy[] - rand() *g-increment ifpresenty[l< pbesty[gbestl then vy[] = vy[] + rand() *g-increment In the simulation, a circle marked the (100,100) position on the pixel field, and agents were represented as colored points. Thus an observer could watch the flocking agents circle around until they found the simulated cornfield. The results were surprising. With p-increment and g-increment set relatively high, the flock seemed to be sucked violently into the cornfield. In a very few iterations the entire flock, usually 15 to 30 individuals, was seen to be clustered within the tiny circle surrounding the goal. With p-increment and g-increment set low, the flock swirled around the goal, realistically approaching it, swinging out rhythmically with subgroups synchronized, and finally “landing” on the target. 1944 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:41:22 UTC from IEEE Xplore. Restrictions apply. 33 Eliminating Ancillary Variables Once it was clear that the paradigm could og~imizesi mple, two-dimensional, linear functions, it was important to identify the parts of the paradip that are necessary for the task. For instance, the authors quickly found that the algorithm worlts just as well, and looks just as realistic, without craziness, so it was removed. Next it was shown that optimization actually occurs slightly faster when nearest neighbor velocity matching is removed, though the visual effect is changed. Theflock is now a swam, but it is well able to find the codielidl. The variables pbest and gbest and their increments are both necessary. Conceptually pbest resembles autobiographical memory, as each individual remembers its own experience (though only one fact about it), and the velocity adjustment associarted with pbest has been called “simple nostalgia” in that the individual tends to return to the place thiat most satisfied it in the past. On the other hand, gbest is conceptually similar to publicized knowledge, or a group norm or standard, which individuals seek to attain. In the simulations, a high value of princrement relative to g-increment results in excessive wandering of isolated individuals through the problem space, while the reverse (relatively high g-increment) results in the flock rushing prematurely toward local minima. Approximately equal values of the two increments Seem to result in the most effective search of the problem domain. 3.4 Multidimensional Search W e th e algorithm seems to impressively ”del a flock searching for a cornfield, most interesting optimization problems are neither linear nor two-dimensional. Since one of the authors’ objectives is to model social behavior, which is multidimensional and collision-free, it seemed a simple step to change presentx and presenty (and of course vx[] and vy[n from onedimensional arrays to D x N matrices, where D is any number of dimensions and N is the number of agents. Multidimensional experiments were performed, using a nonlinear, multidimensional problem: adjusting weights to train a feedforward multilayer pe:nceptron neural network (NN). One of the authors’ first experiments involved training weights for a tluee-layer NN solving the exclusive-or (XOR) problem. This problem requires two input and one output processing elements (PES), plus some number of hidden PES. Besides connections from the piwious layer, the hidden and output PE layers each has a bias PE associated with it. Thus a 2,3,1 NN requires optimization of 13 parameters. This problem was approached by flying the agents through 13-dimensional space until an average sum-squared error per PE criterion was met. The algorithm performed very well on this problem. The thirteendimensional XOR network was trained, to am e < 0.05 criterion, in an average of 30.7 iterations with 20 agents. More complex NN architectures, look longer of course, but results, discussed in Section 5: Results and Early Applications, were still very good. 3.5 Acceleration by Distance Though the algorithm worked well, there WiU; something aesthetically displeasing and hard to understand about it. Velocity adjustments were based on a crude inequality test: ifpresentx > bestx, make it smaller; ifpresentx c bestx, make it bigger. Some experimentation revealed that further revising the algorithm made it easier to und~eirstanda nd improved its performance. Rather than simply testing the sign of the inequality, velocities were adjusted according to their difference, per dimension, from best locations: vx[][] = vx[][] + rand()* p_increment*(pbt?stx[][]- presentx[l[l) 1945 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:41:22 UTC from IEEE Xplore. Restrictions apply. (note the parameters vx and presentx have two sets of brackets because they are now matrices of agents by dimensions; increment and bestx could also have a g instead of p at their beginnings.) 3.6 Current Simplified Version It was soon realized that there is no good way to guess whether p- or g-increment should be larger. Thus, these terms were also stripped out of the algorithm. The stochastic factor was multiplied by 2 to give it a mean of 1, so that agents would “overfly” the target about half the time. This version outperforms the previous versions. Further research will show whether there is an optimum value for the constant currently set at 2, whether the value should be evolved for each problem, or whether the value can be determined from some knowledge of a particular problem. The current simplified particle swarm optimizer now adjusts velocities by the following formula: vxLlLl= VXLILl + 2 * rand() * (pbestx[][] - presentx[]fl) + 2 * rand() * (pbestxfllgbesfl - presentxflf]) 3.7 Other Experiments Other variations on the algorithm were tried, but none seemed to improve on the current simplified version. For instance, it is apparent that the agent is propelled toward a weighted average of the two “best” points in the problem space. One version of the algorithm reduced the two terms to one, which was the point on each dimension midway between pbest and gbest positions. This version had an unfortunate tendency, however, to converge on that pint whether it was an optimum or not. Apparently the two stochastic “kicks” are a necessary part of the process. Another version considered using two types of agents, conceived as “explorers” and “settlers.” Explorers used the inequality test, which tended to cause them to overrun the target by a large distance, while settlers used the difference term. The hypothesis was that explorers would extrapolate outside the “known” region of the problem domain, and the settlers would hill-climb or micro-explore regions that had been found to be good. Again, this method showed no improvement over the current simplified version. Occam’s razor slashed again. Another version that was tested removed the momentum of vx[][]. The new adjustment was: V X i l L l = 2 * rand() * (pbestxflf] - presentx[lfl ) + 2 * rand() * (pbestx[][gbestJ - presentx[l[] ) This version, though simplified, tumed out to be quite ineffective at finding global optima. 4 SWARMS AND PARTICLES As was described in Section 3.3, it became obvious during the simplification of the paradigm that the behavior of the population of agents is now more like a swarm than a flock. The term swarm has a basis in the literature. In particular, the authors use the term in accordance with a paper by Millonas [6], who developed his models for applications in artificial life, and articulated five basic principles of swarm intelligence. First is the proximity principle: the population should be able to carry out simple space and time computations. Second is the quality principle: the population should be able to respond to quality factors in the environment. Third is the principle of diverse response: the population should not commit its activities along excessively narrow channels. Fourth is the principle of stability: the population should not change its mode of behavior every time the environment changes. Fifth is the 1946 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:41:22 UTC from IEEE Xplore. Restrictions apply. principle of adaptability: the population must be able to change behavior mode when it’s worth the computational price. Note that principles four and five are the opposite sides of the same coin. The particle swarm optimization concept and paradigm presented in this paper seem to adhere to all five principles. Basic to the paradigm are n-dimensional space calculations carried out over a series of time steps. The population is responding to the quality factors pbest and gbest. The allocation of responses between pbest and gbest ensures ia diversity of response. The population changes its state (mode of behavior) only when gbest changes, thus adhering to the principle of stability. The population is adaptive because it does change when gbest changes. The term particle was selected as a compromise. While it could be argued that the population members are mass-less and volume-less, anid thus could be called “points,” it is felt that velocities and accelerations are more appropriately applied to particles, even if each is defined to have arbitrarily small mass and volume. Further, Reeves [7] discusses particle systems consisting of clouds of primitive particles as models of diffuse obje:cts such as clouds, frre and smoke. Thus the label the authors have chosen to represent the optimization concept is particle swarm. 5 TESTS AND EARLY APPLICATIONS OF THE OPTIMIZER The paradigm has been tested using systemiatic benchmark tests as well as observing its performance on applications that are known to be difificult. The neural-net application described in Section 3.4, for instance, showed that the particle swarm optimizer could train NN weights as effectively as the usual error backpropagation method. The particle swarm optimizer has also been used to train a neural network to classify the Fisher Iris Data Set 1[3]. Again, the optimizer trained the weights as effectively as the backpropagation method. Over a series of ten training sessions, the particle swarm optimizer paradigm required an average of 284 epochs,, Intriguing informal indications are that the Irihed weights found by particle swarms sometimes generalize from a training set to a test set better than solutions found by gradient descent. For example, on a data set representing electroencephalogram spike waveforms and false positives, a backpropagation NN achieved 89 percent correct on the test data [2]. The particle swarm optimizer was able to train the network so as to achieve 92 percent correct. The particle swarm optimizer was compareid to a benchmark for genetic algorithms in Davis [ 11: the extremely nonlinear Schaffer f6 function. This function is very difficult to optimize, as the highly discontinuous data surface features many 1oc:al optima. The particle swarm paradigm found the global optimum each run, and appears to approximite the results reported for elementary genetic algorithms in Chapter 2 of [ 11 in terms of the number of evaluations required to reach certain performance levels. 6 CONCLUSIONS Particle swarm optimization is an extremely wimple algorithm that seems to be effective for optimizing a wide range of functions. We view it as a ]mid-level form of A-life or biologically derived algorithm, occupying the space in nature between evollutionary search, which requires eons, and neural processing, which occurs on the order of milliseconds. Social optimization occurs in the time frame of ordinary experience - in fact, it is ordinary experieince. In addition to its ties with A-life, particle swarm optimization has obvious ties with evolutioiniuy computation. Conceptually, it seems to lie somewhere between genetic algorithms and evolutionary programming. It is highly dependent on stochastic processes, like evolutionary programming. The adjustment toward pbest and gbest by the particle swarm optimizer is conceptually similar to the crossover operation utilized by genetic algorithms. It uses the concept ofjimess, as do aU evolutionary computation paradigms. 1947 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:41:22 UTC from IEEE Xplore. Restrictions apply. Unique to the concept of particle swarm optimization is flying potential solutions through hyperspace, accelerating toward “better” solutions. Other evolutionary computation schemes operate directly on potential solutions which are represented as locations in hyperspace. Much of the success of particle swarms seems to lie in the agents’ tendency to hurtle past their target. Holland’s chapter on the “optimum allocation of trials” [5] reveals the delicate balance between conservative testing of known regions versus risky exploration of the unknown. It appears that the current version of the paradigm allocates trials nearly optimally. The stochastic factors allow thorough search of spaces between regions that have been found to be relatively good, and the momentum effect caused by nmhfying the extant velocities rather than replacing them results in overshooting, or exploration of unknown regions of the problem domain. The authors of this paper are a social psychologist and an electrical engineer. The particle swarm optimizer serves both of these fields equally well. Why is social behavior so ubiquitous in the animal kingdom? Because it optimizes. What is a good way to solve engineering optimization problems? Modeling social behavior. Much further research remains to be conducted on this simple new concept and paradigm. The goals in developing it have been to keep it simple and robust, and we seem to have succeeded at that. The algorithm is written in a very few lines of code, and requires only specification of the problem and a few parameters in order to solve it. This algorithm belongs ideologically to that philosophical school that allows wisdom to emerge rather than trying to impose it, that emulates nature rather than trying to control it, and that seeks to make things simpler rather than more complex. Once again nature has provided us with a technique for processing information that is at once elegant and versatile. ACKNOWLEDGMENTS Portions of this paper are adapted from a chapter on particle swarm optimization in a book entitled Computational Intelligence PC Tools, to be published in early 1996 by Academic Press Professional (APP). The permission of APP to include this material is gratefully acknowledged. The input and comments of Roy Dobbins and Pat Simpson are appreciated. REFERENCES [l] Davis, L., Ed. (1991). Handbook of Genetic Algorithms. Van Nostrand Reinhold, New York, NY. [2] Eberhart, R. C. and R. W Dobbins (1990). Neural Network PC Tools: A Practical Guide. Academic Press, San Diego, CA. [3] Fisher, R.A. (1936). The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7: 179-188. [4] Heppner, F. and U. Grenander (1990). A stochastic nonlinear model for coordinated bird flocks. In S . Krasner, Ed., The Ubiquity of Chaos. AAAS Publications, Washington, DC. [5] Holland, J. H. (1992). Adaptation in Natural and Artijlcial Systems. MIT Press, Cambridge, MA. [6] Millonas, M. M. (1994). Swarms, phase transitions, and collective intelligence. In C. G. Langton, Ed., Artijicial Life III. Addison Wesley, Reading, MA. [7] Reeves, W. T. (1983). Particle systems - a technique for modeling a class of fuzzy objects. ACM Transactions on Graphics, 2(2):91-108. [SI Reynolds, C. W. (1987). Flocks, herds and schools: a distributed behavioral model. Computer Graphics, 2 1 (4):25-34. [9] Wilson, E.O. (1975). Sociobiology: The new synthesis. Belknap Press, Cambridge, hlA. 1948\"],\n",
    "[11,4,1,\"Bee Colony Optimization: Principles and Applications Dusan Teodorovic, Panta Lucic, Goran Markovic, Mauro Dell' Orco Abstract - The Bee Colony Optimization Metaheuristic (BCO) is proposed in the paper. The BCO represents the new metaheuristic capable to solve difficult combinatorial optimization problems. The artificial bee colony behaves partially alike, and partially differently from bee colonies in nature. In addition to proposing the BCO as a new metaheuristic, we also describe in the paper two BCO algorithms that we call the Bee System (BS) and the Fuzzy Bee System (FBS). In the case of FBS the agents (artificial bees) use approximate reasoning and rules of fuzzy logic in their communication and acting. In this way, the FBS is capable to solve deterministic combinatorial problems, as well as combinatorial problems characterized by uncertainty. The proposed approach is illustrated by three Case studies. Keywords - Key words or phrases in alphabetical order, separated by commas. I. INTRODUCTION Agreat number of traditional engineering models and algorithms used to solve complex problems are based on control and centralization. Various natural systems (social insects colonies) lecture us that very simple individual organisms can create systems able to perform highly complex tasks by dynamically interacting with each other. Bee swarm behavior in nature is, first and foremost, characterized by autonomy and distributed functioning and self-organizing. In the last couple of years, the researchers started studying the behavior of social insects in an attempt to use the Swarm Intelligence concept in order to develop various Artificial Systems. The Bee Colony Optimization (BCO) Metaheuristic that represents the new direction in the field of Swarm Intelligence is introduced in this paper. The primary goal of this paper is to explore the possible applications of collective bee intelligence in solving combinatorial problems characterized by uncertainty. The paper is Dusan Teodorovic is with the Faculty of Transport and Traffic Engineering, University of Belgrade, Serbia (phone +381-1 1-3091-210; fax: +381-1 1-3096-704; e-mail: dusanAsfbg.ac.yu). Panta Lucic is with the CSSI, Inc, Washington D.C., U.S.A. (e-mail: plucicAvt.edu). Goran Markovic is with the Faculty of Transport and Traffic Engineering, University of Belgrade, Serbia (e-mail: Mauro Dell' Orco is with the Technical University of Bari, Italy (e-mail: dellorco poliba.it) organized in the following way. The new computational paradigm - The Bee Colony Optimization is described in Section 2. Sections III, IV, and V are devoted to the description of Case Studies (Traveling Salesman Problem, Ride-Matching Problem, and Routing and Wavelength Assignment Problem). Conclusion is given in the Section VI. II. THE BEE COLONY OPTIMIZATION: THE NEW COMPUTATIONAL PARADIGM Social insects (bees, wasps, ants, termites) have lived on Earth for millions of years, building nests and more complex dwellings, organizing production and procuring food. The colonies of social insects are very flexible and can adapt well to the changing environment. This flexibility allows the colony of social insects to be robust and maintain its life in spite of considerable disturbances. The dynamics of the social insect population is a result of the different actions and interactions of individual insects with each other, as well as with their environment. The interactions are executed via multitude of various chemical and/or physical signals. The final product of different actions and interactions represents social insect colony behavior. Interaction between individual insects in the colony of social insects has been well documented. The examples of such interactive behavior are bee dancing during the food procurement, ants' pheromone secretion, and performance of specific acts, which signal the other insects to start performing the same actions. These communication systems between individual insects contribute to the formation of the `collective intelligence` of the social insect colonies. The term `Swarm Intelligence`, denoting this `collective intelligence` has come into use [1]-[3]. A. Bees in the Nature Self-organization of bees is based on a few relatively simple rules of individual insect's behavior. In spite of the existence of a large number of different social insect species, and variation in their behavioral patterns, it is possible to describe individual insects' as capable of performing a variety of complex tasks [4]. The best example is the collection and processing of nectar, the practice of which is highly organized. Each bee decides to reach the nectar source by following a nestmate who has already discovered a patch of flowers. Each hive has a socalled dance floor area in which the bees that have discovered nectar sources dance, in that way trying to 1-4244-0433-9/06/$20.00 (©2006 IEEE. 151 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:42:12 UTC from IEEE Xplore. Restrictions apply. convince their nestmates to follow them. If a bee decides to leave the hive to get nectar, she follows one of the bee dancers to one of the nectar areas. Upon arrival, the foraging bee takes a load of nectar and returns to the hive relinquishing the nectar to a food storer bee. After she relinquishes the food, the bee can (a) abandon the food source and become again uncommitted follower, (b) continue to forage at the food source without recruiting the nestmates, or (c) dance and thus recruit the nestmates before the return to the food source. The bee opts for one of the above alternatives with a certain probability. Within the dance area, the bee dancers `advertise` different food areas. The mechanisms by which the bee decides to follow a specific dancer are not well understood, but it is considered that `the recruitment among bees is always a function of the quality of the food source` [4]. It is also noted that not all bees start foraging simultaneously. The experiments confirmed, `new bees begin foraging at a rate proportional to the difference between the eventual total and the number presently foraging`. The basic principles of collective bee intelligence in solving combinatorial optimization problems were for a first time used in [5] and [6]. The authors introduced the Bee System (BS) and tested it in the case of Traveling Salesman Problem. The Bee Colony Optimization Metaheuristic (BCO) that has been proposed in this paper represents further improvement and generalization of the Bee System. The basic characteristics of the BCO Metaheuristic are described. Our artificial bee colony behaves partially alike, and partially differently from bee colonies in nature. The Fuzzy Bee System (FBS) capable to solve combinatorial optimization problems characterized by uncertainty is also introduced in the paper. Within FBS, the agents use approximate reasoning and rules of fuzzy logic in their communication and acting. B. The Bee Colony Optimization Metaheuristic Within the Bee Colony Optimization Metaheuristic (BCO), agents that we call - artificial bees collaborate in order to solve difficult combinatorial optimization problem. All artificial bees are located in the hive at the beginning of the search process. During the search process, artificial bees communicate directly. Each artificial bee makes a series of local moves, and in this way incrementally constructs a solution of the problem. Bees are adding solution components to the current partial solution until they create one or more feasible solutions. The search process is composed of iterations. The first iteration is finished when bees create for the first time one or more feasible solutions. The best discovered solution during the first iteration is saved, and then the second iteration begins. Within the second iteration, bees again incrementally construct solutions of the problem, etc. There are one or more partial solutions at the end of each iteration. The analyst-decision maker prescribes the total number of iterations. When flying through the space our artificial bees perform forward pass or backward pass. During forward pass, bees create various partial solutions. They do this via a combination of individual exploration and collective experience from the past. After that, they perform backward pass, i.e. they return to the hive. In the hive, all bees participate in a decision-making process. We assume that every bee can obtain the information about solutions' quality generated by all other bees. In this way, bees exchange information about quality of the partial solutions created. Bees compare all generated partial solutions. Based on the quality of the partial solutions generated, every bee decides whether to abandon the created partial solution and become again uncommitted follower, continue to expand the same partial solution without recruiting the nestmates, or dance and thus recruit the nestmates before returning to the created partial solution. Depending on the quality of the partial solutions generated, every bee possesses certain level of loyalty to the path leading to the previously discovered partial solution. During the second forward pass, bees expand previously created partial solutions, and after that perform again the backward pass and return to the hive. In the hive bees again participate in a decision-making process, perform third forward pass, etc. The iteration ends when one or more feasible solutions are created. Like Dynamic Programming, the BCO also solves combinatorial optimization problems in stages (Figure 1). Each of the defined stages involves one optimizing variable. Let us denote by ST= {stl, st2,..., stm5 a finite set of pre-selected stages, where m is the number of stages. By B we denote the number of bees to participate in the search process, and by I the total number of iterations. The set of partial solutions at stage stj is denoted by Sj First1, 2,..., m). Stag Stage0 0 B1 // 0 40 t.,, 2 B2 0 B3 s> / First Second Third Stage Stage Stage 0 0 0 0 0_ -'D 0 0 0 o_ o0 t t t Itt Fig. 1. First forward pass and the first backward pass 152 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:42:12 UTC from IEEE Xplore. Restrictions apply. The following is pseudo-code of the Bee Colony Optimization: Bee Colony Optimization (1) Initialization. Determine the number of bees B, and the number of iterations I. Select the set of stages ST = {Stl, st2 ,..., Stm}. Find any feasible solution x of the problem. This solution is the initial best solution. (2) Set i: 1. Until i = I, repeat the following steps: (3) Setj 1. Untilj = m, repeat the following steps: Forward pass: Allow bees to fly from the hive and to choose B partial solutions from the set of partial solutions Sj at stage stj. Backward pass: Send all bees back to the hive. Allow bees to exchange information about quality of the partial solutions created and to decide whether to abandon the created partial solution and become again uncommitted follower, continue to expand the same partial solution without recruiting the nestmates, or dance and thus recruit the nestmates before returning to the created partial solution. Set,j: =j + 1. (4) If the best solution xi obtained during the i-th iteration is better than the best- known solution, update the best known solution (x: = xi). (5) Set, i: = i + 1. perform third forward pass, etc. The iteration ends when the bees have visited all nodes. Within the proposed BCO Metaheuristic, various heuristic algorithms describing bees' behavior and/or `reasoning` could be developed and tested. In other words, various BCO algorithms could be developed. These algorithms should describe the ways in which bees decide to abandon the created partial solution, to continue to expand the same partial solution without recruiting the nestmates, or to dance and thus recruit the nestmates before returning to the created partial solution. o 0 BQ0 0 B3 B2 0 0 OIe 0 0 0 0 0 0 'I I I I I 1 First Second Third Stage Stage Stage Fig. 2. Second forward pass Alternatively, forward and backward passes could be performed until some other stopping condition is satisfied. The possible stopping conditions could be, for example, the maximum total number of forward/backward passes, or the maximum total number of forward/backward passes between two objective function value improvements. During the forward pass bees will visit certain number of nodes, create partial solution, and after that return to the hive (node 0). In the hive, bees will participate in a decision making process. Bees compare all generated partial solutions. Based on the quality of the partial solutions generated, every bee will decide whether to abandon the generated path and become again uncommitted follower, continue to fly along discovered path without recruiting the nestmates, or dance and thus recruit the nestmates before returning to the discovered path. Depending on the quality of the partial solutions generated, every bee possesses certain level of loyalty to the path previously discovered. For example, bee B1, B2, and B3 participated in the decision-making process. After comparing all generated partial solutions, bee B1 decided to abandon already generated path, and to join bee B2. The bees B1, and B2 fly together along the path generated by the bee B2. When they reach the end of the path, they are free to make individual decision about next node to be visited. The bee B3 will continue to fly along discovered path without recruiting the nestmates (Figure2). In this way, bees are again performing forward pass. During the second forward pass, bees will visit few more nodes, expand previously created partial solutions, and after that perform again the backward pass and return to the hive (node 0). In the hive, bees will again participate in a decision making process, make a decision, In addition to proposing the BCO as a new metaheuristic, we also describe in the paper two BCO algorithms that we call the Bee System (BS) and the Fuzzy Bee System (FBS). The BS proposed in [5] is described in more details within the Case Study of the Traveling Salesman Problem. In the case of FBS [7], the agents (artificial bees) use approximate reasoning and rules of fuzzy logic in their communication and acting. In this way, the FBS is capable to solve deterministic combinatorial problems, as well as combinatorial problems characterized by uncertainty. The FBS is described in details within the Case Study of Ride- Matching problem. III. CASE STUDY # 1: SOLVING THE TRAVELING SALESMAN PROBLEM BY THE BEE COLONY OPTIMIZATION The proposed Bee System was tested on a large number of numerical examples. The benchmark problems were taken from the following Internet address: The following problems were considered: Eil5 1 .tsp, Berlin52.tsp, St70.tsp, Pr76.tsp, Kroal OO.tsp and a280.tsp. All tests were run on an IBM compatible PC with PIll processor (533MHz). The results obtained are given in Table 1. We can see from the Table 1 that the proposed BCO produced results of a very high quality. The BCO was able to obtain the objective function values that are very close to the optimal values of the objective function. The times required to find the best solutions by the BCO are very low. In other words, the BCO was able to produce `very good` solutions in a `reasonable amount` of computer time. 153 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:42:12 UTC from IEEE Xplore. Restrictions apply. TABLE 1: THE RESULTS OBTAINED BY THE BEE COLONY OPTIMIZATION Problem The best Name Optimal value B0 CP (Numbe Value obtained by (B secPU ofnodes)| (0) the Bee 0noes S~~ystem (B) Eil5l 51 429.983 431.121 0.26% 44 Ber(l5i2n)52 7544.36 7544.366 Oo% 18 St70 (70) 678.597 678.621 0.00350o 238 Pr76 (76) 108159 108790 0.58% 127 KroalOO 21285.4 21441.5 0.730% 58 (10)0 A280 2586.77 2740.63 5.950o 1855 (280) _________________ IV. CASE STUDY #2: SOLVING THE RIDE-MATCHING PROBLEM BY THE FuzzY BEE SYSTEM Urban road networks in many countries are severely congested, resulting in increased travel times, increased number of stops, unexpected delays, greater travel costs, inconvenience to drivers and passengers, increased air pollution and noise level, and increased number of traffic accidents. Expanding traffic network capacities by building more roads is extremely costly as well as environmentally damaging. More efficient usage of the existing supply is vital in order to sustain the growing travel demand. Ridesharing is one of the widely spread Travel Demand Management (TDM) techniques that assumes the participation of two or more persons that all together share vehicle when traveling from few origins to few destinations. All drivers that participate in ride-sharing offer to the operator the following information regarding trips planned for the next week: (a) Vehicle capacity (2, 3, or 4 persons); (b) Days in the week when person is ready to participate in ride-sharing; (c) Trip origin for every day in a week; (d) Trip destination for every day in a week; (e) Desired departure and/or arrival time for every day in a week. The ride-matching problem considered in [7] could be defined in the following way: Make routing and scheduling of the vehicles and passengers for the whole week in the `best possible way`. The following are potential objective functions: (a) Minimize the total distance traveled by all participants; (b) Minimize the total delay; (c) Make relatively equals vehicle utilization. We deal with the deterministic combinatorial optimization problem in the case when the desired departure and/or arrival times are fixed (For example `I want to be picked-up exactly at 8:00 a.m.). On the other hand, in many real-life situations the desired departure and/or arrival times are fuzzy (I want to be picked-up about 8:00 a.m.). In this case, the ride-matching problem should be treated as a combinatorial optimization problem characterized by uncertainty. We solve the problem described by the Fuzzy Bee System. A. The Fuzzy Bee System Bees face many decision-making problems while searching for the best solution. The following are bees' choice dilemmas: (a) What is the next solution component to be added to the partial solution?; (b) Should the partial solution be abandon or not?; (c) Should the same partial solution be expanded without recruiting the nestmates? 444 Long I~~~~~~ iDNan isac O \\ 500 \\ 1 000 M edium Short Fig. 3. Fuzzy sets describing distance The majority of the choice models are based on random utility modeling concepts. These approaches are highly rational. They are based on assumptions that decision-makers possess perfect information processing capabilities and always behave in a rational way (trying to maximize utilities). In order to offer alternative modeling approach, researchers started to use less normative theories. The basic concepts of Fuzzy Sets Theory [8], linguistic variables, approximate reasoning, and computing with words have more understanding for uncertainty, imprecision, and linguistically expressed observations. Following these ideas, we start in our choice model from the assumption that the quantities perceived by bees are `fuzzy` [8]. Artificial bees use approximate reasoning and rules of fuzzy logic in their communication and acting. During the j-th stage bees fly from the hive and choose B partial solutions from the set of partial solutions Si at stage stj (forward pass). When adding the solution component to the current partial solution during the forward pass, specific bee perceives specific solution component as `less attractive`, attractive`, or `very attractive`. We also assume that an artificial bee can perceive a specific attributes as `short`, `medium` or `long` (Figure 3), `cheap`, `medium`, or `expensive`, etc. B. Calculating the solution component attractiveness and choice ofthe next solution component to be added to the partial solution The approximate reasoning algorithm for calculating the solution component attractiveness consists of the rules of the following type: If the attributes of the solution component are VERY GOOD Then the considered solution component is VERY ATTRACTIVE The main advantage of using the approximate reasoning algorithm for calculating the solution component attractiveness is that it is possible to calculate solution component attractiveness even if some of the input data were only approximately known. Let us denote byf the attractiveness value of solution component i. The probability pi for solution component i to be added to the partial solution is equal to the ratio off to the sum of all considered solution component attractiveness values: 154 Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:42:12 UTC from IEEE Xplore. Restrictions apply. Pi= /f (1) Efj In order to choose next solution component to be added to the partial solution, artificial bees use a proportional selection known as the `roulette wheel selection.` (The sections of roulette are in proportion to probabilities pi). In addition to the `roulette wheel selection,` several other ways of selection could be used. C. Bee's partial solutions comparison mechanism In order to describe bee's partial solutions comparison mechanism, we introduce the concept of partial solution badness. We define partial solution badness in the following way: Lk LL(k) ~-mLin Lmax -Lmin where: Lk - badness of the partial solution discovered by the k-th bee L(k) - the objective function values of the partial solution discovered by the k-th bee Lmin -the objective function value of the bestdiscovered partial solution from the beginning of the search process Lmax - the objective function value of the worst discovered partial solution from the beginning of the search process The approximate reasoning algorithm to determine the partial solution badness consists of the rules of the following type: If the discovered partial solution is BAD Then loyalty is LOW Bees use approximate reasoning, and compare their discovered partial solutions with the best, and the worst discovered partial solution from the beginning of the search process. In this way, `historical facts` discovered by the all members of the bee colony have significant influence on the future search directions. D. Calculating the number ofbees changing the path Every partial solution (partial path) that is being advertised in the dance area has two main attributes: (a) the objective function value, and (b) the number of bees that are advertising the partial solution (partial path). The number of bees that are advertising the partial solution is a good indicator of a bees' collective knowledge. It shows how bee colony perceives specific partial solutions. The approximate reasoning algorithm to determine the advertised partial solution attractiveness consists of the rules of the following type: If the length of the advertised path is SHORT and the number of bees advertising the path is SMALL Then the advertised partial solution attractiveness is MEDIUM Path attractiveness calculated in this way can take values from the interval [0,1]. The higher the calculated value, the more attractive is advertised path. Bees are less or more loyal to `old` paths. At the same time, advertised paths are less, or more attractive to bees. Let us note paths Pi and p1. We denote by nij the number of bees that will abandon path Pi , and join nestmates who will fly along path pj. The approximate reasoning algorithm to calculate the number of shifting bees consists of the rules of the following type: If bees' loyalty to path Pi is LOW and path pj 's attractiveness is HIGH Then the number of shifting bees from path Pi to path p1 is HIGH In this way, the number of bees flying along specific path is changed before beginning of the new forward pass. Using collective knowledge and sharing information among themselves, bees concentrate on more promising search paths, and slowly abandon less promising paths. ways of selection could be used. E. Numerical experiment The authors in [7] tested the proposed model in the case of ridesharing demand from Trani, a small city in the southeastern Italy, to Bari, the region capital of Puglia They collected the data regarding 97 travelers demanding for ridesharing, and assumed, for sake of simplicity, that the capacity is 4 passengers for all their cars. In our case, the algorithm chooses 24*4 = 96 out of 97 travelers to build up the `best` path. The authors used a hive of 15 bees, leaving at once. Bees have generated only six `foraging paths`. The other generated paths were abandoned eventually. Changes of the best discovered objective function values are shown in Figure 4. Fig. 4. Changes of the best-discovered objective function values V. CASE STUDY #3: ROUTING AND WAVELENGTH ASSIGNMENT (RWA) IN ALL-OPTICAL NETWORKS Every pair of nodes in optical networks is characterized by a number of requested connections. The total number of established connections in the network depends on the routing and wavelength assignment procedure. Routing and wavelength assignment (RWA) problem in all-optical networks could be defined in the following way: Assign a path through the network and a wavelength on that path for each considered connection between a pair of nodes in 155 i~~~~~~~~~~~~~~~~~~~ Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:42:12 UTC from IEEE Xplore. Restrictions apply. such a way to maximize the total number of established connections in the network. In [9] the authors proposed the BCO heuristic algorithm tailored for the RWA problem. They called the proposed algorithm the BCO-RWA algorithm. Bees decide to choose a physical route in optical network in a random manner. Logit model is one of the most successful and widely accepted discrete choice model. Inspired by the Logit model, the authors in [9] assumed that the probability p,'d of choosing route r in the case of origin-destination pair (s,d) equals: Vd e reRsd iW >0 Pprsd= Eisd ((33)) O,VreRsd iW =0 where Rsd is the total number of available routes between pair of nodes (s,d). The route r is availlable if there is at least one available wavelength on all links that belong to the route r. In the hive every bee makes the decision about abandoning the created partial solution or expanding it in the next forward pass. The authors in [9] assumed that every bee can obtain the information about partial solution quality created by every other bee. They calculated the probability that the bee b will at beginning of the u + 1 forward pass use the same partial tour that is defined in forward pass u in the following way: _ C.. Cb Pb =e u (4) where: Cb - the total number of established lightpaths from the beginning of the search process by the b-th bee Cmax - the maximal number of established lightpaths from the beginning of the search process by any bee u - ordinary number of forward pass, u=1,2,..., U TABLE 2: THE RESULTS COMPARISON Total Number of CPU time number of Number of established [s] Relative requested wave- lightpathsero % light-paths lengths ILP BCO- ILP BCO-ero[0] RWA RWA 1 14 14 4 4.33 0 28 2 23 23 94 4.58 0 28 32 27 27 251 4.68 0 4 28 28 313 4.66 0 1 15 14 4 4.73 6.67 31 2 25 25 83 5.00 0 3 30 30 235 5.19 0 4 31 31 1410 5.21 0 1 15 14 14 5.19 6.67 34 2 27 26 148 5.50 3.70 3 33 33 216 5.64 0 4 34 34 906 5.64 0 1 16 15 23 5.64 6.25 36 32 27 26 325 6.09 3.70 36 34 34 788 6.11 0 4 36 36 1484 6.13 0 1 17 16 16 5.67 5.88 38 2 28 27 247 6.09 3.57 38 35 35 261 6.23 0 4 38 38 1773 6.33 0 1 17 16 31 6.00 5.88 40 2 28 27 491 6.28 3.57 4 35 35 429 6.61 0 4 40 40 1346 6.67 0 The authors in [9] calculated the probability pp that the P-th advertised partial solution will be chosen by any of the uncomitted follower using the following relation: eCp PP = p Ecp p=1 ,(5) where Cp is the total number of the established lightpaths in the case of the P-th advertised partial solution The BCO-RWA algorithm was tested on a few numerical examples. The authors in [9] fornulated corresponding Integer Linear Program (ILP) and discovered optimal solutions for the considered examples. In the next step, they compared the BCO-RWA results with the optimal solution. The comparison for the considered network is shown in the Table 2. We can see from the Table 2 that the proposed BCORWA algorithm has been able to produce optimal, or a near-optimal solutions in a reasonable amount of computer time. VI. CONCLUSION The successful applications of the Bee Colony Optimization to difficult combinatorial optimization problems are very encouraging. It is of a great importance to investigate in future research both advantages and disadvantages of autonomy, distributed functioning and self-organizing in relation to traditional engineering methods relying on control and centralization. ACKNOWLEDGMENT This research was partially supported by the Ministry of Sciences of Serbia. REFERENCES [1] G. Beni, `The concept of Cellular Robotic System`, in Proceedings 1988 IEEE International Symposium on Intelligent Control, Los Alamitos, CA, IEEE Computer Society Press, 1988, pp.57-62. [2] G. Beni, J. Wang, `Swarm Intelligence, in Proceedings Seventh Annual Meeting of the Robotics Society of Japan, Tokyo, RSJ Press, 1989, pp.425-428. [3] E.Bonabeau, M. Dorigo, G. Theraulaz, G., `Swarm Intelligence`, Oxford University Press, Oxford, (1999). [4] S. Camazine, J. Sneyd, `A Model of Collective Nectar Source by Honey Bees: Self-organization Through Simple Rules`, Journal of Theoretical Biology, vol.149, pp.547-571, 1991. [5] P. Lu6i6, D.Teodorovi6, `Bee System: Modeling Combinatorial Optimization Transportation Engineering Problems by Swarm Intelligence`, in Preprints of the TRISTAN IV Triennial Symposium on Transportation Analysis, Sao Miguel, Azores Islands, Portugal 2001, pp. 441-445. [6] P. Lucic, D. Teodorovic, `Computing with Bees: Attacking Complex Transportation Engineering Problems`, International Journal on Artificial Intelligence Tools, vol. 12, pp. 375-394, 2003. [7] D. Teodorovic, M. Dell' Orco, `Bee Colony Optimization-A cooperative learning approach to complex transportation problems` in Abstracts - of 10th EWGT Meeting and 16th Mini EURO Conference, Poznan, 2005. [8] L. Zadeh, `Fuzzy Sets`, Information and Control, vol. 8, pp. 338- 353, 1965. [9] G. Markovic, D. Teodorovic, V. Acimovic-Raspopovic, `Routing and Wavelength Assignment in All-Optical Networks Based on the Bee Colony Optimization`, Submitted for publication, 2006 156\"],\n",
    "[12,4,1,\"Ant System: Optimization by a Colony of Cooperating Agents Marco Dorigo, Member, ZEEE, Vittorio Maniezzo, and Albert0 Colorni 29 Abstract-An analogy with the way ant colonies function has suggested the definition of a new computational paradigm, which we call Ant System. We propose it as a viable new approach to stochastic combinatorial optimization. The main characteristics of this model are positive feedback, distributed computation, and the use of a constructive greedy heuristic. Positive feedback accounts for rapid discovery of good solutions, distributed computation avoids premature convergence, and the greedy heuristic helps find acceptable solutions in the early stages of the search process. We apply the proposed methodology to the classical Traveling Salesman Problem (TSP), and report simulation results. We also discuss parameter selection and the early setups of the model, and compare it with tabu search and simulated annealing using TSP. To demonstrate the robustness of the approach, we show how the Ant System (AS) can be applied to other optimization problems like the asymmetric traveling salesman, the quadratic assignment and the job-shop scheduling. Finally we discuss the salient characteristics-global data structure revision, distributed communication and probabilistic transitions of the AS. I. INTRODUCTION IN this paper we define a new general-purpose heuristic al- gorithm which can be used to solve different combinatorial optimization problems. The new heuristic has the following desirable characteristics: It is versatile, in that it can be applied to similar versions of the same problem; for example, there is a straightforward extension from the traveling salesman problem (TSP) to the asymmetric traveling salesman problem (ATSP). It is robust. It can be applied with only minimal changes to other combinatorial optimization problems such as the quadratic assignment problem (QAP) and the job-shop scheduling problem (JSP). It is a population based approach. This is interesting because it allows the exploitation of positive feedback as a search mechanism, as explained later in the paper. It also Manuscript received November 15, 1991; revised September 3, 1993, July 2, 1994, and December 28, 1994. M. Dorigo was with the Progetto di Intelligenza Artificiale e Robotica, Dipartimento di Elettronica e Informazione, Politecnico di Milano, 20133 Milano, Italy. He is now with INDIA, Universite’ Libre de Bruxelles, 1050 Bruxelles, Belgium (e-mail: mdorigo@ulb.ac.be, http://iridia.ulb.ac. be/dorigo/dorigo.html). V. Maniezzo was with the Progetto di Intelligenza Artificiale e Robotica, Dipartimento di Elettronica e Informazione, Politecnico di Milano, 20133 Milano, Italy. He is now with Dipartimento di Scienze dell’Informazione, Universita’ di Bologna, 47023 Cesena, Italy (e-mail: maniezzo@csr.unibo.it, http://www.csr.unibo.it/-maniezzo). A. Colorni is with the Dipartimento di Elettronica e Informazione, Politecnico di Milano, 20133 Milano, Italy (e-mail: colorni@elet.polimi.it). Publisher Item Identifier S 1083-4419(96)00417-7 makes the system amenable to parallel implementations (though this is not considered in this paper). These desirable properties are counterbalanced by the fact that, for some applications, the Ant System can be outperformed by more specialized algorithms. This is a problem shared by other popular approaches like simulated annealing (SA), and tabu search (TS), with which we compare the Ant System. Nevertheless, we believe that, as is the case with SA and TS, our approach is meaningful in view of applications to problems which, although very similar to well known and studied basic problems, present peculiarities which make the application of the standard best-performing algorithm impossible. This is the case, for example, with the ATSP. In the approach discussed in this paper we distribute the search activities over so-called “ants,” that is, agents with very simple basic capabilities which, to some extent, mimic the behavior of real ants. In fact, research on the behavior of real ants has greatly inspired our work (see [lo], [ll], [21]). One of the problems studied by ethologists was to understand how almost blind animals like ants could manage to establish shortest route paths from their colony to feeding sources and back. It was found that the medium used to communicate information among individuals regarding paths, and used to decide where to go, consists of pheromone trails. A moving ant lays some pheromone (in varying quantities) on the ground, thus marking the path by a trail of this substance. While an isolated ant moves essentially at random, an ant encountering a previously laid trail can detect it and decide with high probability to follow it, thus reinforcing the trail with its own pheromone. The collective behavior that emerges is a form of autocatalytic behavior’ where the more the ants following a trail, the more attractive that trail becomes for being followed. The process is thus characterized by a positive feedback loop, where the probability with which an ant chooses a path increases with the number of ants that previously chose the same path. Consider for example the experimental setting shown in Fig. 1. There is a path along which ants are walking (for example from food source A to the nest E, and vice versa, see Fig. l(a)). Suddenly an obstacle appears and the path is cut off. So at position B the ants walking from A to E (or at position D those walking in the opposite direction) have to decide whether to turn right or left (Fig. l(b)). The choice is influenced by the intensity of the pheromone trails left by preceding ants. A higher level of pheromone on the ‘An autocatalytic [12], i.e. positive feedback, process is a process that reinforces itself, in a way that causes very rapid convergence and, if no limitation mechanism exists, leads to explosion. 10834419/96$05.00 0 1996 13:41:51 UTC from IEEE Xplore. Restrictions apply. 30 E A E &I8 88 IEEE TRANSACTIONS ON SYSTEMS. MAN, AND CYBERNETICS-PART B: CYBERNETICS, VOL. 26, NO. 1, FEBRUARY 1996 i f A f A (a) (b) (c) Fig. 1. An example with real ants. (a) Ants follow a path between points A and E. (b) An obstacle is interposed; ants can choose to go around it following one of the two different paths with equal probability. (c) On the shorter path more pheromone is laid down. right path gives an ant a stronger stimulus and thus a higher probability to turn right. The first ant reaching point B (or D) has the same probability to turn right or left (as there was no previous pheromone on the two alternative paths). Because path BCD is shorter than BHD, the first ant following it will reach D before the first ant following path BHD (Fig. l(c)). The result is that an ant returning from E to D will find a stronger trail on path DCB, caused by the half of all the ants that by chance decided to approach the obstacle via DCBA and by the already arrived ones coming via BCD: they will therefore prefer (in probability) path DCB to path DHB. As a consequence, the number of ants following path BCD per unit of time will be higher than the number of ants following EHD. This causes the quantity of pheromone on the shorter path to grow faster than on the longer one, and therefore the probability with which any single ant chooses the path to follow is quickly biased toward the shorter one. The final result is that very quickly all ants will choose the shorter path. The algorithms that we are going to define in the next sections are models derived from the study of real ant colonies. Therefore we call our system Ant System (AS) and the algorithms we introduce ant algorithms. As we are not interested in simulation of ant colonies, but in the use of artificial ant colonies as an optimization tool, our system will have some major differences with a real (natural) one: e artificial ants will have some memory, 0 they will not be completely blind, 0 they will live in an environment where time is discrete. Nevertheless, we believe that the ant colony metaphor can be useful to explain our model. Consider the graph of Fig. 2(a), which is a possible AS interpretation of the situation of Fig. l(b). To fix the ideas, suppose that the distances between D and H, between B and H, and between B and D-via C-are equal to 1, and let C be positioned half the way between D and B (see Fig. 2(a)). Now let us consider what happens at regular discretized intervals of time: t = 0,1,2, new ants come to B from A, and 30 to D from E at each time unit, that each ant walks at a speed of 1 per time unit, and that I A (a) I A Fig. 2. An example with artificial ants. (a) The initial graph with distances. @) At time t = 0 there is no trail on the graph edges; therefore ants choose whether to turn right or left with equal probability. (c) At time t = 1 trail is stronger on shorter edges, which are therefore, in the average, preferred by ants. while walking an ant lays down at time t a pheromone trail of intensity 1, which, to make the example simpler, evaporates completely and instantaneously in the middle of the successive time interval (t + 1, t + 2). At t = 0 there is no trail yet, but 30 ants are in B and 30 in D. Their choice about which way to go is completely random. Therefore, on the average 15 ants from each node will go toward H and 15 toward C (Fig. 2(b)). At t = 1 the 30 new ants that come to B from A find a trail of intensity 15 on the path that leads to H, laid by the 15 ants that went that way from E, and a trail of intensity 30 on the path to C, obtained as the sum of the trail laid by the 15 ants that went that way from B and by the 15 ants that reached B coming from D via C (Fig. 2(c)). The probability of choosing a path is therefore biased, so that the expected number of ants going toward C will be the double of those going toward H: 20 versus 10 respectively. The same is true for the new 30 ants in D which came from E. This process continues until all of the ants will eventually choose the shortest path. The idea is that if at a given point an ant has to choose among different paths, those which were heavily chosen by preceding ants (that is, those with a high trail level) are chosen with higher probability. Furthermore high trail levels are synonymous with short paths. The paper is organized as follows. Section I1 contains the description of the AS as it is currently implemented and the definition of the application problem: as the algorithm structure partially reflects the problem structure, we introduce them together. Section I11 describes three slightly different ways to apply the proposed algorithm. Sections IV and V report on experiments. In Section VI we compare the AS with other heuristics, and in Section VI1 we substantiate the versatility and robustness of the AS by showing how it can be applied to other optimization problems. In Section VI11 we informally discuss why and how the AS paradigm functions. Conclusions are in Section IX. 11. THE ANT SYSTEM In this section we introduce the AS. We decided to use the well-known traveling salesman problem [26] as benchmark, in order to make the comparison with other heuristic approaches Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:41:51 UTC from IEEE Xplore. Restrictions apply. DORIGO et al.: ANT SYSTEM OPTIMIZATION BY A COLONY OF COOPERATING AGENTS 31 easier [2O]. Although the model definition is influenced by the problem structure, we will show in Section VI1 that the same approach can be used to solve other optimization problems. Given a set of n towns, the TSP can be stated as the problem of finding a minimal length closed tour that visits each town once. We call d,, the length of the path between towns i and j ; in the case of Euclidean TSP, di, is the Euclidean distance between i and j (i.e., d,, = [(z, - 2,)' + (y, - TJ,)']~/'). An instance of the TSP is given by a graph (N, E), where N is the set of towns and E is the set of edges between towns (a fully connected graph in the Euclidean TSP). Let b,(t) (i = 1, . . . , n) be the number of ants in town i at time t and let m = b,(t) be the total number of ants. Each ant is a simple agent with the following characteristics: it chooses the town to go to with a probability that is a function of the town distance and of the amount of trail present on the connecting edge; to force the ant to make legal tours, transitions to already visited towns are disallowed until a tour is completed (this is controlled by a tabu list); when it completes a tour, it lays a substance called trail on each edge ( i , j ) visited. Let r,, (t) be the intensity of trail on edge (i, j ) at time t. Each ant at time t chooses the next town, where it will be at time t + 1. Therefore, if we call an iteration of the AS algorithm the m moves carried out by the m ants in the interval (t, t + l), then every n iterations of the algorithm (which we call a cycle) each ant has completed a tour. At this point the trail intensity is updated according to the following formula (1) where p is a coefficient such that (1 - p) represents the evaporation of trail between time t and t + n, Tz, (t + n) = p . T,, (t) + AT,, m k=l where AT$ is the quantity per unit of length of trail substance (pheromone in real ants) laid on edge ( i , j ) by the kth ant between time t and t + n; it is given by E if kth ant uses edge ( i , j ) in its tour (between time t and t + n) = ( 3 ) 8, l o otherwise where Q is a constant and Lk is the tour length of the kth ant. The coefficient p must be set to a value < 1 to avoid unlimited accumulation of trail (see note 1). In our experiments, we set the intensity of trail at time O , r i j ( O ) , to a small positive constant c. In order to satisfy the constraint that an ant visits all the n different towns, we associate with each ant a data structure called the tabu lis$, that saves the towns already visited up to 'Even though the name chosen recalls tabu search, proposed in [17], [18], there are substantial differences between our approach and tabu search algorithms. We mention here: (i) the absence in the AS of any aspiration function, (ii) the difference of the elements recorded in the tabu list, permutations in the case of tabu search, nodes in the AS (our algorithms are constructive heuristics, which is not the case of tabu search). time t and forbids the ant to visit them again before n iterations (a tour) have been completed. When a tour is completed, the tabu list is used to compute the ant's current solution (i.e., the distance of the path followed by the ant). The tabu list is then emptied and the ant is free again to choose. We define tabuk the dynamically growing vector which contains the tabu list of the kth ant, tabUk the set obtained from the elements of tabuk, and tabuk(s) the sth element of the list (i.e., the sth town visited by the kth ant in the current tour). We call visibility q;j the quantity l/dij. This quantity is not modified during the run of the AS, as opposed to the trail which instead changes according to the previous formula (1). We define the transition probability from town i to town j for the kth ant as where allowedk = {N-tabuk} and where a and p are parameters that control the relative importance of trail versus visibility. Therefore the transition probability is a trade-off between visibility (which says that close towns should be chosen with high probability, thus implementing a greedy constructive heuristic) and trail intensity at time t (that says that if on edge (i,j) there has been a lot of traffic then it is highly desirable, thus implementing the autocatalytic process). 111. THE ALGORITHMS Given the definitions of the preceding section, the socalled ant-cycle algorithm is simply stated as follows. At time zero an initialization phase takes place during which ants are positioned on different towns and initial values rz, (0) for trail intensity are set on edges. The first element of each ant's tabu list is set to be equal to its starting town. Thereafter every ant moves from town i to town j choosing the town to move to with a probability that is a function (with parameters a and p, see formula (4)) of two desirability measures. The first, the trail T,, ( t ) ,g ives information about how many ants in the past have chosen that same edge (i, j ) ; the second, the visibility q,,, says that the closer a town the more desirable it is. Obviously, setting a = 0, the trail level is no longer considered, and a stochastic greedy algorithm with multiple starting points is obtained. After n iterations all ants have completed a tour, and their tabu lists will be full; at this point for each ant k the value of Lk is computed and the values Ar; are updated according to formula (3). Also, the shortest path found by the ants (i.e., mink Lk, k = 1, . . . , m) is saved and all the tabu lists are emptied. This process is iterated until the tour counter reaches the maximum (user-defined) number of cycles NCMAX, or all ants make the same tour. We call this last case stagnation behavior because it denotes a situation in which the algorithm stops searching for alternative solutions. We investigate this situation in Section IV. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:41:51 UTC from IEEE Xplore. Restrictions apply. 32 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS-PART B CYBERNETICS, VOL 26, NO 1, FEBRUARY 1996 Formally the ant-cycle algorithm is: 1. Initialize: Set t := 0 Set NC := 0 For every edge (i,j) set an initial value ri3(t)= c for trail intensity and ArZ3 = 0 Place the m ants on the n nodes {t is the time counter} {NC is the cycles counter} 2. Set s := 1 { s is the tabu list index} For k := 1 to m do Place the starting town of the kth ant in tabuk(s) 3. Repeat until tabu list is full {this step will be repeated (n - 1) times} Set s := s + 1 For k := 1 to m do Choose the town j to move to, with probability P:j (t) given by Eq. (4) {at time t the kth ant is on town i = tabuk(s - 1)) Move the kth ant to the town j Insert town j in tabuk(s) 4. For k := 1 to m do Move the kth ant from tabuk(n) to tabuk(1) Compute the length LI, of the tour described by the kth ant Update the shortest tour found For every edge ( i , j ) For k:= 1 to m do E 0 otherwise if (i,j) E tour described by tabuk 5. For every edge ( 2 , j ) c ompute rZ3(t + n) according to equation rZj(+t n) = p . r,,(t) + Arz3 Set t:=t+n Set NC := NC + 1 For every edge (i, j ) set ArZj := 0 then 6. If (NC < NCMAX) and (not stagnation behavior) Empty all tabu lists Goto step 2 Print shortest tour else stop The complexity of the ant-cycle algorithm i s O(NC.n2 .rn) if we stop the algorithm after NC cycles. In fact step 1 is O(n2 + m), step 2 is O(rn), step 3 i s O(n2 . m), step 4 is O(n2. m) ,s tep 5 is O(n2) ,s tep 6 is O(n.rn).S ince we have experimentally found a linear relation between the number of towns and the best number of ants (see Section V-A), the complexity of the algorithm is O(NC . n3). We also experimented with two other algorithms of the AS, which we called ant-density and ant-quantity algorithms [6], [12]. They differ in the way the trail is updated. In these two models each ant lays its trail at each step, without waiting for the end of the tour. In the ant-density model a quantity Q of trail is left on edge ( i , j ) every time an ant goes from i to j ; in the ant-quantity model an ant going from i to j leaves a quantity Q/d,, of trail on edge (i,j) every time it goes from i to j . Therefore, in the ant-density model we have Q 0 otherwise if the kth ant goes from i and j between time t to t + 1 (5) if the kth ant goes from i to j between time t and t + 1 AI-; = { and in the ant-quantity model we have (6) 0 otherwise. From these definitions it is clear that the increase in trail on edge (i,j) when an ant goes from z to j is independent of dZ, in the ant-density model, while it is inversely proportional to d,, in the ant-quantity model (i.e., shorter edges are made more desirable by ants in the ant-quantity model). IV. EXPERIMENTASLT UDY1 : PARAMETER SE'ITING AND BASIC PROPERTIES We implemented the three algorithms (ant-cycle, ant-density and ant-quantity) of the AS and investigated their relative strengths and weaknesses by experimentation. Since we have not yet developed a mathematical analysis of the models, which would yield the optimal parameter setting in each situation, we ran simulations to collect statistical data for this purpose. The parameters considered here are those that affect directly or indirectly the computation of the probability in formula (4): a: the relative importance of the trail, a! 2 0; 0 ,B: the relative importance of the visibility, /3 2 0; p: trail persistence, 0 5 p < 1 (1 - p can be interpreted * Q: a constant related to the quantity of trail laid by ants as trail evaporation); (see formulas (3), (3, and (6)). The number m of ants has always been set equal to the number n of cities (see Section V-A for the explanation). We tested several values for each parameter while all the others were held constant (over ten simulations for each setting in order to achieve some statistical information about the average evolution). The default value of the parameters was a! = 1, p = 1, p = 0.5, Q = 100. In each experiment only one of the values was changed, except for a and ,B, which have been tested over different sets of values, as discussed at the end of this section. The values tested were: a: E {0,0.5,1,2,5}, ,O E {0,1,2,5}, p E (0.3,0.5,0.7,0.9,0.999} and Q E {I, 100, lOOOO}. Preliminary results, obtained on small-scale problems, have been presented in [6], [7], and [12], [13]; all the tests reported in this section are based, where not otherwise stated, on the Oliver30 problem, a 30-cities problem described in [3413. All the tests have been carried out for N C M A=~ 5 000 cycles and were averaged over ten trials. 31n [34] genetic algorithms were applied to solve the Oliver30 problem; they could find a tour of length 424.635. The same result was often obtained by ant-cycle, which also found a tour of length 423.741. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:41:51 UTC from IEEE Xplore. Restrictions apply. DORIGO et al.: ANT SYSTEM: OPTIMIZATION BY A COLONY OF COOPERATINI ant-density ant-quantity ant-cycle TABLE I COMPARISONA MONGA NT-QUANTITYA,N T-DENSITY, AND ANT-CYCLEA. VERAGESO VER 10 TRIALS Best parameter set Average result Best result a=l, p=5, pO.99 426.740 424.635 a=l, p=5, p0.99 427.315 426.255 a=l, p=5, p0.5 424.250 423.741 To compare the three models we first experimentally determined the parameters best values for each algorithm, and then we ran each algorithm ten times using the best parameters set. Results are shown in Table I. Parameter Q is not shown because its influence was found to be negligible. Both the ant-density and the ant-quantity models have given worse results than those obtained with ant-cycle. The reason is to be found in the kind of feedback information which is used to direct the search process. Ant-cycle uses global information, that is, its ants lay an amount of trail which is proportional to how good the solution produced was. In fact, ants producing shorter paths contribute a higher amount of trail than ants whose tour was poor. On the other side, both ant-quantity and ant-density use local information. Their search is not directed by any measure of the final result achieved. Therefore, it is not surprising that they gave worse performance results (details can be found in [6]). The optimal value p = 0.5 in ant-cycle can be explained by the fact that the algorithm, after using the greedy heuristic to guide search in the early stages of computation, starts exploiting the global information contained in the values rt3 of trail. Ant-cycle needs therefore to have the possibility to forget part of the experience gained in the past in order to better exploit new incoming global information. Given that we found ant-cycle to be superior to the other two algorithms, we decided to deepen our understanding of ant-cycle alone. Figs. 3-5 present traces of a typical run of the ant-cycle algorithm applied to the Oliver30 problem. In particular, Fig. 3 shows the length of the best found tour at each cycle, and Fig. 4 the standard deviation of the tour lengths of the population at each cycle of the same run. Note how in the early cycles the AS identifies good tours which are subsequently refined in the rest of the run. Since the standard deviation of the population’s tour lengths never drops to zero, we are assured that the algorithm actively searches solutions which differ from the best-so-far found, which gives it the possibility of finding better ones. The search for better solutions is carried on in selected regions of the search space determined by the trail resulting from preceding cycles. This can be observed in Fig. 5, in which the vertical axis shows the average node branching of the problem’s graph. Although the graph is initially fully connected, those arcs whose trail level falls below a (very small) value E, which makes their probability of being chosen by ants negligible, are removed. The node branching of node i is therefore given by the number of edges which exit from node i and which have a trail level higher than E. Note how at the beginning of the run an ant could go from any node to any other (except for tabu 3 AGENTS 33 Best lour length 400 Cycles 300 I 0 500 1000 1500 2000 2500 3000 Fig. 3. Evolution of best tour length (Oliver30). Typical run. Tour lenglh standard deviation 60 Cycles 0 500 lob0 li00 aooo 2500 3000 _ . Fig. 4. (Oliver30). Typical run. Evolution of the standard deviation of the population’s tour lengths Average no& branching 5 -- Cycles 0 500 1000 1500 2000 2500 3000 0 - I Fig. 5. (Oliver30). Typical run. Evolution of the average node branching of the problem’s graph list constraints), while at the end the possible choices are significantly reduced. The same process can be observed in the graphs of Fig. 6, where the AS was applied to a very simple 10-cities problem (CCAO, from [20]), and which depict the effect of ant search on the trail distribution. In the figure the length of the edges is proportional to the distances between the towns; the thickness of the edges is proportional to their trail level. Initially (Fig. 6(a)) trail is uniformly distributed on every edge, and search is only directed by visibilities. Later on in the search process (Fig. 6(b)) trail has been deposited on the edges composing good tours, and is evaporated completely from edges which belonged to bad tours. The edges of the worst tours actually resulted to be deleted from the problem graph, thus causing a reduction of the search space. Besides the tour length, we also investigated the stagnation behnvior, i.e. the situation in which all the ants make the same tour. This indicates that the system has ceased to explore new possibilities and no better tour will arise. With some parameter settings we observed that, after several cycles, all the ants followed the same tour despite the stochastic nature of the algorithms because of a much higher trail level on the edges Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:41:51 UTC from IEEE Xplore. Restrictions apply. 34 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS-PART B. CYBERNETICS, VOL. 26, NO. 1, FEBRUARY 1996 10 3 5 (a) CD) Fig. 6. Evolution of trail distribution for the CCAO problem. (a) Trail distribution at the beginning of search. (b) Trail distribution after 100 cycles. Average node branching 15 .. 10 5 - - Fig. 8. Ant-cycle behavior for different combinations of C Y - ~ parameters. *-The algorithm finds the best known solution without entering the stagnation behavior. oo-The algorithm doesn’t find good solutions without entering 0 500 1000 1500 2000 2500 3000 the stagnation behavior. @-The algorithm doesn’t find good solutions and .- 2 , ..... 0 - ..... . ................................... .........._. .. enters the stagnation behavior. Fig. 7. (Oliver30). Typical run obtained setting cy = 5 and p = 2. Average node branching of a rnn going to stagnation behavior comprising that tour than on all the others. This high trail level made the probability that an ant chooses an edge not belonging to the tour very low. For an example, see the Oliver30 problem, whose evolution of average branching is presented in Fig. 7. In fact, after 2500 cycles circa, the number of arcs exiting from each node sticks to the value of 2, which-given the symmetry of the problem-means that ants are always following the same cycle. This led us to also investigate the behavior of the ant-cycle algorithm for different combination of parameters a and p (in this experiment we set N C M A ~= 2500). The results are summarized in Fig. 8, which was obtained running the algorithm ten times for each couple of parameters, averaging the results and ascribing each averaged result to one of the three following different classes. 0 Bad solutions and stagnation: For high values of a the without finding very good solutions. This situation is * Bad solutions and no stagnation: If enough importance was not given to the trail (i.e., Q was set to a low value) algorithm enters the stagnation behavior very quickly 20 Fig. 9. The best tour obtained with 342 cycles of the ant-cycle algorithm for the Oliver30 problem (CY = 1, p = 5, p = 0.5, Q = loo), real length =423.741, integer length =420. represented by the symbol 0 in Fig. 8; then the algorithm did not find very good solutions. This situation is represented by the symbol 00. 0 Good solutions: Very good solutions are found for a and p values in the central area (where the symbol used is 0). In this case we found that different parameter combinations ( i . e . , ( Q = l , p = l ) , ( a ! = l , P = 2 ) , ( a : = l , p = 5 } , (a = 0.5, p = 5)) resulted in the same performance level: the same result (the shortest tour known on the Oliver30 problem) was obtained in approximately the same number values of a! make the algorithm very similar to a stochastic multigreedy algorithm. In Fig. 9 we present the best tour4 we found using the experimentally determined optimal set of parameters values for the ant-cycle algorithm, a! = 1, /3 = 5, p = 0.5, Q = 100. This tour is of length 423.741 and presents two inversions, 2-1 and 25-24, with respect to the best tour published in [34]. The major strengths of the ant-cycle algorithm can be summarized as: 0 Within the range of parameter optimality the algorithm always finds very good solutions for all the tested problems of cycles. The results obtained in this experiment are consistent with our understanding of the algoritha: a high value for a means that is very important and therefore ants tend to choose 4Th~sre sult is not competitive with results obtamed by special-purpose edges chosen by other ants in the past. On the other hand, low algorithms [2] Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:41:51 UTC from IEEE Xplore. Restrictions apply. DORIGO et al.: ANT SYSTEM: OPTIMIZATION BY A COLONY OF COOPERATING AGENTS 35 Best tour length 400 t Cycles 10 10-0 I ri 0 O- g T T-T 0 -0 T 300 ! I I I 8 0 500 1000 1500 Fig. 11. An optimal solution for the 4 x 4 grid problem. Fig. 10. new optimal value (423.741) after NC = 342 cycles. The algorithm finds good values for Oliver30 very quickly and the (Oliver30 and other problems which will be presented later). The algorithm quickly finds good solutions (see Fig. 10; for a comparison with other heuristics, see Section VI); nevertheless it doesn’t exhibit stagnation behavior, i.e. the ants continue to search for new possibly better tours. With increasing dimensions the sensitivity of the parameter values to the problem dimension has been found to be very low. We partially tested the ant-cycle algorithm on the EilonSO and Eilon75 problems [ 141 with a limited number of runs and with a number of cycles bounded by N C M A=~ 3000. Under these restrictions we never got the best-known result, but a quick convergence to satisfactory solutions was maintained for both the problems. V. EXPERIMENTASLT UDY2 : EXTENSIONASN D ADVANCEDPR OPERTIES In this section we discuss experiments which have deepened our understanding of the ant-cycle algorithm. We study how synergy affects the algorithm performance (Section V-A). We compare the performance of ant-cycle when all the ants are initially positioned on a unique starting point with the performance obtained when each ant starts from a different town (Section V-B). Finally, we study the effects of an elitist strategy which increases the importance of the ant that found the best tour (Section V-C), and the change in performance of the AS when the problem dimension increases (Section V-D). A. Synergistic Effects We ran a set of experiments to assess both the impact of the number m of ants, and the importance of communication through trail, on the efficiency of the solution process. In this case, the test problem involved finding a tour in a 4 x 4 grid of evenly spaced points: this is a problem with a priori known optimal solution (160 if each edge has length 10, see Fig. 11). The result was that there is a synergistic effect in using many ants and using the trail communication system; that is, a 1.0 t 1‘0 + (a) (b) Fig. 12. mance. In (a) a = 0, in (b) cy = 1. Synergy: Communication among ants (a > 0) improves perforin which the synergistic effects reach a maximum. The results are shown in Figs. 12 and 13. In Fig. 12 we compare a situation in which ants do not communicate (a = 0), with a situation in which they communicate (a = 1). Results show that communication is indeed exploited by the algorithm. In Fig. 13 we report an experiment in which the 4 x 4 grid problem was solved with m E {4,8,16,32,64}. The abscissa shows the total number of ants used in each set of runs, the ordinate shows the so-called one-ant cycles, that is, the number of cycles required to reach the optimum, multiplied by the number of ants used (in order to evaluate the efficiency per ant, and have comparable data). The algorithm has always been able to identify the optimum with any number m 2 4 of ants. Tests run on a set of T x T grid problems (T = 4,5,6,7,8) have substantiated our hypothesis that the optimal number of ants is close to the number of cities (m M n); this property was used in the assessment of the computational complexity (Section 111). A second set of tests has been carried out with 16 cities randomly distributed (16 cities random graph). Again we found that the optimal performance was reached with 8-16 ants, a number comparable with the dimension of the problem to be solved. B. Initialization This experiment was run in order to study whether the initial ant distribution influences the AS performance. We tested whether there is any difference between the case in which all ants at time t = 0 are in the same city and the case in which they are uniformly distributed5. We used ant-cycle run with n ants is more search-effective with communication among ants than with no communication. In case of commu- We say ants are uniformly distributed if there is, at time t = 0, the same inteeer number of ants on every town (this forces m to be a multiple of n). nicating ants, there is an “optimality point” given by m M n Uniform distribution was the default starting configuration in the experiments Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:41:51 UTC from IEEE Xplore. Restrictions apply. 36 hoblem 4x4 5x5 6 x 6 7 x 7 8x8 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS-PART B: CYEERNETICS, VOL. 26, NO. 1, FEBRUARY 1996 Best Average number of Time requlred to (dmenslon) solution cycles to find the find the optimum* optimum (seconds) 16 160 5.6 8 25 254.1 13.6 75 36 360 60 1020 49 494.1 320 13440 64 640 970 97000 1200 (0 -g 1000 6 800 0 600 ”0 nti, 400 200 z 0 C 5 4 8 16 32 64 Number m of ants Fig. 13. Number of one-ant cycles required to reach optimum as a function of the total number of ants for the 4 x 4 grid problem. Results are averaged over five runs. applied to the 16 cities random graph, to the 4 x 4 grid, and to the Oliver30 problem. In all three cases, distributing ants resulted in better performance. We also tested whether an initial random distribution of the ants over the cities performed better than a uniform one; results show that there is no significant difference between the two choices, even though the random distribution obtained slightly better results. C. Elitist Strategy We use the term “elitist strategy” (because in some way it resembles the elitist strategy used in genetic algorithms [19]) for the modified algorithm in which at every cycle the trail laid on the edges belonging to the best-so-far tour is reinforced more than in the standard version. We added to the trail of each arc of the best tour a quantity e . Q/L*, where e is the number of elitist ants6 and L* is the length of the best found tour. The idea is that the trail of the best tour, so reinforced, will direct the search of all the other ants in probability toward a solution composed by some edges of the best tour itself. The test were carried out again on the Oliver30 problem (the run was stopped after N C M A ~= 2500 cycles) and results indicated that there is an optimal range for the number of elitist ants: below it, increasing their number results in better tours discovered andlor in the best tour being discovered earlier; above it, the elitist ants force the exploration around suboptimal tours in the early phases of the search, so that a decrease in performance results. Fig. 14 shows the outcome of a test on the Oliver30 problem where this behavior is evident. D. Increasing the Problem Dimensions The algorithm complexity presented in Section 111, O(NC. presented in the previous sections. 61n our case the effect of an ant is to in crement the value of the trail on edges belonging to its tour; therefore in our case the equivalent of “saving” an individual is to reinforce its contribution. 2500 2250 2000 1750 1500 1250 loo0 750 500 250 0 Local optima: 425.82 423.91 423.74 0 1 2 4 8 12 16 20 30 Number e of elitist ants Fig. 14. number of elitist ants used (Oliver30). Results are averaged over five runs. Number of cycles required to reach a local optimum related to the n3), says nothing about the actual time required to reach the optimum. The experiment presented in this section is devoted to investigating the efficiency of the algorithm for increasing problem dimensions. Results are reported in Table I1 for the case of similar problems with increasing dimensions (T x T grids with the edge length set to 10, as in Fig. 11). It is interesting to note that, up to problems with 64 cities, the algorithm always found the optimal solution. VI. COMPARISON WITH OTHER HEURISTICS In this section we compare the efficacy of our algorithm to that of other heuristics, both tailored and general-purpose. A. Comparison with TSP-Tailored Heuristics In this section we compare ant-cycle with the heuristics contained in the package “Travel” [4]. This package represents the distances between the cities as an integer matrix and so we implemented an analogous representation in our system7. The results of the comparisons on Oliver30 are shown in Table 111, where the first column is the length of the best tour identified by each heuristic, and the second column is the improvement on the solution as obtained by the 2-opt heuristic (the 2-opt heuristic is an exhaustive exploration of all the permutations obtainable by exchanging 2 cities). Comparisons have been carried out also with the Lin-Kemighan [27] improvement of 71n this case distances between towns are integer numbers and are computed according to the standard code proposed in [3 11. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:41:51 UTC from IEEE Xplore. Restrictions apply. DONG0 et al.: ANT SYSTEM OPTIMIZATION BY A COLONY OF COOPERATING AGENTS 31 As TS SA TABLE In F’ERFORMANCE OF THE ant-cycle ALGORITHMCO MPARED WITH OTHERA PPROACHERSE. SULTS ARE AVERAGEDO VER TEN RUNS, AND ROUNDEDTO THE NEARESTIN TEGER.8 Best Average Std.dev. 420 420.4 1.3 420 420.6 1.5 422 459.8 25.1 Far Insert Near Insert Sweep Random TABLE IV PERFORMANCE OF AS COMPARED TO TS AND SA ON THE OLIVER30 PROBLEM. RESULTS ARE AVERAGEDO VERT ENR UNSU SINGI NTEGEDRIS TANCES the first-column solutions, which has been able to reduce the length of any tour to 420 (or 421, depending on the starting solution provided by the basic algorithms). Note how ant-cycle consistently outperformed 2-opt, while its efficacy-i.e., the effectiveness it has in finding very good solutions--can be compared with that of Lin-Kernighan. On the other hand, our algorithm requires a much longer computational time than any other tested special-purpose heuristic. As a general comment of all the tests, we would like to point out that, given a good parameter setting (for instance a! = 1, ,B = 5, p = 0.5, Q = 100, e = 8), our algorithm consistently found the best known solution for the Oliver30 problem, and converged quickly toward satisfactory solutions. It always identified for Oliver30 the best-known solution of length 423.741 in less than 400 cycles, and it took only =lo0 cycles to reach values under 430. The algorithm never fell into the stagnation behavior. In fact, the average branching was always greater than 2, and the average length of tours was never equal to the best tour found but remained somewhat above it. This indicates that the ants followed different tours. B. Comparison with General-purpose Heuristics We also compare ant-cycle with other general-purpose heuristics. This comparison is more fair to the AS, which in fact is a general-purpose heuristic, and not a specialized algorithm for the TSP. To run the comparisons, we implemented a Simulated Annealing (SA) [l], and a Tabu Search (TS) [17], [18]; we let each of them run 10 times on the Oliver30 data. SA used the annealing function T(t + 1) = a!T(t)w, ith a = 0.99; TS was implemented with tabu list length varying in the interval [20, 501. TS and SA, and the AS as well, were allowed to run for 1 hour on a IBM-compatible PC with 80386 Intel processor. The results are presented in Table IV. Results show that the AS for this problem was as effective as TS and better than SA, when running under the same hardware and time constraints. VII. GENERALITOYF THE APPROACH As we said in Section I, the AS is both versatile and robust. Versatility is exemplified by the ease with which AS can be applied to the asymmetric TSP (ATSP), a particular kind of TSP (Section VII-A). Robustness is exemplified by the possibility of using the same algorithm, although appropriately adapted, to solve other combinatorial optimization problems like the quadratic assignment problem (QAP), and the job-shop scheduling problem (JSP) (Section VII-B). A. Versatility: The ATSP The asymmetric traveling salesman problem is a TSP in which the distance between two nodes is not symmetric (i.e., in general d,, # &). The ATSP is more difficult than the TSP; in fact, while symmetric TSP can be solved optimally even on graphs with several thousand nodes, ATSP instances, and particularly ATSP instances where the distance matrix is almost symmetric, can be solved to the optimum only on graphs with a few dozen nodes [16], [26]. The application of the AS to the ATSP is straightforward, as no modifications of the basic algorithm are necessary. The computational complexity of a cycle of the algorithm remains the same as in the TSP application, as the only differences are in the distance and trail matrices which are no longer symmetric. We chose as test problem the RY48P problem [16], a difficult problem instance with a distance distribution that is hard to solve even with tailored heuristics and branch and bound procedures. We ran AS 5 times on it, each time for 4000 cycles. The average length of the best found tour was 14899, that is 3.3% longer than the optimal one. The average number of cycles to find this result was 1517. B. Robustness: QAP and JSP Let’s now consider the robustness of the AS approach. Many combinatorial problems can be solved by the AS. To apply the autocatalytic algorithm to a combinatorial problem requires defining: 1) an appropriate graph representation with search by many 2) the autocatalytic (i.e. positive) feedback process; 3) the heuristic that allows a constructive definition of the solutions (which we also call “greedy force”); 4) the constraint satisfaction method (that is, the tabu list). This has been done for two well-known combinatorial optimization problems-Quadratic Assignment (QAP) and Job- Shop Scheduling (JSP)--each time obtaining an adapted version of the AS that could effectively handle the relative problem. The most difficult (and ad hoc) tasks to face when applying the AS are to find an appropriate graph representation for the problem to be solved and a greedy force as heuristic. A QAP of order n is the problem that arises when trying to assign n facilities to n simple agents for the problem; Quadratic Assignment Problem: 8The name “basic” means the basic heuristic, with no improvement. locations. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:41:51 UTC from IEEE Xplore. Restrictions apply. 38 IEEE TRANSACTIONS ON SYSTEMS. MAN, AND CYBERNETICS-PART B: CYBERNETICS, VOL. 26, NO. 1, FEBRUARY 1996 Formally the problem is usually defined using two n x n (symmetric) matrices D = {dZ3}, where d,, is the distance between location i and location j ; F = { f h k } , where f h k is the flow (of information, products or some other quantity) between facility h and facility k. A permutation 7r is interpreted as an assignment of facility h = ~ ( ito) lo cation i, for each i = 1,. . . ,n. The problem is then to identify a permutation T of both row and column indexes of the matrix F that minimizes the total cost: n To apply AS to QAP we used the same algorithm as in the case of the TSP, after having studied an approximation of the QAP objective function that allows a problem representation on the basis of a single matrix which is used by the heuristic. The QAP objective function was expressed by a combination of the “potential vectors” of distance and flow matrices. The potential vectors, 2) and 3, are the row sums of each of the two matrices. Consider the following example 50 30 0 50 110 20 50 01 L 8 0 l From the two potential vectors, a third matrix S is obtained, where each element is computed as Szh = d, . fh, d, and fh being elements of the potential vectors. ~ 720 660 780 480 1200 1100 1300 800 1440 1320 1560 960 ’ 1680 1540 1820 1120 The ants choose the node to move to using the inverse of the values of S as visibility data, qzh = l/szhr thus interpreting each element Szh as the heuristic value of the choice of assigning to location i the facility h. To show how the heuristic works to build a solution we assume, for simplicity, that the ants act in a deterministic way (and not probabilistically, as it happens in the algorithm), and we don’t consider the effect of the trail (i.e., we set all trails 7th = 1). In constructing the solution we consider the columns of matrix S one by one, starting from that corresponding to the activity with greatest flow potential, and we assign this activity to the location with least distance potential, according to the “min-max” rule. In our example first activity 3 will be assigned to location 1 because the element a13 is the smallest of its column: we then pair activity 1 to location 2 (the coupling activity 1-location 1 is inhibited because location 1 already has an assigned activity); continuing in the same way one obtains the couplings 2-3, and 4 4 . s= [ TABLE V AVERAGEODV ER FIVE RUNS.B ESTK NOWN RESULTS ARE IN BOLD COMPARISOOFN TH E AS WITH OTHER HEURISTIC APPROACHERSE SULTSA RE Best known Ant System (AS) AS with non deterministic hill climbing Tabu Search Genetic Algorithm Evolution Strategy Sampling & Clustering sirnulared h e a l i n g Nugent -2% 1150 1150 1150 1150 1160 1168 1150 We compared AS, and a version of the AS to which was added a non deterministic hill climbing procedure, with many other well know heuristics (see [28] for more details). Experiments were run on IBM-compatible PC’s with a 80286 Intel processor, and were stopped after one hour time. The test problems used are those known as Nugent problems [29], Elshafei [15], and Krarup 1251. As can be seen in Table V, the performance of AS was always very good [5]. Ant System always found a result within 5% of the best known, while AS with local optimization always found, except for the Nugent 30 problem, the best known solution. This application is described in details in [28]. The JSP can be described as in the following. A set of M machines and a set of J jobs are given. The jth job (j = 1,. . . , J ) consists of an ordered sequence (chain) of operations from a set 0 = {. . . oJm.. .}. Each operation oJm E 0 belongs to job j and has to be processed on machine m for djm consecutive time instants. N = 101 is the total number of operations. The problem is to assign the operations to time intervals in such a way that no two jobs are processed at the same time on the same machine and the maximum of the completion times of all operations is minimized [22]. To apply the AS to JSP we chose the following representation. A JSP with M machines, J jobs and operation set 0 is represented as a directed weighted graph Q = (0’,A) where 0’ = 0 U {oo}, and A is the set of arcs that connect 00 with the first operation of each job and that completely connect the nodes of 0 except for the nodes belonging to a same job. Nodes belonging to a same job are connected in sequence (that is, a node is only connected to its immediate successor, see Fig. 15). Note that graph Q is not the graph with cliques representing machines that is usually utilized to represent the JSP. Node 00 is necessary in order to specify which job will be scheduled first, in case several jobs have their first operation on the same machine. We have therefore N + 1 nodes and 7+ I JI arcs, where all the nodes are painvise connected except 00, which is connected only to the first operation of each job. Each arc is weighted by a pair of numbers, {?-kL,Vkl}. The first, T ~ Lis, t he trail level, while the second is the visibility v k l , and is computed according to a desirability measure derived from a greedy problem specific heuristic like the Longest Processing Time or the Shortest Completion Time. The order in which the nodes are visited by each ant specifies the proposed solution. For instance, consider Job-Shop Scheduling Problem: N(N-1) Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:41:51 UTC from IEEE Xplore. Restrictions apply. DORIGO et al.: ANT SYSTEM OPTIMIZATION BY A COLONY OF COOPERATING AGENTS - 39 o=o, l=oll 2= 0, 3= 0 , 4= O,, 5= 0, 6= o , ~ 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 ~ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 O O o O o o o o o o 0 0 0 0 0 0 0 0 0 0 ~ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Fig. 15. arrows (in bold) are intended to represent a pair of directed arcs. AS graph for a 3 jobs and 2 machines JSP. Connections with no Fig. 16. problem. The steady-state transition matrix for a randomly generated lo-town a 3 x 2 problem (3 jobs, 2 machines): it would be represented in our system by the graph presented in Fig. 15. We suppose the first machine processes operations 1, 3, 5, and the second one the others. All ants are initially in 00; later on they have to identify at each step a feasible permutation of the remaining nodes. To cope with this problem, transition probabilities have to be slightly modified with respect to those computed according to formula (4): in order to have a feasible permutation it is in fact necessary to define the set of allowed nodes in any step not only through the tabu list, but also in a problem-dependent way. For each ant k, let Gk be the set of all the nodes still to be visited and SI, the set of the nodes allowed at the next step. Initially Gk = {1,2,3,4,5,6} and sk = {1,2,3}. Transition probabilities are computed on the basis of formula (4), where the set of allowed nodes is equal to s k . When a node is chosen, it is appended to the tabu list and deleted from Gk and from S k ; if the chosen node is not the last in its job then its immediate successor in the job chain is added to Sk. This procedure ensures the possibility to always produce a feasible solution, possibly the optimal one. The process is iterated until Gk = @. At the end, the order of the nodes in the permutation given by the tabu list specifies the solution proposed by ant k. The trails can thus be computed in the usual way and they are laid down as specified by the ant cycle algorithm. For example, suppose that an ant yielded the solution 7r = (0,1,4,2,5,3,6); this would direct the order of the operations imposing the precedences { ( 1,5), ( 1,3), (5,3)} and {(4,2), (4,6), (2,6)}, respectively. This approach has been implemented and successfully applied to JSP instances of dimension 10 x 10 and 10 x 15 (10 jobs, 15 machines). For each of these problems we always obtained a solution within 10% of the optimum [8], which can be considered a promising result. VIII. DISCUSSIOONF SOME AS CHARACTERISTICS A major issue in defining any distributed system is the definition of the communication protocol. In the AS a set of ants communicate by modifications of a global data structure: after each tour the trail left on each ant’s tour will change the probability with which the same decision will be taken in the future. A heuristic also guides ants in the early stages of the computational process, when experience has not yet accumulated into the problem structure. This heuristic automatically loses importance (remember the coefficient p related to evaporation) as the experience gained by ants, and saved in the problem representation, increases. One way to explain the behavior of AS on the TSP problem is the following. Consider the transition matrix pk(t) of ant k: every element pfj (t) is the transition probability from town i to town j at time t as defined by (4). At time t = 0 each pt3 (0) is proportional to vzj, i.e., closer towns are chosen with higher probability. As the process evolves, p k ( t ) changes its elements according to (1) and (4). The process can therefore be seen as a space deformation, in which path cost is reduced between towns which are connected by edges with a high amount of traffic, and, conversely, path cost is incremented between towns connected by edges with low traffic levels. From simulations we observed that the matrix p k ( t ) , at least in the range of optimality for our parameters, converges to a state’ that is very close to stationary (i.e., variations in the transition matrix p k ( t ) are very small). When this state is reached the behavior of the ants is dependent on the kind of transition matrix obtained. We observed two situations: in the most rare one, occurring (as we saw in Section IV) for particular parameter settings, only two transition probabilities are significantly higher than zero in every row and therefore all the ants choose the same edge at each step and no new tour is searched. In the most common situations instead, most of the rows have only a few transition probabilities with a significant value. In these cases search never stops, even if the number of significant transitions is highly reduced, with respect to the initial situation. Consider for example Fig. 16, obtained as the steady-state transition matrix for a randomly generated 10- town problem: the area of each circle is proportional to the corresponding value of the transition probability. An ant in town 1 has a very high probability to go either to town 5 (near 50%) or to town 2 (near 35%), and a low probability of choosing any other edge. A similar analysis holds for ants in any other town; from towns 9 and 0, for example, any destination is equally probable. 9The stochastic process that rules the evolution of the matrix p k ( t ) is a Markov process with infinitc memory. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:41:51 UTC from IEEE Xplore. Restrictions apply. 40 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS-PART B CYBERNETICS, VOL 26, NO 1, FEBRUARY 1996 Another way to interpret how the algorithm works is to imagine having some kind of probabilistic superimposition of effects: each ant, if isolated (that is, if Q = 0), would move with a local, greedy rule. This greedy rule guarantees only locally optimal moves and will practically always lead to bad final results. The reason the rule doesn’t work is that greedy local improvements lead to very bad final steps (an ant is constrained to make a closed tour and therefore choices for the final steps are constrained by early steps). So the tour followed by an ant ruled by a greedy policy is composed of some (initial) parts that are very good and some (final) parts that are not. If we now consider the effect of the simultaneous presence of many ants, then each one contributes to the trail distribution. Good parts of paths will be followed by many ants and therefore they receive a great amount of trail. On the contrary, bad parts of paths are chosen by ants only when they are obliged by constraint satisfaction (remember the tabu list); these edges will therefore receive trail from only a few ants. IX. CONCLUSION This paper introduces a new search methodology based on a distributed autocatalytic process and its application to the solution of a classical optimization problem. The general idea underlying the Ant System paradigm is that of a population of agents each guided by an autocatalytic process directed by a greedy force. Were an agent alone, the autocatalytic process and the greedy force would tend to make the agent converge to a suboptimal tour with exponential speed. When agents interact it appears that the greedy force can give the right suggestions to the autocatalytic process and facilitate quick convergence to very good, often optimal, solutions without getting stuck in local optima. We have speculated that this behavior could be due to the fact that information gained by agents during the search1 process is used to modify the problem representation and in this way to reduce the region of the space considered by the search process. Even if no tour is completely excluded, bad tours become highly improbable, and the agents search only in the neighborhood of good solutions. The main contributions of this paper are the following. i) ii) We employ positive feedback as a search and optimization tool. The idea is that if at a given point an agent (ant) has to choose between different options and the one actually chosen results to be good, then in the future that choice will appear more desirable than it was before”. We show how synergy can arise and be useful in distributed systems. In the AS the effectiveness of the search carried out by a given number of cooperative ants is greater than that of the search carried out by the same number of ants, each one acting independently from the others. iii) We show how to apply the AS to different combinatorial optimization problems. After introducing the AS by an “Reinforcement of this nature is used by the reproduction-selection mechanism in evolutionary algorithms [23], [30], [33]. The main difference is that in evolutionary algorithms it is applied to favor (or disfavor) complete solutions, while in AS it is used to build solutions. application to the TSP, we show how to apply it to the ATSP, the QAP, and the JSP. We believe our approach to be a very promising one because of its generality (it can be applied to many different problenis, see Section VH), and because of its effectiveness in finding very good solutions to difficult problems. Related work can be classified in the following major areas: * studies of social animal behavior; * research in “natural heuristic algorithms”; stochastic optimization. As already pointed out the research on behavior of social animals is to be considered as a source of inspiration and as a useful metaphor to explain our ideas. We believe that, especially if we are interested in designing inherently parallel algorithms, observation of natural systems can be an invaluable source of inspiration. Neural networks [32], genetic algorithms [23], evolution strategies [30, 331, immune networks [3], simulated annealing [24] are only some examples of models with a “natural flavor”. The main characteristics, which are at least partially shared by members of this class of algorithms, are the use of a natural metaphor, inherent parallelism, stochastic nature, adaptivity, and the use of positive feedback. Our algorithm can be considered as a new member of this class. All this work in “natural optimization” [ 12, 91 fits within the more general research area of stochastic optimization, in which the quest for optimality is traded for computational efficiency. ACKNOWLEDGMENT The authors would like to thank two of the reviewers for the many useful comments on the first version of this paper. We also thank Thomas Back, Hughes Bersini, Jean-Louis Deneubourg, Frank Hoffmeister, Mauro Leoncini, Francesco Maffioli, Bernard Manderik, Giovanni Manzini, Daniele Montanari, Hans-Paul Schwefel and Frank Smieja for the discussions and the many useful comments on early versions of this paper. REFERENCES E. H. L. Aarts and J. H. M. Korst, Simulated Annealing and Boltzmann Machines. New York Wiley, 1988. J. L. Bentley, “Fast algorithms for geometric traveling salesman problems,” ORSA J. Computing, vol. 4, no. 4, pp. 387411, 1992. H. Bersini and F. J. Varela, “The immune recruitment mechanism: A selective evolutionary strategy,” in Proc. Fourth Int. Conf Genetic Algorithms. San Mateo, CA: Morgan Kaufmann, 1991, pp. 520-526. S. C. Boyd, W. R. Pulleyblank and G. Cornuejols, Travel Software Package, Carleton University, 1989. R. E. Burkhard, “Quadratic assignment problems,” Europ. J. Oper. Res., vol. 15, pp. 283-289, 1984. A. Colomi, M. Dorigo and V. Maniezzo, “Distributed optimization by ant colonies,” in Proc. First Europ. Conf ArtiJicial Life, F. Varela and P. Bourgine, Eds. A. Colomi, M. Dorigo and V. Maniezzo, “An investigation of some properties of an ant algorithm,” in Proc. Parallel Problem Solving from Nature Conference (PPSN ’92), R. Manner and B. Manderick Eds. Brussels, Belgium: Elsevier, 1992, pp. 509-520. A. Colorni, M. Dorigo, V. Maniezzo and M. Trubian, “Ant system for job-Shop scheduling,” JORBEL-Belgian J. Oper. Res., Statist. Conzp. Sci., vol. 34, no. 1, pp. 39-53. A. Colorni, M. Dorigo, F. Maffioli, V. Maniezzo, G. Righini and M. Trubian, “Heuristics from nature for hard combinatorial problems,” Tech. Rep. 93425, Dip. Elettronica e Informazione, Politecnico di Milano, Italy, 1993. Paris, France: Elsevier, 1991, pp. 134-142. Authorized licensed use limited to: De Montfort University. Downloaded on August 06,2021 at 13:41:51 UTC from IEEE Xplore. Restrictions apply. DORIGO et al.: ANT SYSTEM OPTIMIZATION BY A COLONY OF COOPERATI? [lo] J. L. Denebourg, J. M. Pasteels and J. C. Verhaeghe, “Probabilistic behavior in ants: A strategy of errors?,” J. Theoret. Biol., vol. 105, pp. 259-271, 1983. [ l l ] J. L. Denebourg and S. Goss, “Collective patterns and decision-making,” Ethology, Ecology & Evolution, vol. 1, pp. 295-311, 1989. [12] M. Dorigo, “Optimization, learning and natural algorithms,” Ph.D. Thesis, Dip. Elettronica e Informazione, Politecnico di Milano, Italy, 1992. [13] M. Dorigo, V. Maniezzo and A. Colorni, “Positive feedback as a search strategy,” Tech. Rep. 91-016, Politecnico di Milano, 1991. [14] S. Eilon, T. H. Watson-Gandy and N. Christofides, “Distribution management: Mathematical modeling and practical analysis,” Oper. Res. Quart., vol. 20, pp. 37-53, 1969. [15] A. E. Elshafei, “Hospital layout as a quadratic assignment problem,” Oper. Res. Quart., vol. 28, pp. 167-179, 1977. [16] M. Fischetti and P. Toth, “An additive bounding procedure for the asymmetric travelling salesman problem,” Mathemat. Prog., vol. 53, [17] F. Glover, “Tabu Search-Part I,” ORSA J. Computing, vol. 1, no. 3, [18] -, “Tabu Search-Part 11,” ORSA J. Computing, vol. 2, no. 1, pp. [ 191 D. E. Goldberg, Genetic Algorithms in Search, Optimization & Machine Learning. Reading, MA: Addison-Wesley, 1989. [20] B. Golden and W. Stewart, “Empiric analysis of heuristics,” in The Travelling Salesman Problem, E. L. Lawler, J. K. Lenstra, A. H. G. Rinnooy-Kan, D. B. Shmoys Eds.. New York Wiley, 1985. 1211 S. Goss, R. Beckers, J. L. Denebourg, S. Aron and J. M. Pasteels, “How trail laying and trail following can solve foraging problems for ant colonies,” in Behavioral Mechanisms of Food Selection, R. N. Hughes Ed., NATO-AS1 Series. Berlin: Springer-Verlag, vol. G 20, 1990. [22] R. L. Graham, E. L. Lawler, J. K. Lenstra and A. H. G. Rinnooy Kan, “Optimization and approximation in deterministic sequencing and scheduling: A survey,’’ in Annals Disc. Math., vol. 5, pp. 287-326, 1979. [23] J. H. Holland, Adaptation in Natural and Art$cial Systems. Ann Arbor, MI: The University of Michigan Press, 1975. [24] S. Kirkpatrick, C. D. Gelatt and M. P. Vecchi, “Optimization by simulated annealing,” Sci., vol. 220, pp. 671-680, 1983. [25] J. Krarup, P. M. Pruzan, “Computer-aided layout design,” Mathemat. Prog. Study, vol. 9, pp. 85-94, 1978. [26] E. L. Lawler, J. K. Lenstra, A. H. G. Rinnooy-Kan and D. B. Shmoys Eds., The Travelling Salesman Problem. New York: Wiley, 1985. [27] S. Lin and B. W. Kernighan, “An effective heuristic algorithm for the TSP,” Oper. Res., vol. 21, 498-516, 1973. [28] V. Maniezzo, A. Colorni and M. Dorigo, “The ant system applied to the quadratic assignment problem,” Tech. Rep. IRIDIN94-28, Universitk Libre de Bruxelles, Belgium, 1994. [29] C. E. Nugent, T. E. Vollmann and J. Ruml, “An experimental comparison of techniques for the assignment of facilities to locations,” Oper. Res., [30] I. Rechenberg, Evolutionsstrategie. Stuttgart: Fromman-Holzbog, 1973. [3 11 G. Reinelt, TSPLIB 1.0, Institut fur Mathematik, Universitat Augsburg, Germany, 1990. [32] D. E. Rumelhart and J. L. McLelland, Parallel Distributed Processing: Explorations in the Microstructure of Cogniton. Cambridge, MA: MIT Press, 1986. [33] H.-P. Schwefel, “Evolutionsstrategie und numerische optimierung,” Ph.D. Thesis, Technische Universitat Berlin, 1975. Also available as Numerical Optimization of Computer Models. New York Wiley, 198 1. [34] D. Whitley, T. Starkweather and D. Fuquay, “Scheduling problems and travelling salesman: The genetic edge recombination operator,” in Proc. Third Int. Con$ on Genetic Algorithms. San Mateo, CA Morgan Kaufmann, 1989. pp. 173-197, 1992. pp. 190-206, 1989. 4-32, 1990. vol. 16, pp. 150-173, 1968. \\IG AGENTS 41 Marco Dorigo (S’92-M’93) was born in Milan, Italy, in 1961. He received the Laurea (Master of Technology) in industrial technologies engineering in 1986, and the Ph.D. in information and systems electronic engineering in 1992 from Politecnico di Milano, Milan, Italy. In 1992 he was a research fellow at the International Computer Science Institute, Berkeley, CA. In 1993 he became a NATO-CNR fellow at the IRIDIA Laboratory, Free University of Brussels, Belgium, and in 1994, he was awarded a Human Capital and Mobility two-year fellowship by the Commission of the European Community to continue his research at the IRIDIA laboratory, where he currently works. His areas of research include evolutionary computation, reinforcement learning, and their application to autonomous robotics and to combinatorial optimization. Dr. Dorigo is an Associate Editor for the IEEE TRANSACTIOONN SSY STEMS, MAN,A ND CYBERNETICanSd, he was the Guest Editor of a special issue on “Learning autonomous robots”. He is a member of the editorial board of the Evolutionary Computation journal and of the Adaptive Behavior journal. He is a member of the Politecnico di Milano Artificial Intelligence and Robotics Project, and of the Italian Association for Artificial Intelligence (AI*IA). He took part to several CEC ESPRIT Projects, and National research projects. Vittorio Maniezzo was born in Ferrara, Italy, in 1962. He received the Laurea (Master of Technology) in electronic engineering in 1986 and the Ph.D. degree in automatic control and computer science engineering in 1993, both from Politecnico di Milano, Milan, Italy. He is currently with the University of Bologna and is a member of the Politecnico di Milano Artificial Intelligence and Robotics Project. He took part to several CEC Esprit and National research projects. His current research interests are in the fields of machine learning (evolutionary techniques for sensonmotor coordination, cognitive modeling) and of combinatorial optimization (evolutionary heuristic algorithms, column generation techniques). Dr. Maniezzo is a member of the Italian Association for Artificial Intelligence (AI*IA) and of the Italian Association for Operations Research (AIRO). Albert0 Colorni was born in Milan, Italy, in 1946. He received the Laurea (Master of Technology) in electronic engineering in 1970 from Politecnico di Milano, Italy. He is an Professor of Operations Research at the Politecnico, and Director of the research center in Decision Support Systems for Environment and Land Use at the ME’ (Master Imprese-Politecnico). He currently works in the fields of Combinatorial Optimization (in particular, of heuristics from nature), DSS methodologies (Analytic Hierarchy Process and ELECTRE methods) and DSS applications to environmental impact assessment. Prof. Colorni is Associate Editor OF Rccerca Operatcva, the Italian journal of Operations Research.\"]\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
