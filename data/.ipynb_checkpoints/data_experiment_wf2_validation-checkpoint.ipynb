{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = [[\n",
    "    1,0,0,\"Support Vector Regression for Electricity Consumption Prediction in a Building in Japan Frederic Magoul es, Michel Piliougine, and David Elizondo   CentraleSupelec, Universite Paris-Saclay, France Universidad de Malaga, Spain De Montfort University, United Kingdom Email: frederic.magoules@hotmail.com Abstract—This work studies how to apply support vector machines in order to forecast the energy consumption of buildings. Usually, support vector regression is implemented using the sequential minimal optimisation algorithm. In this work, an alternative version of that algorithm is used to reduce the execution time. Several experiments were carried out taking into account data measured during one year. The weather conditions were used as independent variables and the consumed amount of electricity was considered as the parameter to predict. The model has been trained using the first six months of the dataset whereas it was validated using the following three months and tested taking into account the last three months of measurements. From obtained results, a good performance of the model is observed. Index Terms—Electricity prediction, Support vector regression, Structural risk minimisation I. INTRODUCTION Energy consumption in buildings represents a great slice of energy use in the world. This issue plays a crucial role in the global economy. It is necessary to manage this problem right now in order to tackle climate change as soon as possible. Balancing energy production and demand is a problem of concern for all society in general, including energy suppliers, households and city councils. In addition, forecasting of energy consumption based on historical data would be an important tool especially when designing new buildings. Nevertheless, the prediction of the energy which will be consumed by a building is a complex task. On the one hand, there are many factors which could influence on that variable, such as the external weather conditions, the modus operandi of the inhabitants or the insulation capacity of the building materials. Moreover, some of these parameters are not easily measurable. On the other hand, since our objective is to perform that forecast as accurate as possible, we need to gather enough historical data to cover all possible scenarios. In order to obtain a valid model to forecast the energy consumption based on the simulated data, we decided to use support vector machines (SVMs). They are a set of supervised learning techniques firmly grounded on the statistical learning theory from [49]. This framework highlights the desired properties of a learning machine that enable it to generalise well to unseen situations. At the beginning, SVMs were introduced to solve classifications problems, but later these ideas were adapted to manage with datasets where the output is a continuous variable: support vector regression [44]. Although a basic SVM is only able to manage with linearly datasets, non-linear problems can be addressed by means of the transformation from the input data space to a feature space with higher dimensionality, for example, using a Gaussian kernel. In this work, a SVM is used to predict the energy consumption in a building. As our objective is to predict the value of a target variable which ranges within a subset of IR, we should talk about support vector regression. The initial problem is reduced to a quadractic optimisation one that can be implemented using several algorithms. One of the fastest ways to perform this duty is known as sequential minimal optimisation (SMO) [40], because at each step, instead of calling a complex numerical procedure, the analytical solution over a pair of variables is computed. The selection of both variables is a key factor in the execution time of the complete procedure. Usually, this selection is performed taking into account information from the first order derivatives. However, an alternative version of SMO which uses second order information is used in this paper, leading us to a reduction in terms of execution time. The weather conditions are taken as inputs. Data collected throughout nine months over different locations are used in the training phase and the obtained model is tested over the next three months. The results show a satisfactory behaviour of the model. The rest of the paper is organised as follows: A review of different works to analyse building energy performance are presented in Section II, with special mention to those which are based on support vector techniques. In Section III, the theory of support vector machines is briefly introduced and how to solve non-linear regression problems. A detailed description of the methodology and the preparation of the dataset is provided in Section IV. A discussion of the results is presented in Section V. Finally, the conclusions of this work are summarised in Section VI. II. PREVIOUS WORKS The prediction of the consumption of energy in buildings is still an open field of research. A proof of this fact is the large number of articles that have been published. [18] present a complete review of these previous works. They classify them into three categories: white-box models (based on physical equations and the building geometry), black-box models (mainly classical statistics and artificial intelligence techniques), and grey-box models (also known as hybrid approaches, where different kinds of models are combined in order to improve their prediction power). In the first group, each element of the building is modelled by physical equations which quantify its thermal behaviour taking into account the architecture and structure of the building, the distribution of the floors and rooms, the type of ventilation and the specifications of the constructive materials [35], [41], [47]. In the second group of techniques, two subgroups can be found: the statistical methods and the artificial intelligence approach. Both of them are characterised by a process where no physical equations are needed in order to forecast the energy consumption of the building, being very useful in most of the cases, where these physical relationships are not known. They automatically extract the underlying information from a dataset of previous measurements, adjusting the model in order to fit these data. However, the true physical equations that rule the thermal behaviour of the building are not explicitly given by the models; they are only able to simulate the system. The main problem of this approach is that enough historical data for a long time period must be collected in order to develop a useful model [61]. Many early works fall into the statistical subgroup of models. Some of them are based on a regression model [8], [17], [42], [26] since its implementation is easy and achieves very good results in long-term forecasting. The main difficulty using these methods is the fact that the function between the multiple inputs and the output is not easy to discover. Other classical techniques have been applied, such as autoregressive moving average (ARMA) models [10], [37] or Fourier series methods [45], [11]. In cases where the relationship between variables is unknown, the measurements are affected by noise or the underlying function presents a strong non-linear nature, it is common to use artificial neural networks (ANNs). There are many papers in the literature which focus on the forecast of energy load in buildings using ANNs [6], [2], [3], [7], [53], [20], [25], [21], [5], [4], [15], [51], [33]. [38] provide a model to forecast the heating demand by ANNs and apply the model to an institutional building in France. [24] use several kinds of ANNs (such as backpropagation neural network, radial basis function network and adaptive neuro-fuzzy interference system) to forecast the energy consumption of a university campus, obtaining high accuracy when a combination of these approaches is performed. [39] present a ANN model to forecast the electricity load of an institutional building in Canada. In addition, principal component analysis was applied to preprocess the input data improving model accuracy. From the results of those works, it can be inferred that the ANN models improve classical methods especially when the goal is the short-term forecasting. Both type of methods, physical or black-box models, can be used to obtain satisfactory results but they show limitations. The first type requires a previous analysis of thousand of building elements including thermal coefficients of each constructive material and detailed geometric characteristics. In addition, a deep knowledge about the thermal phenoma that happen within the building must be wellknown, in terms of equations. On the other hand, black-box methods require a huge amount of data and it is usual that the obtained model, trained for a particular case, cannot be applied to other similar cases. However, a way to overcome these problems could be based on mixing two or more of these techniques obtaining a grey-box method, as we mentioned above. [30] use a genetic algorithm to identify which parts of a given thermal model of a building are defective and are resposible of discrepancies between measured real data and predicted data. [62], starting from a physical model that determines the optimal building configuration for a fixed set of mteorological conditons. They use a genetic algorith in order to combine results of that previous model when it is applied to a wide range of different conditions to stablish the optimal configuration throughtout a year for a specific location. [29] use a complex thermal physical model to generate a set of simulated measurements, and then, using mutivariate regression, a simple model is obtained, which requires less computation time. [34] follow a similar strategy to forecast internal temperature and relative humidity from external temperature and solar irradiation. Instead of using a regression procedure, [52] train a neural network feeding it with simulated data to forecast the energy consumption of a building. In recent years, support vector machines have been used to solve classification, regression and forecasting problems. From one point of view, SVMs [28], [60], [56] keep many similarities with neural networks. However, these techniques have a strong mathematical foundation and high performance. Whereas with ANNs a previous step of pre-processing is important in order to prepare the input data, SVMs does not need any effort due to their ability to deal with highdimensional spaces [13]. Instead of minimising the error over the training data, SVMs are initially designed in order to achieve good results in terms of generalisation error. In addition, if they are combined with an appropriate kernel transformation, a wide range of non-linear underlying functions can be simulated. Furthemore we have shown that parallelization of such techniques leads to very efficient implementation [58], [59], [57], [55], [54] For example, [27] applied this methodology in order to forecast the global electricity consumption of Turkey taking into account different social and economic variables. The future evolution of each input variable was also modelled by an additional SVR model using historical data. First article where SVMs were applied to forecast electricity consumption in building was published by [12]. In that work, three years of data were used to train the model and then model was tested to forecast one year of data. The results show a better performance of the support vector machine than a neural network. The authors state that this improvement is due the fact that SVMs are based in the minimisation of the structural risk (related to the generalisation error) instead of the empirical risk (associated to the training error). They also highlight that in a SVM there are fewer free parameters to set than in an artificial neural network. Finally, whereas an ANN could stop in a local minima, a SVM always finishes providing the global minimum. In the work by [32], SVR is applied to obtain an hour-byhour forecast of the cooling load of an office building. The use of a Gaussian kernel was investigated and an optimal setting of the support vector regression (SVR) parameters was carried out. They stated that the error is reduced by half in comparison with a traditional backpropagation neural network. In the article by [14], several techniques were applied to forecast energy building consumption at an hourly scheme, including SVR and least-squared SVR (a variant of the standard SVR). From their results it can be inferred that the support vector approach is the best technique for predicting future residential electrical consumption at short-time over different scenarios. [50] propose a hybrid methodology in order to predict the annual electric load of a city. They highlight the power of SVR when dealing with non-linear problems. An evolutionary algorithm called differential evaluation is used to search a good set of parameters for SVR. That new approach performs the generalisation error obtained by other different alternatives, such as classical regression, backpropagation neural network and SVR with default parameters. [22] implements a SVR model and apply it to a residential building in New York. They focus their study in the analysis of the influence of the temporal interval of the measurements and the granularity of the position of the sensors in the building. Another good example where SVR are combined with other techniques to improve performance is the work by [19], where a novel decomposition procedure is used to disaggregate the target time series into two components, separating the main trend from the local oscillations. Then, each one is modelled separately by a SVM. Finally, [1] review several works from the literature where ANNs and SVMs are used to predict the electricity load of buildings, highlighting the potential of a hybridisation of both approaches. III. SUPPORT VECTOR REGRESSION A growth in interest on support vector machines has risen in the last decades. This type of learning machines have been successfully applied to a wide range of problems, including optical character recognition [31], text classification [23] and face detection [36]. Although support vector machines were initially designed to solve classification problems, later their principles were adapted in order to solve other kind of problems, such as novelty detection [43] or regression [44]. Support vector machines are based on the statistical learning theory developed by Vapnik and Chervonenkis several decades ago [48]. The main goal of this theory is to provide a well-founded mathematical background to design learning machines with a high performance in terms of generalisation error, when unseen data are used as input. This error arises as the sum of two components: the approximation error and the estimation error. The approximation error is associated to the selection of the family of functions in which the model will be searched. However, the estimation error is related to the ability of the training method when searching the best model among that family which fits the training data. At a first glance, support vector machines might seem another type of artificial neural network and in fact they have many similarities. However, they are based on different principles. Neural networks try to minimise the estimation error over the training data, following the empirical risk minimisation (ERM) principle. In contrast, support vector machines are designed to meet the structural risk minimisation (SRM) principle in order to improve the generalisation error. The structural risk not only takes into account the error over the training set, but also it includes a penalty term to control the complexity of the model. The greater complexity has a family of functions, the greater risk of overfitting exists, losing generalisation power. The structural risk could be seen as a trade-off between approximation error and estimation error. Suppose we are given a training dataset composed of m samples: (¯??1, ??1),(¯??2, ??2),...,(¯????, ????) Each input vector ??¯?? ? IR?? contains ?? independent variables or features and the corresponding output ???? ? IR is the dependent variable associated to each input. Our goal is to estimate a function ?? : IR?? ? IR which allows us to forecast the output value ???? for an unseen value of ??¯??. The basic support vector algorithm is only able to provide a linear function of the form ??(¯??)=¯???? ??¯ + ??. This function describes the hyperplane that, being as simple as possible, fits closest to the dataset. In order to measure that fitting performance, a loss function must be defined. Since measurements could be affected by noise, we have selected the ??-insensitive loss function, which only takes into account differences between the hyperplane and the points beyond than a specific threshold: ??(??, ??(¯??)) = { |?? - ??(¯??)| - ?? if |?? - ??(¯??)| = ?? 0 in other case (1) This threshold ??, set by the user beforehand, defines a band around the dataset between the hyperplanes (¯???? ??¯+??)-?? and (¯???? ??¯+??)+??. Deviations from the solution hyperplane smaller than ?? are neglected and they are not taken into account. Only points beyond the band affect to the estimation error. In Fig. 1 an example in two dimensions is depicted. For each measured point, we can define two slack variables ???? and ??* ?? . The objective of these variables is to penalise only those points beyond the ??-band. Whereas ???? measures the deviation of a point that is upper the band (it is equal to zero in other case), ??* ?? measures the deviation of a point that is under the band. However, if the underlying dataset has a non-linear nature, this procedure will fail. For that reason, it is necessary to apply a previous transformation ?? : IR?? ? IR?? to the points in the input space. Through this function the input space is mapped to a feature space with more dimensionality. With an appropriate function, a non-linear dataset in the input space could be transformed into a linear dataset in the feature space. Therefore, the transformed dataset could be modelled by a support vector machine. Then, instead of the previous formulation for ??, we will consider the following one: ??(¯??)=¯???? ??(¯??) + ?? (2) The SVR solution will be given by the following quadratic optimisation problem: min. 1 2 ??¯?? ??¯ + ????? ??=1 (???? + ??* ?? ) (3) ??.??. ???? - (¯???? ??(¯????) + ??) = ?? + ???? ?? = 1,...,?? (¯???? ??(¯????) + ??) - ???? = ?? + ??* ?? ?? = 1,...,?? ????, ??* ?? = 0 ?? = 1,...,?? The objective function has two terms. The first term penalises the complexity of the learned function, in this case the complexity of the hyperplane. The second term measures the loss due to the estimation error. The constant ?? is the regularisation parameter that balances loss and penalty and must be adjusted beforehand. Last optimisation problem is expressed in its primal formulation. However, it is possible to obtain its dual formulation. The Lagrangian for the dual is: ? = 1 2 ??¯?? ??¯ + ????? ??=1 (???? + ??* ?? ) (4) -??? ??=1 ???? (?? + ???? - ???? + ¯???? ??(¯????) + ??) -??? ??=1 ??* ?? (?? + ??* ?? + ???? - ??¯?? ??(¯????) - ??) -??? ??=1 ???? ???? -??? ??=1 ??* ?? ??* ?? where ? ?? = 1,...,?? ????, ??* ?? , ????, ??* ?? = 0. Equating the partial derivatives of ? to zero, we obtain the Karush-Khun-Tucker conditions: ?? ???¯ = 0 ? ??¯ = ??? ??=1 (???? - ??* ?? ) ??(¯????) (5) ?? ??? = 0 ? ??? ??=1 ???? = ??? ??=1 ??* ?? (6) ?? ????? = 0 ? ?? = ???? + ???? ? ?? = 1,...,?? (7) ?? ???* ?? = 0 ? ?? = ??* ?? + ??* ?? ? ?? = 1,...,?? (8) From the two last conditions and taking into account that ????, ??* ?? , ????, ??* ?? = 0, we can infer that 0 = ???? = ??; 0 = ??* ?? = ?? ? ?? = 1,...,??. Finally, substituting and operating in ?, we obtain the dual formulation: max.??? ??=1 (???? - ??* ?? )???? - ?? ??? ??=1 (???? + ??* ?? ) (9) - 1 2 ??? ??=1 ??? ??=1 (???? - ??* ?? )(???? - ??* ?? )(??(¯????) ?? ??(¯???? )) ??.??. ??? ??=1 ???? = ??? ??=1 ??* ?? 0 = ???? = ?? ? ?? = 1,...,?? 0 = ??* ?? = ?? ? ?? = 1,...,?? Each point of the dataset is associated with a pair of Lagrange multipliers (????, ??* ?? ). When the SVM has been trained and the optimal hyperplane has been found, only the points which lie outside the ??-band have one of their Lagrange multipliers distinct to zero (actually ???? > 0 or ??* ?? > 0). These points are called support vectors and the model given by the support vector machine only depends on these samples (every point in the dataset with both multipliers equal to zero has not any influence in the provided model). An example of this fact can be seen in Fig. 1, where points inside the band can be removed and the modelling hyperplane remains unaffected. Fig. 1. ??-Tube around the solution hyperplane We do not need to give the function ?? explicitly since in the dual formulation it appears inside a dot product as ??(¯????)?? ??(¯???? ). It could be defined implicitly by means of a kernel function: ??(¯????, ??¯?? ) = ??(¯????) ?? ??(¯???? ) (10) Then, the objective function can be rewritten as: ??? ??=1 (????-??* ?? )????-?? ??? ??=1 (????+??* ?? )-1 2 ??? ??=1 ??? ??=1 (????-??* ?? )(????-??* ?? )??(¯????, ??¯?? ) (11) Perhaps it is not possible to find a hyperplane in the input space in such a way the measured points are enough close having a small value of ??. However, using a kernel ??(¯????, ??¯?? ) and its implicit transformation function ??, the input data could be arranged in a different way in the feature space. In this new feature space, with more dimensionality than the input space, the task of finding a hyperplane fitting the data could be easier, even for small value of ??. One of the most frequently used kernel functions is the Gaussian kernel, which was used in this work: ??(¯????, ??¯?? ) = exp ( -??(¯???? - ??¯?? ) ?? (¯???? - ??¯?? ) ) (12) The effect of using different values for the free parameters of the algorithm (?? and ??) has a strong influence in the performance of the predicted model. The initial dataset has been split into three subsets: the training set, the validation set and the testing set. The training set is used to adjust the model using the SVR training process. For each different combination of the values of the parameters, the training process must be repeated, obtaining a different trained SVM. Among all these SVM, the model with less error when predicting the validation set is considered as the final model (the validation set is not used to construct the model but also to select the most suitable). Finally, the testing set is only used to measure the predictive power of the selected model. Generally, the main problem of support vector regression is the high execution time over databases with a large number of samples, as it happens with data from sensor installed in buildings with the aim to forecast its behaviour in terms of electricity load. In addition, the kernel matrix is huge and in many cases is not possible to store it in the available RAM memory, that means the virtual memory will be used, with a dramatically reduction in efficiency. Therefore, [40] proposes the sequential minimal optimisation (SMO) with the aim of overcoming this problem. In each iteration, only two Lagrange multipliers are optimised simultaneously by means of the analytical solution, which can be computed very quickly, avoiding a numerical procedure. Once these Langrange multipliers have been optimised, the Kernel matrix is updated in order to reflect the changes. The procedure is finished when the Karush-Kuhn-Tucker (KKT) conditions are fulfilled. The selection of the two Lagrange multipliers to be optimised in each step, which are called the working set, is a crucial step, since it directly influences on the speed of convergence. A heuristic rule is used to decide these multipliers, among all of them. An improvement over the original SMO algorithm is proposed [16]. Indeed, the difference between both versions is the selection of the pair of Langrange multiupliers to optimimise at each step. Whereas in the original version we are only using first order information, this new version takes into account second order information. This fact leads us to Characteristic Value Location Sendai (Miyagi, Japan) Latitude 38°16' N Longitude 140°52' E Elevation 73 m Building length 8.3 m Building width 5.7 m Building height 3.5 m TABLE I MAIN CHARACTERISTIC OF THE BUILDING AND ITS EMPLACEMENT. the optimal solution faster allowing a decrease of the execution time. IV. METHODOLOGY The main objective of this experiment is to infer knowledge about the behaviour of a building in terms of energy consumption based on measured data in normal running conditions. Next, this knowledge can be useful in order to analyse and forecast the behaviour in case of abnormal situations. In order to achieve this goal, we propose the use of a predictive model based on support vector machines. We want to show the relationship between the variability of different input parameters and the output of the model. LIBSVM [9] is a software tool which includes support vector machines to solve several kinds of problems such as classification, regression and distribution estimation. This library has been implemented for many programming languages including C++, Java, MATLAB and LabVIEW. For this work, the MATLAB programming environment has been used, version R2011b for 64 bits running over an Intel Core 2 Duo CPU (T7800) at 2.67 GHz with 4 GB of RAM (the operating system was Microsoft Windows 7 Professional 64 bits). The weather conditions and the consumption data were acquired from a single room building located in Sendai (Miyagi, Japan). The main characteristics of this emplacement and the building dimensions can be seen in Table I. The measurement period started on 1 February 2013 and finished on 31 January 2014 (1 natural year). Typical environmental parameters vary throughout time and were registered at regular intervals of time (such as the air temperature or the relative humidity). All these variables were acquired every 15 minutes. In addition, the electrical consumption of each interval was measured by means of a counter installed in the conection point. Finally, these values were averaged throughout the day in order to generate a time series to be plotted easily in a graph. The following variables have been taken into account: · Temperature and humidity conditions (independent variables) – Outdoor air temperature – Indoor air temperature – Outdoor relative humidity – Indoor relativity humidity · Energy consumption measurement (dependent variable) V. RESULTS A preprocessing procedure had to be applied to the data before using them to train the SVM model. For example, power recordings along a day have been integrated to achieve daily values of daily energy consumption. In addition, weather parameters such as temperature and humidity have been averaged throughout each day. Fig. 2. Evolution of both indoor and outdoor temperature. Fig. 3. Evolution of the relative humidity throughout the measurement period. In order to quantify the performance of the proposed model, the normalised root mean square error was used. The root mean square error tries to determine an averaged value of all the deviations between the predicted and the measured values. It is usual to normalise this index dividing it between the mean value of the mesured magnitude. It can be calculated by the following expression: ?????????? = 1 ??¯ \u0006\u0007\u0007? 1 ?? ??? ??=1 (???? - ????)2 (13) In this work, the objective is to evaluate the predictive power of the SVM comparing its output with measured data. A SVM was trained with data acquired from 1 February 2013 to 31 July 2013 (a training period of 6 months). The validation set (used only the select the best model) starts on 1 August 2013 and finishes on 31 October 2013. Then, the selected trained model was applied to forecast the energy consumption for the last three months in the time series: from 1 November 2013 to 31 January 2014. The following steps have been performed: 1) Determination of the independent and dependent variables. 2) Splitting of the measurement period into training, validation and testing set. 3) For each combination of ?? and ?? execute the training algorith using the training dataset. 4) Select the trained SVM with less NRMSE (error) among the validation set. 5) Calculating NRMSE error over the testing set and plotting the results. In order to set values for ?? and ?? (?? has been set to 0.1), the training routine has been call several times with different parameters. Therefore, the SVR algorithm has been repeated for different combinations of both parameters within a range, and then, only the trained SVR that optimise the error has been considered for prediction [46]. The resulting NRMSE for each combination can be seen in Table II. In our case, using ?? = 5000 and ?? = 10-4, the minimum NRMSE was achieved. ??=10-8 ??=10-7 ??=10-6 ??=10-5 ??=10-4 ??=10-3 ??= C=1 0.308 0.308 0.307 0.303 0.275 0.218 0.2 C=5 0.308 0.308 0.306 0.288 0.214 0.167 0.1 C=10 0.308 0.307 0.303 0.273 0.178 0.163 0.2 C=50 0.308 0.306 0.288 0.207 0.180 0.158 0.2 C=100 0.307 0.303 0.273 0.176 0.182 0.158 0.2 C=500 0.306 0.288 0.207 0.195 0.167 0.175 0.2 C=1000 0.303 0.273 0.176 0.200 0.162 0.195 0.3 C=5000 0.288 0.206 0.195 0.198 0.157 0.237 0.4 C=104 0.273 0.176 0.203 0.188 0.159 0.250 0.4 C=5·104 0.206 0.196 0.211 0.169 0.173 0.300 0.4 C=105 0.176 0.203 0.204 0.162 0.180 0.332 0.4 C=5·105 0.196 0.214 0.191 0.158 0.197 0.445 0.4 C=106 0.203 0.209 0.184 0.159 0.203 0.526 0.4 C=5·106 0.209 0.208 0.171 0.172 0.215 0.757 0.4 C=107 0.208 0.210 0.168 0.174 0.268 0.787 0.4 TABLE II NORMALISED ROOT MEAN SQUARED ERROR FOR EACH COMBINATION OF THE ADJUSTABLE PARAMETERS. Fig. 4 shows the evolution of the electrical consumption throughout the measurement period. Fig. 5 shows the evolution of the measured and predicted electrical consumption throughout the testing period. The NRMSE for the training, validation and testing periods are summarised in Table III. As can be seen, the normalised root mean square error is around 15% for the three sets. This experiment shows that the proposed model is satisfactory even when it is used to forecast unseen values. VI. CONCLUSIONS AND FUTURE WORK The main conclusion of this paper is that we have found a powerful tool to construct forecasting models in order to predict the energy consumption of buildings. Several implementation of SVR are available, but LIBSVM uses a variation 194 Fig. 4. Evolution of the electrical consumption throughout the measurement period. 0 20 40 60 80 100 120 140 160 180 01/11/2013 08/11/2013 15/11/2013 22/11/2013 29/11/2013 06/12/2013 13/12/2013 20/12/2013 27/12/2013 03/01/2014 10/01/2014 17/01/2014 24/01/2014 31/01/2014 energy (kWh) measured comsumpon predicted comsumpon Fig. 5. Comparison between the measured and predicted consumption over the testing period. over SMO that uses second order information to select the Lagrange multipliers to be optimised. Thanks to this alternative SMO implementation it is possible to obtain very good results in terms of execution time for large datasets, such as those resulting from real-time measurements, as in our case. Taking into account the calculated values for the NRMSE, our experiment shows that SVR models are accurate, not only in predicting the energy consumption over the training dataset, but also over unseen data (the testing dataset). In fact, the great advantage of SVR (and in general of SVMs) is its capacity of generalisation. Future work should focus on the use of SVR in order to forecast energy consumption of different buildings. This Performance - Training period Value Normalised root mean square error 15.02% Performance - Validation period Value Normalised root mean square error 15.75% Performance - Testing period Value Normalised root mean square error 14.88% TABLE III PERFORMANCE PARAMETERS OF THE PROPOSED MODEL. task requires the real-time measurement of data from several buildings under different meteorological conditions. In addition to the contributing variables considered in this paper, other constructive parameters from the building must be used as input of the model, such as its dimensions, heat transfer coefficient or the ratio of windowed surfaces. These additional inputs will be fixed for each building and allow the tuning of the model for each case. REFERENCES [1] A. Ahmad, M. Hassan, M. Abdullah, H. Rahman, F. Hussin, H. Abdullah, and R. Saidur. A review on applications of {ANN} and {SVM} for building electrical energy consumption forecasting. Renew Sust Energ Rev, 33:102 – 109, 2014. [2] M. Aydinalp, V. I. Ugursal, and A. S. Fung. Modeling of the appliance, lighting, and space-cooling energy consumptions in the residential sector using neural networks. Appl Energ, 71(2):87–110, 2002. [3] M. Aydinalp, V. I. Ugursal, and A. S. Fung. Modeling of the space and domestic hot-water heating energy-consumption in the residential sector using neural networks. Appl Energ, 79(2):159–178, 2004. [4] M. Aydinalp-Koksal and V. I. Ugursal. Comparison of neural network, conditional demand analysis, and engineering approaches for modeling end-use energy consumption in the residential sector. Appl Energ, 85(4):271 – 296, 2008. [5] A. Azadeh, S. F. Ghaderi, and S. Sohrabkhani. Annual electricity consumption forecasting by neural network in high energy consuming industrial sectors. Energ Convers Manage, 49(8):2272 – 2278, 2008. [6] A. E. Ben-Nakhi and M. A. Mahmoud. Energy conservation in buildings through efficient A/C control using neural networks. Appl Energ, 73(1):5–23, 2002. [7] A. E. Ben-Nakhi and M. A. Mahmoud. Cooling load prediction for buildings using general regression neural networks. Energ Convers Manage, 45(13-14):2127–2141, 2004. [8] D. W. Bunn and E. D. Farmer. Comparative Models for Electrical Load Forecasting. John Wiley & Sons, 1985. [9] C. C. Chang and C. J. Lin. LIBSVM: A library for support vector machines. ACM Trans Intell Syst Technol, 2(3):27:1–27:27, 2011. [10] J. F. Chen, W. M. Wang, and C. M. Huang. Analysis of an adaptive time-series autoregressive moving-average (ARMA) model for shortterm load forecasting. Electr Power Syst Res, 34(3):187–196, 1995. [11] A. Dhar, T. A. Reddy, and D. E. Claridge. Modeling hourly energy use in commercial buildings with fourier series functional form. ASME J Sol Energ Eng, 120(3):217 – 223, 1998. [12] B. Dong, C. Cao, and S. E. Lee. Applying support vector machines to predict building energy consumption in tropical region. Energ Buildings, 37(5):545–553, 2005. [13] A. I. Dounis. Artificial intelligence for energy conservation in buildings. Adv Build Energ Res, 4(1):267–299, 2010. [14] R. E. Edwards, J. New, and L. E. Parker. Predicting future hourly residential electrical consumption: A machine learning case study. Energ Buildings, 49:591–603, 2012. [15] B. B. Ekici and U. T. Aksoy. Prediction of building energy consumption by using artificial neural networks. Adv Eng Softw, 40(5):356–362, 2009. [16] R. E. Fan, P. H. Chen, and C. J. Lin. Working set selection using second order information for training support vector machines. J Mach Learn Res, 6:1889–1918, 2005. [17] M. F. Fels. PRISM: An introduction. Energ Buildings, 9(1-2):5–18, 1986. [18] A. Foucquier, S. Robert, F. Suard, L. Stphan, and A. Jay. State of the art in building modelling and energy performances prediction: A review. Renew Sust Energ Rev, 23(0):272 – 288, 2013. [19] L. Ghelardoni, A. Ghio, and D. Anguita. Energy load forecasting using empirical mode decomposition and support vector regression. IEEE T Smart Grid, 4(1):549–556, 2013. [20] P. A. Gonzalez and J. M. Zamarre no. ˜ Prediction of hourly energy consumption in buildings based on a feedback artificial neural network. Energ Buildings, 37(6):595–601, 2005. [21] A. Hernandez Neto and F. A. Sanzovo Fiorelli. Comparison between detailed model simulation and artificial neural network for forecasting building energy consumption. Energ Buildings, 40(12):2169–2176, 2008. 195 [22] R. K. Jain, K. M. Smith, P. J. Culligan, and J. E. Taylor. Forecasting energy consumption of multi-family residential buildings using support vector regression: Investigating the impact of temporal and spatial monitoring granularity on performance accuracy. Appl Energ, 123(15):168 – 178, 2014. [23] T. Joachims. Text categorization with support vector machines: learning with many relevant features. In Proceedings of the 10th European Conference on Machine Learning (ECML-98), pages 137–142, Chemnitz (Germany), 1998. [24] R. Z. Jovanovic, A. A. Sretenovi c, and B. D. Zivkovi ? c. Ensemble of various neural networks for prediction of heating energy consumption. Energ Buildings, 94(0):189 – 199, 2015. [25] S. Karatasou, M. Santamouris, and V. Geros. Modeling and predicting building’s energy use with artificial neural networks: Methods and results. Energ Buildings, 38(8):949 – 958, 2006. [26] S. Katipamula, T. A. Reddy, and D. E. Claridge. Multivariate regression modeling. ASME J Sol Energ Eng, 120(3):177–184, 1998. [27] K. Kavaklioglu. Modeling and prediction of Turkey’s electricity consumption using Support Vector Regression. Appl Energ, 88(1):368–375, 2011. [28] F. Lai, F. Magoules, and F. Lherminier. Vapnik’s learning theory applied   to energy consumption forecasts in residential buildings. International Journal of Computer Mathematics, 85(10):1563–1588, 2008. [29] J. C. Lam and S. C. Hui. Sensitivity analysis of energy performance of office buildings. Build Environ, 31(1):27 – 39, 1996. [30] P. Lauret, H. Boyer, C. Riviere, and A. Bastide. A genetic algorithm applied to the validation of building thermal models. Energ Buildings, 37(8):858 – 866, 2005. [31] Y. Lecun, L. D. Jackel, H. A. Eduard, N. Bottou, C. Cartes, J. S. Denker, H. Drucker, E. Sackinger, P. Simard, and V. Vapnik. Learning algorithms for classification: A comparison on handwritten digit recognition. In J. H. Oh, S. Z. Cho, and C. L. Kwon, editors, Neural Networks: The Statistical Mechanics Perspective. CTP-PBSRI Joint Workshop on Theoretical Physics (Progress in Neural Processing), pages 261–276, Postech (Pohang, Korea), 1995. World Scientific. [32] Q. Li, Q. Meng, J. Cai, H. Yoshino, and A. Mochida. Applying support vector machine to predict hourly cooling load in the building. Appl Energ, 86(10):2249–2256, 2009. [33] F. Magoules, H.-X. Zhao, and D. Elizondo. Development of an   RDP neural network for building energy consumption fault detection diagnosis. Energy and Buildings, 62:133–138, 2013. [34] N. Mendes, R. C. Oliveira, and G. H. Dos Santos. Energy efficiency and thermal comfort analysis using the PowerDomus hygrothermal simulation tool. In Proceedings of the 9th International Building Performance Simulation Association IBPSA Conference, pages 755–762, Montreal, Canada, 2005. [35] M. Ordenes, R. Lamberts, and S. Guths. Estimation of thermophysical   properties using natural signal analysis with heat and moisture transfer model. Energ Buildings, 41(12):1360 – 1367, 2009. [36] E. Osuna, R. Freund., and F. Girosi. Training support vector machines: an application to face detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 130–136, 1997. [37] S. S. Pappas, L. Ekonomou, D. C. Karamousantas, G. E. Chatzarakis, S. K. Katsikas, and P. Liatsis. Electricity demand loads modeling using autoregressive moving average (ARMA) models. Energy, 33(9):1353– 1360, 2008. [38] S. Paudel, M. Elmtiri, W. L. Kling, O. L. Corre, and B. Lacarriere.   Pseudo dynamic transitional modeling of building heating energy demand using artificial neural network. Energ Buildings, 70:81 – 93, 2014. [39] R. Platon, V. R. Dehkordi, and J. Martel. Hourly prediction of a building’s electricity consumption using case-based reasoning, artificial neural networks and principal component analysis. Energ Buildings, 92:10 – 18, 2015. [40] J. C. Platt. Fast training of support vector machines using sequential minimal optimization. In B. Scholkopf, C. J. C. Burges, and A. J.   Smola, editors, Advances in Kernel Methods: Support Vector Learning, chapter 12, pages 185–208. MIT Press, Cambridge (MA, USA), 1999. [41] M. Qin, G. Walton, R. Belarbi, and F. Allard. Simulation of whole building coupled hygrothermal-airflow transfer in different climates. Energ Convers Manage, 52(2):1470 – 1478, 2011. [42] D. Ruch and D. Claridge. A four-parameter change-point model for predicting energy consumption in commercial buildings. ASME J Sol Energ Eng, 114(2):77–83, 1992. [43] B. Scholkopf, R. Williamson, A. Smola, J. Shawe-Taylor, and J. Platt.   Support vector method for novelty detection. In T. K. L. Sara A. Solla and K. Muller, editors,   Advances in Neural Information Processing Systems 12, pages 582–588, Denver (CO, USA), 2000. MIT Press. [44] A. J. Smola and B. Scholkopf. A tutorial on support vector regression.   Stat Comput, 14(3):199–222, 2004. [45] M. S. Sodha, B. Kaur, A. Kumar, and N. K. Bansal. A comparison of the admittance and fourier methods for predicting heating/cooling loads. Sol Energy, 36(2):125 – 127, 1986. [46] A. Statnikov, C. F. Aliferis, D. P. Hardin, and I. Guyon. A Gentle Introduction to Support Vector Machines in Biomedicine. Volume 1: Theory and Methods. World Scientific, Signapore, 2011. [47] L. Stephan, A. Bastide, and E. Wurtz. Optimizing opening dimensions for naturally ventilated buildings. Appl Energ, 88(8):2791 – 2801, 2011. [48] V. N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998. [49] V. N. Vapnik. The nature of statistical learning theory. Springer-Verlag New York, Inc., New York, NY, USA, 2nd edition, 2010. [50] J. Wang, L. Li, D. Niu, and Z. Tan. An annual load forecasting model based on support vector regression with differential evolution algorithm. Appl Energ, 94:65–70, 2012. [51] S. L. Wong, K. K. W. Wan, and T. N. T. Lam. Artificial neural networks for energy analysis of office buildings with daylighting. Appl Energ, 87(2):551–557, 2010. [52] X. Xu, J. E. Taylor, A. L. Pisello, and P. J. Culligan. The impact of placebased affiliation networks on energy conservation: An holistic model that integrates the influence of buildings, residents and the neighborhood context. Energ Buildings, 55:637 – 646, 2012. [53] J. Yang, H. Rivard, and R. Zmeureanu. On-line building energy prediction using adaptive artificial neural networks. Energ Buildings, 37(12):1250–1259, 2005. [54] H.-X. Zhao and F. Magoules. A parallel statistical learning approach to   the prediction of building energy consumption based on large datasets. In Q. Guo and Y. Guo, editors, Proceedings of the 6th International Symposium on Distributed Computing and Applications to Business, Engineering and Science (DCABES), Wuhan, Hubei, China, October 16–19, 2009. Publishing House of Electronics Industry, 2009. [55] H.-X. Zhao and F. Magoules. A new parallel implementation of SVM   on multi-core systems. In Y. Li, editor, Proceedings of the International Conference on Modeling, Simulation and Control (ICMSC 2010), Cairo, Egypt, November 2–4, 2010. ISBN/ISSN: 978-1-4244-8823-0, 2010. [56] H.-X. Zhao and F. Magoules. Parallel support vector machines applied   to the prediction of multiple buildings energy consumption. Journal of Algorithms and Computational Technology, 4(2):231–250, 2010. [57] H.-X. Zhao and F. Magoules.   Feature selection for support vector regression in the application of building energy prediction. In Proceedings of the 9th IEEE International Symposium on Applied Machine Intelligence and Informatics (SAMI 2011), Smolenice, Slovakia, January 27–29, 2011. IEEE Computer Society, 2011. [58] H.-X. Zhao and F. Magoules. New parallel support vector regression   for predicting building energy consumption. In Proceedings of the IEEE Symposium Series on Computational Intelligence (SSCI 2011), Paris, France, April 11–15, 2011. IEEE Computer Society, 2011. [59] H.-X. Zhao and F. Magoules. Parallel support vector machines on multi-   core and multiprocessor systems. In R. Fox, editor, Proceedings of the 11th International Conference on Artificial Intelligence and Applications (AIA 2011), Innsbruck, Austria, February 14–16, 2011. IASTED, 2011. [60] H.-X. Zhao and F. Magoules. Feature selection for predicting building   energy consumption based on statistical learning method. Journal of Algorithms and Computational Technology, 6(1):59–78, 2012. [61] H.-X. Zhao and F. Magoules. A review on the prediction of building   energy consumption. Renew Sust Energ Rev, 16(6):3586 – 3592, 2012. [62] E. Znouda, N. Ghrab-Morcos, and A. Hadj-Alouane. Optimization of mediterranean building design using genetic algorithms. Energ Buildings, 39(2):148 – 153, 2007.\"\n",
    "]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
